Prediction,Target,log,Final
"public class A { @ Override public void visit ( String key , Object value ) { log . debug ( ""Load annotation attribute {} = {} ({})"" , name , value , value . getClass ( ) . getName ( ) ) ; if ( ""ordinal"" . equals ( name ) ) { extensionInfo . ordinal = Integer . parseInt ( value . toString ( ) ) ; } else if ( ""plugins"" . equals ( name ) ) { if ( value instanceof String ) { log . debug ( ""Found plugins {}"" , Arrays . toString ( ( String ) value ) ) ; extensionInfo . plugins . add ( ( String ) value ) ; } else if ( value instanceof String [ ] ) { log . debug ( ""Found plugins {}"" , Arrays . toString ( ( String [ ] ) value ) ) ; extensionInfo . plugins . addAll ( Arrays . asList ( ( String [ ] ) value ) ) ; } else { log . debug ( ""Found plugin {}"" , value . toString ( ) ) ; extensionInfo . plugins . add ( value . toString ( ) ) ; } } else { String pointClassName = ( ( Type ) value ) . getClassName ( ) ; log . debug ( ""Found point "" + pointClassName ) ; extensionInfo . points . add ( pointClassName ) ; } super . visit ( key , value ) ; } }","public class A { @ Override public void visit ( String key , Object value ) { log . debug ( ""Load annotation attribute {} = {} ({})"" , name , value , value . getClass ( ) . getName ( ) ) ; if ( ""ordinal"" . equals ( name ) ) { extensionInfo . ordinal = Integer . parseInt ( value . toString ( ) ) ; } else if ( ""plugins"" . equals ( name ) ) { if ( value instanceof String ) { log . debug ( ""Found plugin {}"" , value ) ; extensionInfo . plugins . add ( ( String ) value ) ; } else if ( value instanceof String [ ] ) { log . debug ( ""Found plugins {}"" , Arrays . toString ( ( String [ ] ) value ) ) ; extensionInfo . plugins . addAll ( Arrays . asList ( ( String [ ] ) value ) ) ; } else { log . debug ( ""Found plugin {}"" , value . toString ( ) ) ; extensionInfo . plugins . add ( value . toString ( ) ) ; } } else { String pointClassName = ( ( Type ) value ) . getClassName ( ) ; log . debug ( ""Found point "" + pointClassName ) ; extensionInfo . points . add ( pointClassName ) ; } super . visit ( key , value ) ; } }","log . debug ( ""Found plugin {}"" , value ) ;",Same Information
"public class A { @ POST @ Path ( ""/transfers/reply"" ) @ Consumes ( MediaType . APPLICATION_XML ) @ Produces ( MediaType . APPLICATION_JSON ) @ Secured ( permission = TRANSFERS_REPLY , description = ""Start transfer reply workflow."" ) public Response transferReply ( InputStream transferReply ) { try ( AccessInternalClient client = accessInternalClientFactory . getClient ( ) ) { return client . startTransferReplyWorkflow ( transferReply ) . toResponse ( ) ; } catch ( Exception e ) { logger . error ( ""Exception during startTransferReplyWorkflow"" , e ) ; return Response . status ( Status . INTERNAL_SERVER_ERROR ) . entity ( getErrorEntity ( Status . INTERNAL_SERVER_ERROR , e . getLocalizedMessage ( ) ) ) . build ( ) ; } } }","public class A { @ POST @ Path ( ""/transfers/reply"" ) @ Consumes ( MediaType . APPLICATION_XML ) @ Produces ( MediaType . APPLICATION_JSON ) @ Secured ( permission = TRANSFERS_REPLY , description = ""Start transfer reply workflow."" ) public Response transferReply ( InputStream transferReply ) { try ( AccessInternalClient client = accessInternalClientFactory . getClient ( ) ) { return client . startTransferReplyWorkflow ( transferReply ) . toResponse ( ) ; } catch ( Exception e ) { LOGGER . error ( e ) ; return Response . status ( Status . INTERNAL_SERVER_ERROR ) . entity ( getErrorEntity ( Status . INTERNAL_SERVER_ERROR , e . getLocalizedMessage ( ) ) ) . build ( ) ; } } }",LOGGER . error ( e ) ;,Same Information
"public class A { protected final void setChannelError ( @ Nullable final Throwable cause ) { if ( hasException ( ) ) { return ; } LOG . error ( ""Channel {} not set due to {}"" , address , cause . getMessage ( ) ) ; exception . set ( cause ) ; } }","public class A { protected final void setChannelError ( @ Nullable final Throwable cause ) { if ( hasException ( ) ) { return ; } LOG . error ( String . format ( ""A channel exception set on %s"" , toString ( ) ) ) ; exception . set ( cause ) ; } }","LOG . error ( String . format ( ""A channel exception set on %s"" , toString ( ) ) ) ;",Same Information
"public class A { public static void changeCharsetToUtf ( JdbcConnection jdbcCon ) throws DatabaseException , SQLException { Statement stmt = jdbcCon . createStatement ( ) ; String dbName = jdbcCon . getCatalog ( ) ; String sql = String . format ( ""ALTER DATABASE `%s` CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;"" , dbName ) ; int result = stmt . executeUpdate ( sql ) ; logger . info ( ""Changed MySQL charset of '{}' with result {}"" , dbName , result ) ; } }","public class A { public static void changeCharsetToUtf ( JdbcConnection jdbcCon ) throws DatabaseException , SQLException { Statement stmt = jdbcCon . createStatement ( ) ; String dbName = jdbcCon . getCatalog ( ) ; String sql = String . format ( ""ALTER DATABASE `%s` CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;"" , dbName ) ; int result = stmt . executeUpdate ( sql ) ; LOGGER . info ( ""ALTER charset execute result: {}"" , result ) ; } }","LOGGER . info ( ""ALTER charset execute result: {}"" , result ) ;",Same Information
"public class A { public String encrypt ( final String str ) { try { byte [ ] utf8 = str . getBytes ( ""UTF8"" ) ; byte [ ] enc = ecipher . doFinal ( utf8 ) ; return Base64 . encodeBase64String ( enc ) ; } catch ( UnsupportedEncodingException e ) { LOGGER . trace ( ""DesEncrypter unsupported encoding exception"" , e ) ; throw new ApplicationException ( ""DesEncrypter failed - UnsupportedEncodingException "" , e ) ; } catch ( GeneralSecurityException e ) { LOGGER . trace ( ""DesEncrypter encryption failed - GeneralSecurityException"" , e ) ; throw new ApplicationException ( ""DesEncrypter encryption failed - GeneralSecurityException"" , e ) ; } } }","public class A { public String encrypt ( final String str ) { try { byte [ ] utf8 = str . getBytes ( ""UTF8"" ) ; byte [ ] enc = ecipher . doFinal ( utf8 ) ; return Base64 . encodeBase64String ( enc ) ; } catch ( UnsupportedEncodingException e ) { LOGGER . trace ( ""DesEncrypter unsupported encoding exception"" , e ) ; throw new ApplicationException ( ""DesEncrypter failed - UnsupportedEncodingException "" , e ) ; } catch ( GeneralSecurityException e ) { LOGGER . trace ( ""DesEncrypter encryption failed"" , e ) ; throw new ApplicationException ( ""DesEncrypter encryption failed - GeneralSecurityException"" , e ) ; } } }","LOGGER . trace ( ""DesEncrypter encryption failed"" , e ) ;",Same Information
"public class A { @ Override public void run ( ) { webRtcReceiver [ i - 1 ] = new WebRtcEndpoint . Builder ( pipeline ) . build ( ) ; recordingFile [ i - 1 ] = getRecordUrl ( ""-receiver"" + i + extension ) ; recorder [ i - 1 ] = new RecorderEndpoint . Builder ( pipeline , recordingFile [ i - 1 ] ) . withMediaProfile ( mediaProfileSpecType ) . build ( ) ; webRtcSender . connect ( webRtcReceiver [ i - 1 ] ) ; webRtcSender . connect ( recorder [ i - 1 ] ) ; try { getPage ( i ) . subscribeEvents ( ""playing"" ) ; getPage ( i ) . initWebRtc ( webRtcReceiver [ i - 1 ] , WebRtcChannel . AUDIO_AND_VIDEO , WebRtcMode . RCV_ONLY ) ; Assert . assertTrue ( ""Not received media in receiver "" + i , getPage ( i ) . waitForEvent ( ""playing"" ) ) ; recorder [ i - 1 ] . record ( ) ; Thread . sleep ( PLAYTIME_MS ) ; recorder [ i - 1 ] . stopAndWait ( ) ; Thread . sleep ( 4000 ) ; } catch ( InterruptedException e ) { log . error ( ""Exception in receiver "" + i , e ) ; } latch . countDown ( ) ; } }","public class A { @ Override public void run ( ) { webRtcReceiver [ i - 1 ] = new WebRtcEndpoint . Builder ( pipeline ) . build ( ) ; recordingFile [ i - 1 ] = getRecordUrl ( ""-receiver"" + i + extension ) ; recorder [ i - 1 ] = new RecorderEndpoint . Builder ( pipeline , recordingFile [ i - 1 ] ) . withMediaProfile ( mediaProfileSpecType ) . build ( ) ; webRtcSender . connect ( webRtcReceiver [ i - 1 ] ) ; webRtcSender . connect ( recorder [ i - 1 ] ) ; try { getPage ( i ) . subscribeEvents ( ""playing"" ) ; getPage ( i ) . initWebRtc ( webRtcReceiver [ i - 1 ] , WebRtcChannel . AUDIO_AND_VIDEO , WebRtcMode . RCV_ONLY ) ; Assert . assertTrue ( ""Not received media in receiver "" + i , getPage ( i ) . waitForEvent ( ""playing"" ) ) ; recorder [ i - 1 ] . record ( ) ; Thread . sleep ( PLAYTIME_MS ) ; recorder [ i - 1 ] . stopAndWait ( ) ; Thread . sleep ( 4000 ) ; } catch ( InterruptedException e ) { log . error ( ""InterruptedException in receiver "" + i , e ) ; } latch . countDown ( ) ; } }","log . error ( ""InterruptedException in receiver "" + i , e ) ;",Same Information
"public class A { private void scheduleClockSyncJob ( ) { logger . debug ( ""Scheduling clock sync job to run every {} seconds"" , CLOCK_SYNC_INTERVAL_SEC ) ; cancelClockSyncJob ( ) ; clockSyncJob = scheduler . scheduleWithFixedDelay ( ( ) -> { if ( this . isGConcerto ) { try { connector . sendCommand ( NuvoCommand . CFGTIME . getValue ( ) + DATE_FORMAT . format ( new Date ( ) ) ) ; } catch ( NuvoException e ) { logger . debug ( ""Error syncing clock: {}"" , e . getMessage ( ) ) ; } } else { this . cancelClockSyncJob ( ) ; } } , INITIAL_CLOCK_SYNC_DELAY_SEC , CLOCK_SYNC_INTERVAL_SEC , TimeUnit . SECONDS ) ; } }","public class A { private void scheduleClockSyncJob ( ) { logger . debug ( ""Schedule clock sync job"" ) ; cancelClockSyncJob ( ) ; clockSyncJob = scheduler . scheduleWithFixedDelay ( ( ) -> { if ( this . isGConcerto ) { try { connector . sendCommand ( NuvoCommand . CFGTIME . getValue ( ) + DATE_FORMAT . format ( new Date ( ) ) ) ; } catch ( NuvoException e ) { logger . debug ( ""Error syncing clock: {}"" , e . getMessage ( ) ) ; } } else { this . cancelClockSyncJob ( ) ; } } , INITIAL_CLOCK_SYNC_DELAY_SEC , CLOCK_SYNC_INTERVAL_SEC , TimeUnit . SECONDS ) ; } }","logger . debug ( ""Schedule clock sync job"" ) ;",Same Information
"public class A { @ Override protected void onEvent ( Event event ) { boolean initialized = CartridgeAgentConfiguration . getInstance ( ) . isInitialized ( ) ; if ( ! initialized ) { try { TopologyManager . acquireReadLock ( ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( ""Complete topology event received"" ) ; } CompleteTopologyEvent completeTopologyEvent = ( CompleteTopologyEvent ) event ; extensionHandler . onCompleteTopologyEvent ( completeTopologyEvent ) ; } catch ( Exception e ) { if ( log . isErrorEnabled ( ) ) { log . error ( ""Error processing CompleteTopologyEvent"" , e ) ; } } finally { TopologyManager . releaseReadLock ( ) ; } } } }","public class A { @ Override protected void onEvent ( Event event ) { boolean initialized = CartridgeAgentConfiguration . getInstance ( ) . isInitialized ( ) ; if ( ! initialized ) { try { TopologyManager . acquireReadLock ( ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( ""Complete topology event received"" ) ; } CompleteTopologyEvent completeTopologyEvent = ( CompleteTopologyEvent ) event ; extensionHandler . onCompleteTopologyEvent ( completeTopologyEvent ) ; } catch ( Exception e ) { if ( log . isErrorEnabled ( ) ) { log . error ( ""Error processing complete topology event"" , e ) ; } } finally { TopologyManager . releaseReadLock ( ) ; } } } }","log . error ( ""Error processing complete topology event"" , e ) ;",Same Information
"public class A { static boolean hasAddressSpaceChanged ( AddressSpace original , AddressSpace addressSpace ) { boolean changed = false ; if ( ! Objects . equals ( original . getMetadata ( ) , addressSpace . getMetadata ( ) ) ) { log . debug ( ""Metadata changed from {} to {}"" , original . getMetadata ( ) , addressSpace . getMetadata ( ) ) ; changed = true ; } if ( ! Objects . equals ( original . getSpec ( ) , addressSpace . getSpec ( ) ) ) { log . debug ( ""Spec changed from {} to {}"" , original . getSpec ( ) , addressSpace . getSpec ( ) ) ; changed = true ; } if ( ! Objects . equals ( original . getStatus ( ) , addressSpace . getStatus ( ) ) ) { log . debug ( ""Status changed from {} to {}"" , original . getStatus ( ) , addressSpace . getStatus ( ) ) ; changed = true ; } return changed ; } }","public class A { static boolean hasAddressSpaceChanged ( AddressSpace original , AddressSpace addressSpace ) { boolean changed = false ; if ( ! Objects . equals ( original . getMetadata ( ) , addressSpace . getMetadata ( ) ) ) { log . debug ( ""Meta changed from {} to {}"" , original . getMetadata ( ) , addressSpace . getMetadata ( ) ) ; changed = true ; } if ( ! Objects . equals ( original . getSpec ( ) , addressSpace . getSpec ( ) ) ) { log . debug ( ""Spec changed from {} to {}"" , original . getSpec ( ) , addressSpace . getSpec ( ) ) ; changed = true ; } if ( ! Objects . equals ( original . getStatus ( ) , addressSpace . getStatus ( ) ) ) { log . debug ( ""Status changed from {} to {}"" , original . getStatus ( ) , addressSpace . getStatus ( ) ) ; changed = true ; } return changed ; } }","log . debug ( ""Meta changed from {} to {}"" , original . getMetadata ( ) , addressSpace . getMetadata ( ) ) ;",Same Information
"public class A { public synchronized HttpConnection getFreeConnection ( HostConfiguration hostConfiguration ) { HttpConnectionWithReference connection = null ; HostConnectionPool hostPool = getHostPool ( hostConfiguration , false ) ; if ( ( hostPool != null ) && ( hostPool . freeConnections . size ( ) > 0 ) ) { connection = ( HttpConnectionWithReference ) hostPool . freeConnections . removeLast ( ) ; freeConnections . remove ( connection ) ; storeReferenceToConnection ( connection , hostConfiguration , this ) ; if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""Getting free connection, hostConfig="" + hostConfiguration ) ; } idleConnectionHandler . remove ( connection ) ; } else if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""Unable to get free connection, hostConfig="" + hostConfiguration ) ; } return connection ; } }","public class A { public synchronized HttpConnection getFreeConnection ( HostConfiguration hostConfiguration ) { HttpConnectionWithReference connection = null ; HostConnectionPool hostPool = getHostPool ( hostConfiguration , false ) ; if ( ( hostPool != null ) && ( hostPool . freeConnections . size ( ) > 0 ) ) { connection = ( HttpConnectionWithReference ) hostPool . freeConnections . removeLast ( ) ; freeConnections . remove ( connection ) ; storeReferenceToConnection ( connection , hostConfiguration , this ) ; if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""Getting free connection, hostConfig="" + hostConfiguration ) ; } idleConnectionHandler . remove ( connection ) ; } else if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""There were no free connections to get, hostConfig="" + hostConfiguration ) ; } return connection ; } }","LOG . debug ( ""There were no free connections to get, hostConfig="" + hostConfiguration ) ;",Same Information
"public class A { private boolean isReachableAndValidHueBridge ( BridgeJsonParameters bridge ) { String host = bridge . getInternalIpAddress ( ) ; String id = bridge . getId ( ) ; String description ; if ( host == null ) { logger . debug ( ""Bridge not discovered: ip is null"" ) ; return false ; } if ( id == null ) { logger . debug ( ""Bridge not discovered: no ID"" ) ; return false ; } if ( id . length ( ) < 10 ) { logger . debug ( ""Bridge not discovered: id {} is shorter then 10."" , id ) ; return false ; } if ( ! id . substring ( 6 , 10 ) . equals ( BRIDGE_INDICATOR ) ) { logger . debug ( ""Bridge not discovered: id {} does not contain bridge indicator {} or its at the wrong position."" , id , BRIDGE_INDICATOR ) ; return false ; } try { description = doGetRequest ( DESC_URL_PATTERN . replace ( ""HOST"" , host ) ) ; } catch ( IOException e ) { logger . debug ( ""Bridge not discovered: Failure accessing description file for ip: {}"" , host ) ; return false ; } if ( ! description . contains ( MODEL_NAME_PHILIPS_HUE ) ) { logger . debug ( ""Bridge not discovered: Description does not containing the model name: {}"" , description ) ; return false ; } return true ; } }","public class A { private boolean isReachableAndValidHueBridge ( BridgeJsonParameters bridge ) { String host = bridge . getInternalIpAddress ( ) ; String id = bridge . getId ( ) ; String description ; if ( host == null ) { logger . debug ( ""Bridge not discovered: ip is null"" ) ; return false ; } if ( id == null ) { logger . debug ( ""Bridge not discovered: id is null"" ) ; return false ; } if ( id . length ( ) < 10 ) { logger . debug ( ""Bridge not discovered: id {} is shorter then 10."" , id ) ; return false ; } if ( ! id . substring ( 6 , 10 ) . equals ( BRIDGE_INDICATOR ) ) { logger . debug ( ""Bridge not discovered: id {} does not contain bridge indicator {} or its at the wrong position."" , id , BRIDGE_INDICATOR ) ; return false ; } try { description = doGetRequest ( DESC_URL_PATTERN . replace ( ""HOST"" , host ) ) ; } catch ( IOException e ) { logger . debug ( ""Bridge not discovered: Failure accessing description file for ip: {}"" , host ) ; return false ; } if ( ! description . contains ( MODEL_NAME_PHILIPS_HUE ) ) { logger . debug ( ""Bridge not discovered: Description does not containing the model name: {}"" , description ) ; return false ; } return true ; } }","logger . debug ( ""Bridge not discovered: id is null"" ) ;",Same Information
"public class A { protected final Service createService ( ) { Service service = null ; URL wsdlURL ; if ( StringUtils . isNotBlank ( wsdlLocation ) && StringUtils . isNotBlank ( serviceName ) ) { try { URIResolver uriResolver = new URIResolver ( ) ; uriResolver . resolve ( """" , wsdlLocation , this . getClass ( ) ) ; wsdlURL = uriResolver . isResolved ( ) ? uriResolver . getURL ( ) : new URL ( wsdlLocation ) ; service = AccessController . doPrivileged ( ( PrivilegedAction < Service > ) ( ) -> Service . create ( wsdlURL , QName . valueOf ( serviceName ) ) ) ; auditRemoteConnection ( wsdlURL ) ; } catch ( Exception e ) { LOGGER . info ( ""Unable to create service from WSDL location. Set log level for \""org.codice.ddf.security.claims.attributequery.common\"" to DEBUG for more information."" ) ; LOGGER . debug ( ""Unable to create service. WSDL_URL: "" + wsdlURL , e ) ; } } return service ; } }","public class A { protected final Service createService ( ) { Service service = null ; URL wsdlURL ; if ( StringUtils . isNotBlank ( wsdlLocation ) && StringUtils . isNotBlank ( serviceName ) ) { try { URIResolver uriResolver = new URIResolver ( ) ; uriResolver . resolve ( """" , wsdlLocation , this . getClass ( ) ) ; wsdlURL = uriResolver . isResolved ( ) ? uriResolver . getURL ( ) : new URL ( wsdlLocation ) ; service = AccessController . doPrivileged ( ( PrivilegedAction < Service > ) ( ) -> Service . create ( wsdlURL , QName . valueOf ( serviceName ) ) ) ; auditRemoteConnection ( wsdlURL ) ; } catch ( Exception e ) { LOGGER . info ( ""Unable to create service from WSDL location. Set log level for \""org.codice.ddf.security.claims.attributequery.common\"" to DEBUG for more information."" ) ; LOGGER . debug ( ""Unable to create service from WSDL location."" , e ) ; } } return service ; } }","LOGGER . debug ( ""Unable to create service from WSDL location."" , e ) ;",Same Information
"public class A { protected synchronized void close ( ) { if ( isClosed ) { return ; } try { if ( stmt != null ) { try { stmt . close ( ) ; } catch ( SQLException e ) { log . error ( ""Error closing SQL statement"" , e ) ; } } } finally { if ( conn != null ) { try { conn . close ( ) ; } catch ( SQLException e ) { log . error ( ""Error closing connection"" , e ) ; } } } isClosed = true ; } }","public class A { protected synchronized void close ( ) { if ( isClosed ) { return ; } try { if ( stmt != null ) { try { stmt . close ( ) ; } catch ( SQLException e ) { log . error ( ""Error closing SQL statement"" , e ) ; } } } finally { if ( conn != null ) { try { conn . close ( ) ; } catch ( SQLException e ) { log . error ( ""Error closing SQL Connection"" , e ) ; } } } isClosed = true ; } }","log . error ( ""Error closing SQL Connection"" , e ) ;",Same Information
"public class A { @ Override public void create ( Object key , Object value ) throws IOException { long start = stats . startUpdate ( ) ; Collection < Document > docs = Collections . emptyList ( ) ; boolean exceptionHappened = false ; try { try { docs = serializer . toDocuments ( index , value ) ; } catch ( Exception e ) { exceptionHappened = true ; stats . incFailedEntries ( ) ; logger . info ( ""Failed to update index for "" + value + "" due to "" + e . getMessage ( ) ) ; } if ( ! exceptionHappened ) { docs . forEach ( doc -> SerializerUtil . addKey ( key , doc ) ) ; writer . addDocuments ( docs ) ; } } finally { stats . endUpdate ( start ) ; } } }","public class A { @ Override public void create ( Object key , Object value ) throws IOException { long start = stats . startUpdate ( ) ; Collection < Document > docs = Collections . emptyList ( ) ; boolean exceptionHappened = false ; try { try { docs = serializer . toDocuments ( index , value ) ; } catch ( Exception e ) { exceptionHappened = true ; stats . incFailedEntries ( ) ; logger . info ( ""Failed to add index for "" + value + "" due to "" + e . getMessage ( ) ) ; } if ( ! exceptionHappened ) { docs . forEach ( doc -> SerializerUtil . addKey ( key , doc ) ) ; writer . addDocuments ( docs ) ; } } finally { stats . endUpdate ( start ) ; } } }","logger . info ( ""Failed to add index for "" + value + "" due to "" + e . getMessage ( ) ) ;",Same Information
"public class A { @ OnReturn public static void onReturn ( @ BindReturn @ Nullable Object returnValue , @ BindReceiver HttpURLConnection httpURLConnection , @ BindTraveler @ Nullable TraceEntryOrTimer entryOrTimer ) { if ( httpURLConnection instanceof HasTraceEntryMixin ) { if ( returnValue instanceof HasTraceEntryMixin ) { TraceEntry traceEntry = ( ( HasTraceEntryMixin ) httpURLConnection ) . glowroot$getTraceEntry ( ) ; ( ( HasTraceEntryMixin ) returnValue ) . glowroot$setTraceEntry ( traceEntry ) ; } else if ( returnValue != null && ! outputStreamIssueAlreadyLogged . getAndSet ( true ) ) { logger . info ( ""found non-instrumented http url connection input stream, please"" + "" report to the Glowroot project: {}"" , returnValue . getClass ( ) . getName ( ) ) ; } } ConnectAdvice . onReturn ( entryOrTimer ) ; } }","public class A { @ OnReturn public static void onReturn ( @ BindReturn @ Nullable Object returnValue , @ BindReceiver HttpURLConnection httpURLConnection , @ BindTraveler @ Nullable TraceEntryOrTimer entryOrTimer ) { if ( httpURLConnection instanceof HasTraceEntryMixin ) { if ( returnValue instanceof HasTraceEntryMixin ) { TraceEntry traceEntry = ( ( HasTraceEntryMixin ) httpURLConnection ) . glowroot$getTraceEntry ( ) ; ( ( HasTraceEntryMixin ) returnValue ) . glowroot$setTraceEntry ( traceEntry ) ; } else if ( returnValue != null && ! outputStreamIssueAlreadyLogged . getAndSet ( true ) ) { logger . info ( ""found non-instrumented http url connection output stream, please"" + "" report to the Glowroot project: {}"" , returnValue . getClass ( ) . getName ( ) ) ; } } ConnectAdvice . onReturn ( entryOrTimer ) ; } }","logger . info ( ""found non-instrumented http url connection output stream, please"" + "" report to the Glowroot project: {}"" , returnValue . getClass ( ) . getName ( ) ) ;",Same Information
"public class A { @ Test public void testQuery ( ) throws Exception { CreationTools . createJobDef ( null , true , ""pyl.KillMe"" , null , ""jqm-tests/jqm-test-pyl/target/test.jar"" , TestHelpers . qNormal , 42 , ""jqm-test-kill"" , null , ""Franquin"" , ""ModuleMachin"" , ""other"" , ""other"" , false , cnx ) ; cnx . commit ( ) ; JqmClientFactory . getClient ( ) . enqueue ( ""jqm-test-kill"" , ""test"" ) ; JqmClientFactory . getClient ( ) . enqueue ( ""jqm-test-kill"" , ""test"" ) ; JqmClientFactory . getClient ( ) . enqueue ( ""jqm-test-kill"" , ""test"" ) ; JqmClientFactory . getClient ( ) . enqueue ( ""jqm-test-kill"" , ""test"" ) ; JqmClientFactory . getClient ( ) . enqueue ( ""jqm-test-kill"" , ""test"" ) ; jqmlogger . debug ( ""COUNT RUNNING "" + cnx . runSelectSingle ( ""ji_select_count_running"" , Integer . class ) ) ; jqmlogger . debug ( ""COUNT ALL "" + cnx . runSelectSingle ( ""ji_select_count_all"" , Integer . class ) ) ; Assert . assertEquals ( 0 , Query . create ( ) . setQueryLiveInstances ( true ) . setQueryHistoryInstances ( false ) . addStatusFilter ( com . enioka . jqm . api . State . RUNNING ) . addStatusFilter ( com . enioka . jqm . api . State . ENDED ) . run ( ) . size ( ) ) ; Assert . assertEquals ( 5 , Query . create ( ) . setQueryLiveInstances ( true ) . setQueryHistoryInstances ( false ) . addStatusFilter ( com . enioka . jqm . api . State . SUBMITTED ) . run ( ) . size ( ) ) ; } }","public class A { @ Test public void testQuery ( ) throws Exception { CreationTools . createJobDef ( null , true , ""pyl.KillMe"" , null , ""jqm-tests/jqm-test-pyl/target/test.jar"" , TestHelpers . qNormal , 42 , ""jqm-test-kill"" , null , ""Franquin"" , ""ModuleMachin"" , ""other"" , ""other"" , false , cnx ) ; cnx . commit ( ) ; JqmClientFactory . getClient ( ) . enqueue ( ""jqm-test-kill"" , ""test"" ) ; JqmClientFactory . getClient ( ) . enqueue ( ""jqm-test-kill"" , ""test"" ) ; JqmClientFactory . getClient ( ) . enqueue ( ""jqm-test-kill"" , ""test"" ) ; JqmClientFactory . getClient ( ) . enqueue ( ""jqm-test-kill"" , ""test"" ) ; JqmClientFactory . getClient ( ) . enqueue ( ""jqm-test-kill"" , ""test"" ) ; jqmlogger . debug ( ""COUNT RUNNING "" + cnx . runSelectSingle ( ""ji_select_count_running"" , Integer . class ) ) ; jqmlogger . debug ( ""COUNT ALL "" + cnx . runSelectSingle ( ""ji_select_count_all"" , Integer . class ) ) ; Assert . assertEquals ( 0 , Query . create ( ) . setQueryLiveInstances ( true ) . setQueryHistoryInstances ( false ) . addStatusFilter ( com . enioka . jqm . api . State . RUNNING ) . addStatusFilter ( com . enioka . jqm . api . State . ENDED ) . run ( ) . size ( ) ) ; Assert . assertEquals ( 5 , Query . create ( ) . setQueryLiveInstances ( true ) . setQueryHistoryInstances ( false ) . addStatusFilter ( com . enioka . jqm . api . State . SUBMITTED ) . run ( ) . size ( ) ) ; } }","jqmlogger . debug ( ""COUNT ALL "" + cnx . runSelectSingle ( ""ji_select_count_all"" , Integer . class ) ) ;",Same Information
"public class A { public Model getConciseBoundedDescription ( LiteralLabel literal , CBDStructureTree structureTree ) throws Exception { logger . trace ( ""Computing CBD for {} ..."" , literal ) ; long start = System . currentTimeMillis ( ) ; String query = generateQuery ( literal , structureTree ) ; System . out . println ( query ) ; if ( workaround ) { return constructWithReplacement ( endpoint , query ) ; } try ( QueryExecution qe = qef . createQueryExecution ( query ) ) { Model model = qe . execConstruct ( ) ; long end = System . currentTimeMillis ( ) ; logger . trace ( ""Got {} triples in {} ms."" , model . size ( ) , ( end - start ) ) ; return model ; } catch ( Exception e ) { logger . error ( ""cBD retrieval failed when using query\n"" + query ) ; throw new Exception ( ""CBD retrieval failed when using query\n"" + query , e ) ; } } }","public class A { public Model getConciseBoundedDescription ( LiteralLabel literal , CBDStructureTree structureTree ) throws Exception { logger . trace ( ""Computing CBD for {} ..."" , literal ) ; long start = System . currentTimeMillis ( ) ; String query = generateQuery ( literal , structureTree ) ; System . out . println ( query ) ; if ( workaround ) { return constructWithReplacement ( endpoint , query ) ; } try ( QueryExecution qe = qef . createQueryExecution ( query ) ) { Model model = qe . execConstruct ( ) ; long end = System . currentTimeMillis ( ) ; logger . trace ( ""Got {} triples in {} ms."" , model . size ( ) , ( end - start ) ) ; return model ; } catch ( Exception e ) { logger . error ( ""CBD retrieval failed when using query\n{}"" , query ) ; throw new Exception ( ""CBD retrieval failed when using query\n"" + query , e ) ; } } }","logger . error ( ""CBD retrieval failed when using query\n{}"" , query ) ;",Same Information
"public class A { public static void sendApplicationInstanceActivatedEvent ( String appId , String instanceId ) { if ( log . isInfoEnabled ( ) ) { log . info ( ""Publishing application instance activated event: [application] "" + appId + "" [instance] "" + instanceId ) ; } ApplicationInstanceActivatedEvent applicationActivatedEvent = new ApplicationInstanceActivatedEvent ( appId , instanceId ) ; publishEvent ( applicationActivatedEvent ) ; } }","public class A { public static void sendApplicationInstanceActivatedEvent ( String appId , String instanceId ) { if ( log . isInfoEnabled ( ) ) { log . info ( ""Publishing application instance active event: [application] "" + appId + "" [instance] "" + instanceId ) ; } ApplicationInstanceActivatedEvent applicationActivatedEvent = new ApplicationInstanceActivatedEvent ( appId , instanceId ) ; publishEvent ( applicationActivatedEvent ) ; } }","log . info ( ""Publishing application instance active event: [application] "" + appId + "" [instance] "" + instanceId ) ;",Same Information
"public class A { @ InternalMethod @ RequestMapping ( value = ""internal/{fileName:.+}"" , method = RequestMethod . POST ) public UploadResponse uploadInternal ( @ PathVariable ( ) final String fileName , @ RequestParam ( value = CloudifyConstants . UPLOAD_FILE_PARAM_NAME , required = true ) final MultipartFile file ) throws RestErrorException { String name = fileName ; if ( StringUtils . isEmpty ( fileName ) ) { name = file . getOriginalFilename ( ) ; } if ( logger . isLoggable ( Level . INFO ) ) { logger . info ( ""[uploadInternal] - request to upload file "" + name ) ; } String uploadKey = null ; try { uploadKey = uploadRepo . put ( name , file ) ; } catch ( IOException e ) { if ( logger . isLoggable ( Level . WARNING ) ) { logger . warning ( ""could not upload file "" + name + "" error was - "" + e . getMessage ( ) ) ; } throw new RestErrorException ( CloudifyMessageKeys . UPLOAD_FAILED . getName ( ) , name , e . getMessage ( ) ) ; } if ( logger . isLoggable ( Level . INFO ) ) { logger . info ( ""[uploadInternal] - successfuly uploaded file "" + name + "" [upload key = "" + uploadKey + ""]"" ) ; } UploadResponse response = new UploadResponse ( ) ; response . setUploadKey ( uploadKey ) ; return response ; } }","public class A { @ InternalMethod @ RequestMapping ( value = ""internal/{fileName:.+}"" , method = RequestMethod . POST ) public UploadResponse uploadInternal ( @ PathVariable ( ) final String fileName , @ RequestParam ( value = CloudifyConstants . UPLOAD_FILE_PARAM_NAME , required = true ) final MultipartFile file ) throws RestErrorException { String name = fileName ; if ( StringUtils . isEmpty ( fileName ) ) { name = file . getOriginalFilename ( ) ; } if ( logger . isLoggable ( Level . INFO ) ) { logger . info ( ""[uploadInternal] - received request to upload file "" + name ) ; } String uploadKey = null ; try { uploadKey = uploadRepo . put ( name , file ) ; } catch ( IOException e ) { if ( logger . isLoggable ( Level . WARNING ) ) { logger . warning ( ""could not upload file "" + name + "" error was - "" + e . getMessage ( ) ) ; } throw new RestErrorException ( CloudifyMessageKeys . UPLOAD_FAILED . getName ( ) , name , e . getMessage ( ) ) ; } if ( logger . isLoggable ( Level . INFO ) ) { logger . info ( ""[uploadInternal] - successfuly uploaded file "" + name + "" [upload key = "" + uploadKey + ""]"" ) ; } UploadResponse response = new UploadResponse ( ) ; response . setUploadKey ( uploadKey ) ; return response ; } }","logger . info ( ""[uploadInternal] - received request to upload file "" + name ) ;",Same Information
"public class A { private void setupLoginTty ( ) throws Exception { String setupTtyScriptName = Configuration . getString ( Constants . SERENGETI_SETUP_LOGIN_TTY_SCRIPT , Constants . SERENGETI_DEFAULT_SETUP_LOGIN_TTY_SCRIPT ) ; String setupTtyScript = getScriptName ( setupTtyScriptName ) ; String cmd = sudoCmd + "" "" + setupTtyScript ; String action = ""Setup login tty for "" + nodeIP ; logger . info ( action + "" command is: "" + cmd ) ; SSHUtil sshUtil = new SSHUtil ( ) ; String errMsg = null ; for ( int i = 0 ; i < Constants . SET_PASSWORD_MAX_RETRY_TIMES ; i ++ ) { try { if ( sshUtil . execCmd ( sshUser , privateKeyFile , nodeIP , sshPort , cmd , null , null ) ) { logger . info ( ""Succesfully logged in "" + action + "", port "" + sshPort . getPort ( ) + "", command is: "" + cmd ) ; return ; } } catch ( JSchException e ) { logger . warn ( ""Got exception when "" + action , e ) ; if ( errMsg == null ) { errMsg = e . getMessage ( ) ; } } try { Thread . sleep ( 3000 ) ; } catch ( InterruptedException e1 ) { logger . info ( ""Interrupted when waiting for setup login tty, retry immediately..."" ) ; } } logger . info ( action + "" failed"" ) ; throw SetPasswordException . FAIL_TO_SETUP_LOGIN_TTY ( nodeIP , errMsg ) ; } }","public class A { private void setupLoginTty ( ) throws Exception { String setupTtyScriptName = Configuration . getString ( Constants . SERENGETI_SETUP_LOGIN_TTY_SCRIPT , Constants . SERENGETI_DEFAULT_SETUP_LOGIN_TTY_SCRIPT ) ; String setupTtyScript = getScriptName ( setupTtyScriptName ) ; String cmd = sudoCmd + "" "" + setupTtyScript ; String action = ""Setup login tty for "" + nodeIP ; logger . info ( action + "" command is: "" + cmd ) ; SSHUtil sshUtil = new SSHUtil ( ) ; String errMsg = null ; for ( int i = 0 ; i < Constants . SET_PASSWORD_MAX_RETRY_TIMES ; i ++ ) { try { if ( sshUtil . execCmd ( sshUser , privateKeyFile , nodeIP , sshPort , cmd , null , null ) ) { logger . info ( action + "" succeed."" ) ; return ; } } catch ( JSchException e ) { logger . warn ( ""Got exception when "" + action , e ) ; if ( errMsg == null ) { errMsg = e . getMessage ( ) ; } } try { Thread . sleep ( 3000 ) ; } catch ( InterruptedException e1 ) { logger . info ( ""Interrupted when waiting for setup login tty, retry immediately..."" ) ; } } logger . info ( action + "" failed"" ) ; throw SetPasswordException . FAIL_TO_SETUP_LOGIN_TTY ( nodeIP , errMsg ) ; } }","logger . info ( action + "" succeed."" ) ;",Same Information
"public class A { protected static void clearReferences ( ) { Enumeration < Driver > drivers = DriverManager . getDrivers ( ) ; while ( drivers . hasMoreElements ( ) ) { Driver driver = drivers . nextElement ( ) ; if ( driver . getClass ( ) . getClassLoader ( ) == getInstance ( ) ) { try { DriverManager . deregisterDriver ( driver ) ; } catch ( SQLException e ) { log . warn ( ""SQL driver deregistration failed"" , e ) ; } } } for ( WeakReference < Class < ? > > refClazz : getInstance ( ) . cachedClasses . values ( ) ) { if ( refClazz == null ) { continue ; } Class < ? > clazz = refClazz . get ( ) ; if ( clazz != null && clazz . getName ( ) . contains ( ""openmrs"" ) ) { try { Field [ ] fields = clazz . getDeclaredFields ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { Field field = fields [ i ] ; int mods = field . getModifiers ( ) ; if ( field . getType ( ) . isPrimitive ( ) || ( field . getName ( ) . indexOf ( ""$"" ) != - 1 ) ) { continue ; } if ( Modifier . isStatic ( mods ) ) { try { if ( clazz . equals ( OpenmrsClassLoader . class ) && ""log"" . equals ( field . getName ( ) ) ) { continue ; } field . setAccessible ( true ) ; if ( Modifier . isFinal ( mods ) ) { if ( ! ( field . getType ( ) . getName ( ) . startsWith ( ""javax."" ) ) ) { nullInstance ( field . get ( null ) ) ; } } else { field . set ( null , null ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( ""Set field "" + field . getName ( ) + "" to null in class "" + clazz . getName ( ) ) ; } } } catch ( Exception t ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Exception while setting field "" + field . getName ( ) + "" to null in class "" + clazz . getName ( ) , t ) ; } } } } } catch ( Exception t ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Could not clean fields for class "" + clazz . getName ( ) , t ) ; } } } } OpenmrsClassLoader . log = null ; getInstance ( ) . cachedClasses . clear ( ) ; } }","public class A { protected static void clearReferences ( ) { Enumeration < Driver > drivers = DriverManager . getDrivers ( ) ; while ( drivers . hasMoreElements ( ) ) { Driver driver = drivers . nextElement ( ) ; if ( driver . getClass ( ) . getClassLoader ( ) == getInstance ( ) ) { try { DriverManager . deregisterDriver ( driver ) ; } catch ( SQLException e ) { log . warn ( ""SQL driver deregistration failed"" , e ) ; } } } for ( WeakReference < Class < ? > > refClazz : getInstance ( ) . cachedClasses . values ( ) ) { if ( refClazz == null ) { continue ; } Class < ? > clazz = refClazz . get ( ) ; if ( clazz != null && clazz . getName ( ) . contains ( ""openmrs"" ) ) { try { Field [ ] fields = clazz . getDeclaredFields ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { Field field = fields [ i ] ; int mods = field . getModifiers ( ) ; if ( field . getType ( ) . isPrimitive ( ) || ( field . getName ( ) . indexOf ( ""$"" ) != - 1 ) ) { continue ; } if ( Modifier . isStatic ( mods ) ) { try { if ( clazz . equals ( OpenmrsClassLoader . class ) && ""log"" . equals ( field . getName ( ) ) ) { continue ; } field . setAccessible ( true ) ; if ( Modifier . isFinal ( mods ) ) { if ( ! ( field . getType ( ) . getName ( ) . startsWith ( ""javax."" ) ) ) { nullInstance ( field . get ( null ) ) ; } } else { field . set ( null , null ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( ""Set field "" + field . getName ( ) + "" to null in class "" + clazz . getName ( ) ) ; } } } catch ( Exception t ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Could not set field "" + field . getName ( ) + "" to null in class "" + clazz . getName ( ) , t ) ; } } } } } catch ( Exception t ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Could not clean fields for class "" + clazz . getName ( ) , t ) ; } } } } OpenmrsClassLoader . log = null ; getInstance ( ) . cachedClasses . clear ( ) ; } }","log . debug ( ""Could not set field "" + field . getName ( ) + "" to null in class "" + clazz . getName ( ) , t ) ;",Same Information
"public class A { protected boolean filter ( ExcludeList avoid , StoragePool pool , DiskProfile dskCh , DeploymentPlan plan ) { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( ""Checking if storage pool is suitable, name: "" + pool . getName ( ) + "" ,poolId: "" + pool . getId ( ) ) ; } if ( avoid . shouldAvoid ( pool ) ) { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( ""StoragePool is in avoid set, skipping this pool"" ) ; } return false ; } Long clusterId = pool . getClusterId ( ) ; if ( clusterId != null ) { ClusterVO cluster = clusterDao . findById ( clusterId ) ; if ( ! ( cluster . getHypervisorType ( ) == dskCh . getHypervisorType ( ) ) ) { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( ""StoragePool does not have hypervisorType, skipping this pool"" ) ; } return false ; } } else if ( pool . getHypervisor ( ) != null && ! pool . getHypervisor ( ) . equals ( HypervisorType . Any ) && ! ( pool . getHypervisor ( ) == dskCh . getHypervisorType ( ) ) ) { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( ""StoragePool does not have required hypervisorType, skipping this pool"" ) ; } return false ; } if ( ! checkHypervisorCompatibility ( dskCh . getHypervisorType ( ) , dskCh . getType ( ) , pool . getPoolType ( ) ) ) { return false ; } Volume volume = volumeDao . findById ( dskCh . getVolumeId ( ) ) ; if ( ! storageMgr . storagePoolCompatibleWithVolumePool ( pool , volume ) ) { return false ; } if ( pool . isManaged ( ) && ! storageUtil . managedStoragePoolCanScale ( pool , plan . getClusterId ( ) , plan . getHostId ( ) ) ) { return false ; } List < Volume > requestVolumes = new ArrayList < > ( ) ; requestVolumes . add ( volume ) ; if ( dskCh . getHypervisorType ( ) == HypervisorType . VMware ) { if ( pool . getPoolType ( ) == Storage . StoragePoolType . DatastoreCluster && storageMgr . isStoragePoolDatastoreClusterParent ( pool ) ) { return false ; } if ( pool . getParent ( ) != 0L ) { StoragePoolVO datastoreCluster = storagePoolDao . findById ( pool . getParent ( ) ) ; if ( datastoreCluster == null || ( datastoreCluster != null && datastoreCluster . getStatus ( ) != StoragePoolStatus . Up ) ) { return false ; } } try { boolean isStoragePoolStoragepolicyComplaince = storageMgr . isStoragePoolComplaintWithStoragePolicy ( requestVolumes , pool ) ; if ( ! isStoragePoolStoragepolicyComplaince ) { return false ; } } catch ( StorageUnavailableException e ) { s_logger . warn ( String . format ( ""Could not verify storage policy complaince against storage pool %s due to exception %s"" , pool . getUuid ( ) , e . getMessage ( ) ) ) ; return false ; } } return storageMgr . storagePoolHasEnoughIops ( requestVolumes , pool ) && storageMgr . storagePoolHasEnoughSpace ( requestVolumes , pool , plan . getClusterId ( ) ) ; } }","public class A { protected boolean filter ( ExcludeList avoid , StoragePool pool , DiskProfile dskCh , DeploymentPlan plan ) { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( ""Checking if storage pool is suitable, name: "" + pool . getName ( ) + "" ,poolId: "" + pool . getId ( ) ) ; } if ( avoid . shouldAvoid ( pool ) ) { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( ""StoragePool is in avoid set, skipping this pool"" ) ; } return false ; } Long clusterId = pool . getClusterId ( ) ; if ( clusterId != null ) { ClusterVO cluster = clusterDao . findById ( clusterId ) ; if ( ! ( cluster . getHypervisorType ( ) == dskCh . getHypervisorType ( ) ) ) { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( ""StoragePool's Cluster does not have required hypervisorType, skipping this pool"" ) ; } return false ; } } else if ( pool . getHypervisor ( ) != null && ! pool . getHypervisor ( ) . equals ( HypervisorType . Any ) && ! ( pool . getHypervisor ( ) == dskCh . getHypervisorType ( ) ) ) { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( ""StoragePool does not have required hypervisorType, skipping this pool"" ) ; } return false ; } if ( ! checkHypervisorCompatibility ( dskCh . getHypervisorType ( ) , dskCh . getType ( ) , pool . getPoolType ( ) ) ) { return false ; } Volume volume = volumeDao . findById ( dskCh . getVolumeId ( ) ) ; if ( ! storageMgr . storagePoolCompatibleWithVolumePool ( pool , volume ) ) { return false ; } if ( pool . isManaged ( ) && ! storageUtil . managedStoragePoolCanScale ( pool , plan . getClusterId ( ) , plan . getHostId ( ) ) ) { return false ; } List < Volume > requestVolumes = new ArrayList < > ( ) ; requestVolumes . add ( volume ) ; if ( dskCh . getHypervisorType ( ) == HypervisorType . VMware ) { if ( pool . getPoolType ( ) == Storage . StoragePoolType . DatastoreCluster && storageMgr . isStoragePoolDatastoreClusterParent ( pool ) ) { return false ; } if ( pool . getParent ( ) != 0L ) { StoragePoolVO datastoreCluster = storagePoolDao . findById ( pool . getParent ( ) ) ; if ( datastoreCluster == null || ( datastoreCluster != null && datastoreCluster . getStatus ( ) != StoragePoolStatus . Up ) ) { return false ; } } try { boolean isStoragePoolStoragepolicyComplaince = storageMgr . isStoragePoolComplaintWithStoragePolicy ( requestVolumes , pool ) ; if ( ! isStoragePoolStoragepolicyComplaince ) { return false ; } } catch ( StorageUnavailableException e ) { s_logger . warn ( String . format ( ""Could not verify storage policy complaince against storage pool %s due to exception %s"" , pool . getUuid ( ) , e . getMessage ( ) ) ) ; return false ; } } return storageMgr . storagePoolHasEnoughIops ( requestVolumes , pool ) && storageMgr . storagePoolHasEnoughSpace ( requestVolumes , pool , plan . getClusterId ( ) ) ; } }","s_logger . debug ( ""StoragePool's Cluster does not have required hypervisorType, skipping this pool"" ) ;",Same Information
"public class A { @ Override public IBond getBond ( IAtom atom1 , IAtom atom2 ) { logger . debug ( ""Getting bond: atom1="" + atom1 , "" atom2="" + atom2 ) ; return super . getBond ( atom1 , atom2 ) ; } }","public class A { @ Override public IBond getBond ( IAtom atom1 , IAtom atom2 ) { logger . debug ( ""Getting bond for atoms: atom1="" + atom1 , "" atom2="" + atom2 ) ; return super . getBond ( atom1 , atom2 ) ; } }","logger . debug ( ""Getting bond for atoms: atom1="" + atom1 , "" atom2="" + atom2 ) ;",Same Information
"public class A { private static void createFileIfRequired ( String csvFile ) throws IOException { try { File file = new File ( csvFile ) ; if ( file . createNewFile ( ) ) { logger . debug ( ""File created: {}"" , file . getName ( ) ) ; final boolean append = true ; BufferedWriter headerWriter = new BufferedWriter ( new OutputStreamWriter ( new FileOutputStream ( csvFile , append ) , StandardCharsets . UTF_8 ) ) ; final String headerToAppend = ""ContainerId,Timestamp[uts],ReceivedMessages[msgs/sec],SentMessages[msgs/sec]"" ; headerWriter . write ( headerToAppend ) ; headerWriter . newLine ( ) ; headerWriter . close ( ) ; } else { logger . debug ( ""File already exists. Writing data to: {}"" , csvFile ) ; } } catch ( IOException e ) { logger . error ( ""An error occurred while creating csv file."" ) ; e . printStackTrace ( ) ; throw new IOException ( ""An error occurred while creating csv file: can not continue"" ) ; } } }","public class A { private static void createFileIfRequired ( String csvFile ) throws IOException { try { File file = new File ( csvFile ) ; if ( file . createNewFile ( ) ) { logger . debug ( ""File created: {}"" , file . getName ( ) ) ; final boolean append = true ; BufferedWriter headerWriter = new BufferedWriter ( new OutputStreamWriter ( new FileOutputStream ( csvFile , append ) , StandardCharsets . UTF_8 ) ) ; final String headerToAppend = ""ContainerId,Timestamp[uts],ReceivedMessages[msgs/sec],SentMessages[msgs/sec]"" ; headerWriter . write ( headerToAppend ) ; headerWriter . newLine ( ) ; headerWriter . close ( ) ; } else { logger . debug ( ""File already exists. Writting data to: {}"" , csvFile ) ; } } catch ( IOException e ) { logger . error ( ""An error occurred while creating csv file."" ) ; e . printStackTrace ( ) ; throw new IOException ( ""An error occurred while creating csv file: can not continue"" ) ; } } }","logger . debug ( ""File already exists. Writting data to: {}"" , csvFile ) ;",Same Information
"public class A { private static boolean _verifyProcess ( String verifyProcessClassName ) throws VerifyException { if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Initializing verification "" + verifyProcessClassName ) ; } try { Class < ? > clazz = Class . forName ( verifyProcessClassName ) ; VerifyProcess verifyProcess = ( VerifyProcess ) clazz . newInstance ( ) ; if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Running verification "" + verifyProcessClassName ) ; } verifyProcess . verify ( ) ; if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Finished verification "" + verifyProcessClassName ) ; } return true ; } catch ( ClassNotFoundException classNotFoundException ) { _log . error ( verifyProcessClassName + "" cannot be found"" , classNotFoundException ) ; } catch ( IllegalAccessException illegalAccessException ) { _log . error ( verifyProcessClassName + "" cannot be accessed"" , illegalAccessException ) ; } catch ( InstantiationException instantiationException ) { _log . error ( verifyProcessClassName + "" cannot be instantiated"" , instantiationException ) ; } return false ; } }","public class A { private static boolean _verifyProcess ( String verifyProcessClassName ) throws VerifyException { if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Initializing verification "" + verifyProcessClassName ) ; } try { Class < ? > clazz = Class . forName ( verifyProcessClassName ) ; VerifyProcess verifyProcess = ( VerifyProcess ) clazz . newInstance ( ) ; if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Running verification "" + verifyProcessClassName ) ; } verifyProcess . verify ( ) ; if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Finished verification "" + verifyProcessClassName ) ; } return true ; } catch ( ClassNotFoundException classNotFoundException ) { _log . error ( verifyProcessClassName + "" cannot be found"" , classNotFoundException ) ; } catch ( IllegalAccessException illegalAccessException ) { _log . error ( verifyProcessClassName + "" cannot be accessed"" , illegalAccessException ) ; } catch ( InstantiationException instantiationException ) { _log . error ( verifyProcessClassName + "" cannot be initiated"" , instantiationException ) ; } return false ; } }","_log . error ( verifyProcessClassName + "" cannot be initiated"" , instantiationException ) ;",Same Information
"public class A { @ SuppressWarnings ( { ""unchecked"" } ) public Object doSendCommandStart ( Object [ ] args ) { Command cmd = ( Command ) args [ 0 ] ; String host = ( String ) args [ 2 ] ; Integer port = ( Integer ) args [ 3 ] ; String targetURL = ""redis://"" + host + "":"" + port ; String redisAction = cmd . name ( ) ; if ( logger . isDebugable ( ) ) { logger . debug ( ""REDIS INVOKE START: "" + targetURL + "", action: "" + redisAction , redisAction ) ; } Map < String , Object > params = new HashMap < String , Object > ( ) ; params . put ( CaptureConstants . INFO_CLIENT_REQUEST_URL , targetURL ) ; params . put ( CaptureConstants . INFO_CLIENT_REQUEST_ACTION , redisAction ) ; params . put ( CaptureConstants . INFO_CLIENT_APPID , appid ) ; params . put ( CaptureConstants . INFO_CLIENT_TYPE , ""redis.client.Jedis"" ) ; UAVServer . instance ( ) . runMonitorCaptureOnServerCapPoint ( CaptureConstants . CAPPOINT_APP_CLIENT , Monitor . CapturePhase . PRECAP , params ) ; UAVServer . instance ( ) . runSupporter ( ""com.creditease.uav.apm.supporters.InvokeChainSupporter"" , ""registerAdapter"" , JedisClientAdapter . class ) ; ivcContextParams = ( Map < String , Object > ) UAVServer . instance ( ) . runSupporter ( ""com.creditease.uav.apm.supporters.InvokeChainSupporter"" , ""runCap"" , InvokeChainConstants . CHAIN_APP_CLIENT , InvokeChainConstants . CapturePhase . PRECAP , params , JedisClientAdapter . class , args ) ; return null ; } }","public class A { @ SuppressWarnings ( { ""unchecked"" } ) public Object doSendCommandStart ( Object [ ] args ) { Command cmd = ( Command ) args [ 0 ] ; String host = ( String ) args [ 2 ] ; Integer port = ( Integer ) args [ 3 ] ; String targetURL = ""redis://"" + host + "":"" + port ; String redisAction = cmd . name ( ) ; if ( logger . isDebugable ( ) ) { logger . debug ( ""REDIS INVOKE START: "" + targetURL + "" action: "" + redisAction , null ) ; } Map < String , Object > params = new HashMap < String , Object > ( ) ; params . put ( CaptureConstants . INFO_CLIENT_REQUEST_URL , targetURL ) ; params . put ( CaptureConstants . INFO_CLIENT_REQUEST_ACTION , redisAction ) ; params . put ( CaptureConstants . INFO_CLIENT_APPID , appid ) ; params . put ( CaptureConstants . INFO_CLIENT_TYPE , ""redis.client.Jedis"" ) ; UAVServer . instance ( ) . runMonitorCaptureOnServerCapPoint ( CaptureConstants . CAPPOINT_APP_CLIENT , Monitor . CapturePhase . PRECAP , params ) ; UAVServer . instance ( ) . runSupporter ( ""com.creditease.uav.apm.supporters.InvokeChainSupporter"" , ""registerAdapter"" , JedisClientAdapter . class ) ; ivcContextParams = ( Map < String , Object > ) UAVServer . instance ( ) . runSupporter ( ""com.creditease.uav.apm.supporters.InvokeChainSupporter"" , ""runCap"" , InvokeChainConstants . CHAIN_APP_CLIENT , InvokeChainConstants . CapturePhase . PRECAP , params , JedisClientAdapter . class , args ) ; return null ; } }","logger . debug ( ""REDIS INVOKE START: "" + targetURL + "" action: "" + redisAction , null ) ;",Same Information
"public class A { protected InputStream getInputStream ( String profileId ) throws IOException { final String profileUrl = registryBaseURl + profileId + ""/xml"" ; log . debug ( ""Connecting to {}"" , profileUrl ) ; return new URL ( profileUrl ) . openStream ( ) ; } }","public class A { protected InputStream getInputStream ( String profileId ) throws IOException { final String profileUrl = registryBaseURl + profileId + ""/xml"" ; LOG . debug ( ""Opening input stream at {}"" , profileUrl ) ; return new URL ( profileUrl ) . openStream ( ) ; } }","LOG . debug ( ""Opening input stream at {}"" , profileUrl ) ;",Same Information
"public class A { public static PlanInstance createPlanInstance ( final Csar csar , final QName serviceTemplateId , final long serviceTemplateInstanceId , final QName planId , final String operationName , final String correlationId , final String chorCorrelationId , final String choreographyPartners , final Object input ) throws CorrelationIdAlreadySetException { if ( Objects . isNull ( planId ) ) { LOG . error ( ""No plan ID provided!"" ) ; return null ; } final TPlan storedPlan ; try { storedPlan = ToscaEngine . resolvePlanReference ( csar , planId ) ; } catch ( NotFoundException e ) { LOG . error ( ""Plan with ID {} does not exist in CSAR {}!"" , planId , csar . id ( ) . csarName ( ) ) ; return null ; } final PlanInstance plan = new PlanInstance ( ) ; plan . setCorrelationId ( correlationId ) ; plan . setChoreographyCorrelationId ( chorCorrelationId ) ; plan . setChoreographyPartners ( choreographyPartners ) ; plan . setLanguage ( PlanLanguage . fromString ( storedPlan . getPlanLanguage ( ) ) ) ; plan . setType ( PlanType . fromString ( storedPlan . getPlanType ( ) ) ) ; plan . setState ( PlanInstanceState . RUNNING ) ; plan . setTemplateId ( planId ) ; final Optional < PlanInstance > planOptional = planRepo . findAll ( ) . stream ( ) . filter ( p -> p . getCorrelationId ( ) . equals ( correlationId ) ) . findFirst ( ) ; if ( planOptional . isPresent ( ) ) { throw new CorrelationIdAlreadySetException ( ""Plan instance with correlation ID "" + correlationId + "" is already existing."" ) ; } Map < String , String > inputMap = new HashMap < > ( ) ; if ( input instanceof HashMap ) { inputMap = ( HashMap < String , String > ) input ; } for ( final TParameter param : storedPlan . getInputParameters ( ) . getInputParameter ( ) ) { new PlanInstanceInput ( param . getName ( ) , inputMap . getOrDefault ( param . getName ( ) , """" ) , param . getType ( ) ) . setPlanInstance ( plan ) ; } stiRepo . find ( serviceTemplateInstanceId ) . ifPresent ( serviceTemplateInstance -> plan . setServiceTemplateInstance ( serviceTemplateInstance ) ) ; planRepo . add ( plan ) ; return plan ; } }","public class A { public static PlanInstance createPlanInstance ( final Csar csar , final QName serviceTemplateId , final long serviceTemplateInstanceId , final QName planId , final String operationName , final String correlationId , final String chorCorrelationId , final String choreographyPartners , final Object input ) throws CorrelationIdAlreadySetException { if ( Objects . isNull ( planId ) ) { LOG . error ( ""Plan ID is null! Unable to create PlanInstance!"" ) ; return null ; } final TPlan storedPlan ; try { storedPlan = ToscaEngine . resolvePlanReference ( csar , planId ) ; } catch ( NotFoundException e ) { LOG . error ( ""Plan with ID {} does not exist in CSAR {}!"" , planId , csar . id ( ) . csarName ( ) ) ; return null ; } final PlanInstance plan = new PlanInstance ( ) ; plan . setCorrelationId ( correlationId ) ; plan . setChoreographyCorrelationId ( chorCorrelationId ) ; plan . setChoreographyPartners ( choreographyPartners ) ; plan . setLanguage ( PlanLanguage . fromString ( storedPlan . getPlanLanguage ( ) ) ) ; plan . setType ( PlanType . fromString ( storedPlan . getPlanType ( ) ) ) ; plan . setState ( PlanInstanceState . RUNNING ) ; plan . setTemplateId ( planId ) ; final Optional < PlanInstance > planOptional = planRepo . findAll ( ) . stream ( ) . filter ( p -> p . getCorrelationId ( ) . equals ( correlationId ) ) . findFirst ( ) ; if ( planOptional . isPresent ( ) ) { throw new CorrelationIdAlreadySetException ( ""Plan instance with correlation ID "" + correlationId + "" is already existing."" ) ; } Map < String , String > inputMap = new HashMap < > ( ) ; if ( input instanceof HashMap ) { inputMap = ( HashMap < String , String > ) input ; } for ( final TParameter param : storedPlan . getInputParameters ( ) . getInputParameter ( ) ) { new PlanInstanceInput ( param . getName ( ) , inputMap . getOrDefault ( param . getName ( ) , """" ) , param . getType ( ) ) . setPlanInstance ( plan ) ; } stiRepo . find ( serviceTemplateInstanceId ) . ifPresent ( serviceTemplateInstance -> plan . setServiceTemplateInstance ( serviceTemplateInstance ) ) ; planRepo . add ( plan ) ; return plan ; } }","LOG . error ( ""Plan ID is null! Unable to create PlanInstance!"" ) ;",Same Information
"public class A { @ Override public void receivedValue ( Destination type , String dataId , Object object , List < DataRequest > achievedRequests ) { if ( type == Transfer . Destination . OBJECT ) { WORKER_LOGGER . info ( ""Received data "" + dataId + "" with object "" + object ) ; this . dataManager . storeValue ( dataId , object ) ; } else { String nameId = ( new File ( dataId ) ) . getName ( ) ; WORKER_LOGGER . info ( ""Received data "" + nameId + "" with path "" + dataId ) ; this . dataManager . storeFile ( nameId , dataId ) ; } for ( DataRequest dr : achievedRequests ) { WorkerDataRequest wdr = ( WorkerDataRequest ) dr ; wdr . getListener ( ) . fetchedValue ( dataId ) ; if ( NIOTracer . extraeEnabled ( ) ) { NIOTracer . emitDataTransferEvent ( NIOTracer . TRANSFER_END ) ; } if ( WORKER_LOGGER_DEBUG ) { WORKER_LOGGER . debug ( ""Pending parameters: "" + ( ( MultiOperationFetchListener ) wdr . getListener ( ) ) . getMissingOperations ( ) ) ; } } } }","public class A { @ Override public void receivedValue ( Destination type , String dataId , Object object , List < DataRequest > achievedRequests ) { if ( type == Transfer . Destination . OBJECT ) { WORKER_LOGGER . info ( ""Received data "" + dataId + "" with associated object "" + object ) ; this . dataManager . storeValue ( dataId , object ) ; } else { String nameId = ( new File ( dataId ) ) . getName ( ) ; WORKER_LOGGER . info ( ""Received data "" + nameId + "" with path "" + dataId ) ; this . dataManager . storeFile ( nameId , dataId ) ; } for ( DataRequest dr : achievedRequests ) { WorkerDataRequest wdr = ( WorkerDataRequest ) dr ; wdr . getListener ( ) . fetchedValue ( dataId ) ; if ( NIOTracer . extraeEnabled ( ) ) { NIOTracer . emitDataTransferEvent ( NIOTracer . TRANSFER_END ) ; } if ( WORKER_LOGGER_DEBUG ) { WORKER_LOGGER . debug ( ""Pending parameters: "" + ( ( MultiOperationFetchListener ) wdr . getListener ( ) ) . getMissingOperations ( ) ) ; } } } }","WORKER_LOGGER . info ( ""Received data "" + dataId + "" with associated object "" + object ) ;",Same Information
"public class A { @ Verify public void globalVerify ( ) { ICacheOperationCounter total = new ICacheOperationCounter ( ) ; for ( ICacheOperationCounter counter : results ) { total . add ( counter ) ; } logger . info ( name + "": "" + total + "" from "" + results . size ( ) + "" worker Threads"" ) ; } }","public class A { @ Verify public void globalVerify ( ) { ICacheOperationCounter total = new ICacheOperationCounter ( ) ; for ( ICacheOperationCounter counter : results ) { total . add ( counter ) ; } logger . info ( name + "": "" + total + "" from "" + results . size ( ) + "" worker threads"" ) ; } }","logger . info ( name + "": "" + total + "" from "" + results . size ( ) + "" worker threads"" ) ;",Same Information
"public class A { public void cut ( ) { try { ( ( XulWindow ) this . getXulDomContainer ( ) . getDocumentRoot ( ) . getRootElement ( ) ) . cut ( ) ; paste . setDisabled ( false ) ; } catch ( XulException e ) { logger . error ( ""Failed to cut window"" , e ) ; } } }","public class A { public void cut ( ) { try { ( ( XulWindow ) this . getXulDomContainer ( ) . getDocumentRoot ( ) . getRootElement ( ) ) . cut ( ) ; paste . setDisabled ( false ) ; } catch ( XulException e ) { logger . error ( e . getMessage ( ) , e ) ; } } }","logger . error ( e . getMessage ( ) , e ) ;",Same Information
"public class A { public static void generateUDTAtRuntime ( final Session session , AbstractUDTClassProperty < ? > udtClassProperty ) { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( format ( ""Generating schema for udt of type %s"" , udtClassProperty . udtClass . getCanonicalName ( ) ) ) ; } udtClassProperty . componentsProperty . stream ( ) . flatMap ( x -> x . getUDTClassProperties ( ) . stream ( ) ) . forEach ( x -> generateUDTAtRuntime ( session , x ) ) ; final String udtKeyspace = udtClassProperty . staticKeyspace . orElseGet ( session :: getLoggedKeyspace ) ; final SchemaContext schemaContext = new SchemaContext ( udtKeyspace , true , true ) ; final String udtSchema = udtClassProperty . generateSchema ( schemaContext ) ; if ( ACHILLES_DML_LOGGER . isDebugEnabled ( ) ) { ACHILLES_DML_LOGGER . debug ( format ( ""Generating schema for %s"" , udtSchema ) ) ; } final ResultSet resultSet = session . execute ( udtSchema ) ; resultSet . getExecutionInfo ( ) . isSchemaInAgreement ( ) ; } }","public class A { public static void generateUDTAtRuntime ( final Session session , AbstractUDTClassProperty < ? > udtClassProperty ) { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( format ( ""Generating schema for udt of type %s"" , udtClassProperty . udtClass . getCanonicalName ( ) ) ) ; } udtClassProperty . componentsProperty . stream ( ) . flatMap ( x -> x . getUDTClassProperties ( ) . stream ( ) ) . forEach ( x -> generateUDTAtRuntime ( session , x ) ) ; final String udtKeyspace = udtClassProperty . staticKeyspace . orElseGet ( session :: getLoggedKeyspace ) ; final SchemaContext schemaContext = new SchemaContext ( udtKeyspace , true , true ) ; final String udtSchema = udtClassProperty . generateSchema ( schemaContext ) ; if ( ACHILLES_DML_LOGGER . isDebugEnabled ( ) ) { ACHILLES_DML_LOGGER . debug ( udtSchema + ""\n"" ) ; } final ResultSet resultSet = session . execute ( udtSchema ) ; resultSet . getExecutionInfo ( ) . isSchemaInAgreement ( ) ; } }","ACHILLES_DML_LOGGER . debug ( udtSchema + ""\n"" ) ;",Same Information
"public class A { @ Override public void jvmRouteSwitchover ( String fromJvmRoute , String toJvmRoute ) { try { Node oldNode = getBalancerContext ( ) . jvmRouteToSipNode . get ( fromJvmRoute ) ; Node newNode = getBalancerContext ( ) . jvmRouteToSipNode . get ( toJvmRoute ) ; if ( oldNode != null && newNode != null ) { int updatedRoutes = 0 ; for ( String key : userToMap . keySet ( ) ) { Node n = userToMap . get ( key ) ; if ( n . equals ( oldNode ) ) { userToMap . replace ( key , newNode ) ; updatedRoutes ++ ; } } if ( logger . isInfoEnabled ( ) ) { logger . info ( ""Switchover occured where fromJvmRoute="" + fromJvmRoute + "" and toJvmRoute="" + toJvmRoute + "" with "" + updatedRoutes + "" updated routes."" ) ; } } else { if ( logger . isInfoEnabled ( ) ) { logger . info ( ""Switchover failed where fromJvmRoute="" + fromJvmRoute + "" and toJvmRoute="" + toJvmRoute ) ; } } } catch ( Throwable t ) { if ( logger . isInfoEnabled ( ) ) { logger . info ( ""Switchover failed where fromJvmRoute="" + fromJvmRoute + "" and toJvmRoute="" + toJvmRoute ) ; logger . info ( ""This is not a fatal failure, logging the reason for the reason: "" + t . getMessage ( ) , t ) ; } } } }","public class A { @ Override public void jvmRouteSwitchover ( String fromJvmRoute , String toJvmRoute ) { try { Node oldNode = getBalancerContext ( ) . jvmRouteToSipNode . get ( fromJvmRoute ) ; Node newNode = getBalancerContext ( ) . jvmRouteToSipNode . get ( toJvmRoute ) ; if ( oldNode != null && newNode != null ) { int updatedRoutes = 0 ; for ( String key : userToMap . keySet ( ) ) { Node n = userToMap . get ( key ) ; if ( n . equals ( oldNode ) ) { userToMap . replace ( key , newNode ) ; updatedRoutes ++ ; } } if ( logger . isInfoEnabled ( ) ) { logger . info ( ""Switchover occured where fromJvmRoute="" + fromJvmRoute + "" and toJvmRoute="" + toJvmRoute + "" with "" + updatedRoutes + "" updated routes."" ) ; } } else { if ( logger . isInfoEnabled ( ) ) { logger . info ( ""Switchover failed where fromJvmRoute="" + fromJvmRoute + "" and toJvmRoute="" + toJvmRoute ) ; } } } catch ( Throwable t ) { if ( logger . isInfoEnabled ( ) ) { logger . info ( ""Switchover failed where fromJvmRoute="" + fromJvmRoute + "" and toJvmRoute="" + toJvmRoute ) ; logger . info ( ""This is not a fatal failure, logging the reason for the failure "" , t ) ; } } } }","logger . info ( ""This is not a fatal failure, logging the reason for the failure "" , t ) ;",Same Information
"public class A { public String saveContent ( String containerId , String userId , Number taskId , String payload , String marshallingType ) { userId = getUser ( userId ) ; containerId = context . getContainerId ( containerId , new ByTaskIdContainerLocator ( taskId . longValue ( ) ) ) ; logger . debug ( ""About to unmarshal task outcome parameters from payload: '{}'"" , payload ) ; Map < String , Object > parameters = marshallerHelper . unmarshal ( containerId , payload , marshallingType , Map . class ) ; logger . debug ( ""About to set content of a task with id '{}' with data {}"" , taskId , parameters ) ; Long contentId = userTaskService . saveContentFromUser ( taskId . longValue ( ) , userId , parameters ) ; String response = marshallerHelper . marshal ( containerId , marshallingType , contentId ) ; return response ; } }","public class A { public String saveContent ( String containerId , String userId , Number taskId , String payload , String marshallingType ) { userId = getUser ( userId ) ; containerId = context . getContainerId ( containerId , new ByTaskIdContainerLocator ( taskId . longValue ( ) ) ) ; logger . debug ( ""About to unmarshal task content parameters from payload: '{}'"" , payload ) ; Map < String , Object > parameters = marshallerHelper . unmarshal ( containerId , payload , marshallingType , Map . class ) ; logger . debug ( ""About to set content of a task with id '{}' with data {}"" , taskId , parameters ) ; Long contentId = userTaskService . saveContentFromUser ( taskId . longValue ( ) , userId , parameters ) ; String response = marshallerHelper . marshal ( containerId , marshallingType , contentId ) ; return response ; } }","logger . debug ( ""About to unmarshal task content parameters from payload: '{}'"" , payload ) ;",Same Information
"public class A { public int getButton ( int integrationID , int component ) { synchronized ( deviceButtonMapLock ) { if ( deviceButtonMap != null ) { List < Integer > buttonList = deviceButtonMap . get ( integrationID ) ; if ( buttonList != null && component <= buttonList . size ( ) ) { return buttonList . get ( component - 1 ) ; } else { logger . debug ( ""Could not find button component {} for id {}"" , component , integrationID ) ; return 0 ; } } else { logger . debug ( ""No button found for integration ID {}"" , integrationID ) ; return 0 ; } } } }","public class A { public int getButton ( int integrationID , int component ) { synchronized ( deviceButtonMapLock ) { if ( deviceButtonMap != null ) { List < Integer > buttonList = deviceButtonMap . get ( integrationID ) ; if ( buttonList != null && component <= buttonList . size ( ) ) { return buttonList . get ( component - 1 ) ; } else { logger . debug ( ""Could not find button component {} for id {}"" , component , integrationID ) ; return 0 ; } } else { logger . debug ( ""Device to button map not populated"" ) ; return 0 ; } } } }","logger . debug ( ""Device to button map not populated"" ) ;",Same Information
"public class A { public synchronized HttpConnection getFreeConnection ( HostConfiguration hostConfiguration ) { HttpConnectionWithReference connection = null ; HostConnectionPool hostPool = getHostPool ( hostConfiguration , false ) ; if ( ( hostPool != null ) && ( hostPool . freeConnections . size ( ) > 0 ) ) { connection = ( HttpConnectionWithReference ) hostPool . freeConnections . removeLast ( ) ; freeConnections . remove ( connection ) ; storeReferenceToConnection ( connection , hostConfiguration , this ) ; if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""Free connection to host "" + hostConfiguration ) ; } idleConnectionHandler . remove ( connection ) ; } else if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""There were no free connections to get, hostConfig="" + hostConfiguration ) ; } return connection ; } }","public class A { public synchronized HttpConnection getFreeConnection ( HostConfiguration hostConfiguration ) { HttpConnectionWithReference connection = null ; HostConnectionPool hostPool = getHostPool ( hostConfiguration , false ) ; if ( ( hostPool != null ) && ( hostPool . freeConnections . size ( ) > 0 ) ) { connection = ( HttpConnectionWithReference ) hostPool . freeConnections . removeLast ( ) ; freeConnections . remove ( connection ) ; storeReferenceToConnection ( connection , hostConfiguration , this ) ; if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""Getting free connection, hostConfig="" + hostConfiguration ) ; } idleConnectionHandler . remove ( connection ) ; } else if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""There were no free connections to get, hostConfig="" + hostConfiguration ) ; } return connection ; } }","LOG . debug ( ""Getting free connection, hostConfig="" + hostConfiguration ) ;",Same Information
"public class A { private boolean requestAnalyze ( long scheduleID , RDBAnalyze rdbAnalyze , AnalyzeInstance analyzeInstance , Map < String , Set < String > > generateRule , int [ ] analyzer ) { String host = analyzeInstance . getHost ( ) ; String url = ""http://"" + analyzeInstance . getHost ( ) + "":"" + analyzeInstance . getPort ( ) + ""/receivedSchedule"" ; ScheduleInfo scheduleInfo = new ScheduleInfo ( scheduleID , rdbAnalyze . getDataPath ( ) , rdbAnalyze . getPrefixes ( ) , generateRule . get ( host ) , analyzer ) ; HttpHeaders headers = new HttpHeaders ( ) ; headers . add ( ""Content-Type"" , ""application/json"" ) ; HttpEntity < ScheduleInfo > httpEntity = new HttpEntity < ScheduleInfo > ( scheduleInfo , headers ) ; boolean status = Boolean . FALSE ; try { ResponseEntity < String > executeResult = restTemplate . postForEntity ( url , httpEntity , String . class ) ; JSONObject responseMessage = JSONObject . parseObject ( executeResult . getBody ( ) ) ; if ( ! responseMessage . getBooleanValue ( ""checked"" ) ) { LOG . error ( ""allocation {} scheduleJob response error. responseMessage:{}"" , analyzeInstance . toString ( ) , responseMessage . toJSONString ( ) ) ; deleteResult ( rdbAnalyze , scheduleID ) ; } else { status = Boolean . TRUE ; } } catch ( Exception e ) { LOG . error ( ""allocation {} scheduleJob error. {}"" , analyzeInstance . toString ( ) , e . getMessage ( ) ) ; deleteResult ( rdbAnalyze , scheduleID ) ; } Set < String > ports = generateRule . get ( host ) ; for ( String port : ports ) { if ( status ) { updateCurrentAnalyzerStatus ( rdbAnalyze . getId ( ) , scheduleID , host , port , 0 , status , AnalyzeStatus . CHECKING ) ; } else { updateCurrentAnalyzerStatus ( rdbAnalyze . getId ( ) , scheduleID , host , port , 0 , status , AnalyzeStatus . ERROR ) ; } } return status ; } }","public class A { private boolean requestAnalyze ( long scheduleID , RDBAnalyze rdbAnalyze , AnalyzeInstance analyzeInstance , Map < String , Set < String > > generateRule , int [ ] analyzer ) { String host = analyzeInstance . getHost ( ) ; String url = ""http://"" + analyzeInstance . getHost ( ) + "":"" + analyzeInstance . getPort ( ) + ""/receivedSchedule"" ; ScheduleInfo scheduleInfo = new ScheduleInfo ( scheduleID , rdbAnalyze . getDataPath ( ) , rdbAnalyze . getPrefixes ( ) , generateRule . get ( host ) , analyzer ) ; HttpHeaders headers = new HttpHeaders ( ) ; headers . add ( ""Content-Type"" , ""application/json"" ) ; HttpEntity < ScheduleInfo > httpEntity = new HttpEntity < ScheduleInfo > ( scheduleInfo , headers ) ; boolean status = Boolean . FALSE ; try { ResponseEntity < String > executeResult = restTemplate . postForEntity ( url , httpEntity , String . class ) ; JSONObject responseMessage = JSONObject . parseObject ( executeResult . getBody ( ) ) ; if ( ! responseMessage . getBooleanValue ( ""checked"" ) ) { LOG . error ( ""allocation {} scheduleJob response error. responseMessage:{}"" , analyzeInstance . toString ( ) , responseMessage . toJSONString ( ) ) ; deleteResult ( rdbAnalyze , scheduleID ) ; } else { status = Boolean . TRUE ; } } catch ( Exception e ) { LOG . error ( ""allocation {} scheduleJob fail. "" , analyzeInstance . toString ( ) , e ) ; deleteResult ( rdbAnalyze , scheduleID ) ; } Set < String > ports = generateRule . get ( host ) ; for ( String port : ports ) { if ( status ) { updateCurrentAnalyzerStatus ( rdbAnalyze . getId ( ) , scheduleID , host , port , 0 , status , AnalyzeStatus . CHECKING ) ; } else { updateCurrentAnalyzerStatus ( rdbAnalyze . getId ( ) , scheduleID , host , port , 0 , status , AnalyzeStatus . ERROR ) ; } } return status ; } }","LOG . error ( ""allocation {} scheduleJob fail. "" , analyzeInstance . toString ( ) , e ) ;",Same Information
"public class A { @ Override public CEFParserResult evaluate ( FunctionArgs args , EvaluationContext context ) { final String cef = valueParam . required ( args , context ) ; final boolean useFullNames = useFullNamesParam . optional ( args , context ) . orElse ( false ) ; final CEFParser parser = CEFParserFactory . create ( ) ; if ( cef == null || cef . isEmpty ( ) ) { LOG . debug ( ""NULL or empty parameter passed to CEF parser function. Not evaluating."" ) ; return null ; } LOG . debug ( ""Running CEF parser for [{}]."" , cef ) ; final MappedMessage message ; try ( Timer . Context timer = parseTime . time ( ) ) { message = new MappedMessage ( parser . parse ( cef . trim ( ) ) , useFullNames ) ; } catch ( Exception e ) { LOG . error ( ""Unable to map message [{}]."" , cef , e ) ; return null ; } final Map < String , Object > fields = new HashMap < > ( ) ; fields . put ( ""cef_version"" , message . cefVersion ( ) ) ; fields . put ( ""device_vendor"" , message . deviceVendor ( ) ) ; fields . put ( ""device_product"" , message . deviceProduct ( ) ) ; fields . put ( ""device_version"" , message . deviceVersion ( ) ) ; fields . put ( ""device_event_class_id"" , message . deviceEventClassId ( ) ) ; fields . put ( ""name"" , message . name ( ) ) ; fields . put ( ""severity"" , message . severity ( ) ) ; fields . putAll ( message . mappedExtensions ( ) ) ; return new CEFParserResult ( fields ) ; } }","public class A { @ Override public CEFParserResult evaluate ( FunctionArgs args , EvaluationContext context ) { final String cef = valueParam . required ( args , context ) ; final boolean useFullNames = useFullNamesParam . optional ( args , context ) . orElse ( false ) ; final CEFParser parser = CEFParserFactory . create ( ) ; if ( cef == null || cef . isEmpty ( ) ) { LOG . debug ( ""NULL or empty parameter passed to CEF parser function. Not evaluating."" ) ; return null ; } LOG . debug ( ""Running CEF parser for [{}]."" , cef ) ; final MappedMessage message ; try ( Timer . Context timer = parseTime . time ( ) ) { message = new MappedMessage ( parser . parse ( cef . trim ( ) ) , useFullNames ) ; } catch ( Exception e ) { LOG . error ( ""Error while parsing CEF message: {}"" , cef , e ) ; return null ; } final Map < String , Object > fields = new HashMap < > ( ) ; fields . put ( ""cef_version"" , message . cefVersion ( ) ) ; fields . put ( ""device_vendor"" , message . deviceVendor ( ) ) ; fields . put ( ""device_product"" , message . deviceProduct ( ) ) ; fields . put ( ""device_version"" , message . deviceVersion ( ) ) ; fields . put ( ""device_event_class_id"" , message . deviceEventClassId ( ) ) ; fields . put ( ""name"" , message . name ( ) ) ; fields . put ( ""severity"" , message . severity ( ) ) ; fields . putAll ( message . mappedExtensions ( ) ) ; return new CEFParserResult ( fields ) ; } }","LOG . error ( ""Error while parsing CEF message: {}"" , cef , e ) ;",Same Information
"public class A { private void removePeerMSPIDMap ( final Peer peer ) { Properties properties = peer . getProperties ( ) ; if ( null != properties ) { final String mspid = properties . getProperty ( Peer . PEER_ORGANIZATION_MSPID_PROPERTY ) ; if ( ! isNullOrEmpty ( mspid ) ) { logger . debug ( format ( ""Channel %s mapping peer %s to mspid %s"" , name , peer , mspid ) ) ; synchronized ( peerMSPIDMap ) { final Collection < Peer > peers = peerMSPIDMap . get ( mspid ) ; if ( peers != null ) { peers . remove ( peer ) ; if ( peers . isEmpty ( ) ) { peerMSPIDMap . remove ( mspid ) ; } } } } } } }","public class A { private void removePeerMSPIDMap ( final Peer peer ) { Properties properties = peer . getProperties ( ) ; if ( null != properties ) { final String mspid = properties . getProperty ( Peer . PEER_ORGANIZATION_MSPID_PROPERTY ) ; if ( ! isNullOrEmpty ( mspid ) ) { logger . debug ( format ( ""Channel %s removing mapping peer %s to mspid %s"" , name , peer , mspid ) ) ; synchronized ( peerMSPIDMap ) { final Collection < Peer > peers = peerMSPIDMap . get ( mspid ) ; if ( peers != null ) { peers . remove ( peer ) ; if ( peers . isEmpty ( ) ) { peerMSPIDMap . remove ( mspid ) ; } } } } } } }","logger . debug ( format ( ""Channel %s removing mapping peer %s to mspid %s"" , name , peer , mspid ) ) ;",Same Information
"public class A { public void testMapSubkeyUsage ( ) throws Exception { entity . config ( ) . set ( TestEntity . CONF_MAP_THING_OBJECT . subKey ( ""a"" ) , 1 ) ; log . info ( ""Map-subkey: "" + MutableMap . copyOf ( entity . getConfigMap ( ) . asMapWithStringKeys ( ) ) ) ; Assert . assertEquals ( entity . getConfig ( TestEntity . CONF_MAP_THING_OBJECT ) , ImmutableMap . < String , Object > of ( ""a"" , 1 ) ) ; } }","public class A { public void testMapSubkeyUsage ( ) throws Exception { entity . config ( ) . set ( TestEntity . CONF_MAP_THING_OBJECT . subKey ( ""a"" ) , 1 ) ; log . info ( ""Map-SubKey: "" + MutableMap . copyOf ( entity . getConfigMap ( ) . asMapWithStringKeys ( ) ) ) ; Assert . assertEquals ( entity . getConfig ( TestEntity . CONF_MAP_THING_OBJECT ) , ImmutableMap . < String , Object > of ( ""a"" , 1 ) ) ; } }","log . info ( ""Map-SubKey: "" + MutableMap . copyOf ( entity . getConfigMap ( ) . asMapWithStringKeys ( ) ) ) ;",Same Information
"public class A { private CatalogModel loadCatalogModel ( ) { CatalogModel model = null ; try { LoadModel < CatalogModel > loadModel = new LoadModel < > ( CatalogModel . class ) ; loadModel = getCommandService ( ) . executeCommand ( loadModel ) ; model = loadModel . getModel ( ) ; if ( model != null ) { fireLoad ( model ) ; } } catch ( Exception e ) { log . error ( ""Error loading the CatalogModel"" , e ) ; throw new RuntimeException ( ""Error loading the CatalogModel"" , e ) ; } return model ; } }","public class A { private CatalogModel loadCatalogModel ( ) { CatalogModel model = null ; try { LoadModel < CatalogModel > loadModel = new LoadModel < > ( CatalogModel . class ) ; loadModel = getCommandService ( ) . executeCommand ( loadModel ) ; model = loadModel . getModel ( ) ; if ( model != null ) { fireLoad ( model ) ; } } catch ( Exception e ) { logger . error ( ""Error loading the CatalogModel"" , e ) ; throw new RuntimeException ( ""Error loading the CatalogModel"" , e ) ; } return model ; } }","logger . error ( ""Error loading the CatalogModel"" , e ) ;",Same Information
"public class A { @ Override public void delete ( ScriptKey scriptKey ) { LOGGER . trace ( MessageFormat . format ( ""Deleting Script {0}."" , scriptKey . toString ( ) ) ) ; ScriptVersionKey scriptVersionKey = new ScriptVersionKey ( new ScriptKey ( scriptKey . getScriptId ( ) , scriptKey . getScriptVersion ( ) ) ) ; ScriptVersionConfiguration . getInstance ( ) . delete ( scriptVersionKey ) ; ActionConfiguration . getInstance ( ) . deleteByScript ( scriptKey ) ; ScriptParameterConfiguration . getInstance ( ) . deleteByScript ( scriptKey ) ; ScriptLabelConfiguration . getInstance ( ) . deleteByScript ( scriptKey ) ; getDeleteStatement ( scriptKey ) . ifPresent ( getMetadataRepository ( ) :: executeUpdate ) ; } }","public class A { @ Override public void delete ( ScriptKey scriptKey ) { log . trace ( MessageFormat . format ( ""Deleting script {0}"" , scriptKey . toString ( ) ) ) ; ScriptVersionKey scriptVersionKey = new ScriptVersionKey ( new ScriptKey ( scriptKey . getScriptId ( ) , scriptKey . getScriptVersion ( ) ) ) ; ScriptVersionConfiguration . getInstance ( ) . delete ( scriptVersionKey ) ; ActionConfiguration . getInstance ( ) . deleteByScript ( scriptKey ) ; ScriptParameterConfiguration . getInstance ( ) . deleteByScript ( scriptKey ) ; ScriptLabelConfiguration . getInstance ( ) . deleteByScript ( scriptKey ) ; getDeleteStatement ( scriptKey ) . ifPresent ( getMetadataRepository ( ) :: executeUpdate ) ; } }","log . trace ( MessageFormat . format ( ""Deleting script {0}"" , scriptKey . toString ( ) ) ) ;",Same Information
"public class A { @ Override public InputStream retrieveBlock ( String key , long byteRangeStart , long byteRangeEnd ) throws IOException { try { GetObjectRequest request = new GetObjectRequest ( this . bucketName , key ) ; request . setRange ( byteRangeStart , byteRangeEnd ) ; COSObject cosObject = ( COSObject ) this . callCOSClientWithRetry ( request ) ; return cosObject . getObjectContent ( ) ; } catch ( CosServiceException e ) { String errMsg = String . format ( ""Retrieving key [%s] with byteRangeStart [%d] occurs "" + ""an CosServiceException: [%s]."" , key , byteRangeStart , e . toString ( ) ) ; LOG . error ( errMsg ) ; handleException ( new Exception ( errMsg ) , key ) ; return null ; } catch ( CosClientException e ) { String errMsg = String . format ( ""Retrieving key [%s] with byteRangeStart [%d] "" + ""occurs an exception: [%s]."" , key , byteRangeStart , e . toString ( ) ) ; LOG . error ( errMsg ) ; handleException ( new Exception ( errMsg ) , key ) ; } return null ; } }","public class A { @ Override public InputStream retrieveBlock ( String key , long byteRangeStart , long byteRangeEnd ) throws IOException { try { GetObjectRequest request = new GetObjectRequest ( this . bucketName , key ) ; request . setRange ( byteRangeStart , byteRangeEnd ) ; COSObject cosObject = ( COSObject ) this . callCOSClientWithRetry ( request ) ; return cosObject . getObjectContent ( ) ; } catch ( CosServiceException e ) { String errMsg = String . format ( ""Retrieving key [%s] with byteRangeStart [%d] occurs "" + ""an CosServiceException: [%s]."" , key , byteRangeStart , e . toString ( ) ) ; LOG . error ( errMsg ) ; handleException ( new Exception ( errMsg ) , key ) ; return null ; } catch ( CosClientException e ) { String errMsg = String . format ( ""Retrieving key [%s] with byteRangeStart [%d] "" + ""occurs an exception: [%s]."" , key , byteRangeStart , e . toString ( ) ) ; LOG . error ( ""Retrieving COS key: [{}] with byteRangeStart: [{}] "" + ""occurs an exception: [{}]."" , key , byteRangeStart , e ) ; handleException ( new Exception ( errMsg ) , key ) ; } return null ; } }","LOG . error ( ""Retrieving COS key: [{}] with byteRangeStart: [{}] "" + ""occurs an exception: [{}]."" , key , byteRangeStart , e ) ;",Same Information
"public class A { @ Test public void sampleMultiple ( ) { setupLogger ( 10 * Constants . SECOND_MS ) ; doReturn ( true ) . when ( mBaseLogger ) . isWarnEnabled ( ) ; for ( int i = 0 ; i < 10 ; i ++ ) { mSamplingLogger . warn ( ""warning1"" ) ; mLogger . warn ( ""warning2"" ) ; } verify ( mBaseLogger , times ( 1 ) ) . warn ( ""warning1"" ) ; verify ( mBaseLogger , times ( 1 ) ) . warn ( ""warning2"" ) ; } }","public class A { @ Test public void sampleMultiple ( ) { setupLogger ( 10 * Constants . SECOND_MS ) ; doReturn ( true ) . when ( mBaseLogger ) . isWarnEnabled ( ) ; for ( int i = 0 ; i < 10 ; i ++ ) { mSamplingLogger . warn ( ""warning1"" ) ; mSamplingLogger . warn ( ""warning2"" ) ; } verify ( mBaseLogger , times ( 1 ) ) . warn ( ""warning1"" ) ; verify ( mBaseLogger , times ( 1 ) ) . warn ( ""warning2"" ) ; } }","mSamplingLogger . warn ( ""warning2"" ) ;",Same Information
"public class A { private static void createTarFile ( File archiveFile ) { try { archiveFile . createNewFile ( ) ; } catch ( IOException e ) { log . error ( ""Error creating tar file"" , e ) ; } } }","public class A { private static void createTarFile ( File archiveFile ) { try { archiveFile . createNewFile ( ) ; } catch ( IOException e ) { LOG . error ( ""Error creating a tar file"" ) ; } } }","LOG . error ( ""Error creating a tar file"" ) ;",Same Information
"public class A { @ Override public List < AbstractPlan > buildPlans ( final String csarName , final AbstractDefinitions definitions ) { final List < AbstractPlan > plans = new ArrayList < > ( ) ; for ( final AbstractServiceTemplate serviceTemplate : definitions . getServiceTemplates ( ) ) { if ( ! serviceTemplate . hasBuildPlan ( ) ) { LOG . debug ( ""ServiceTemplate {} has no BuildPlan, generating a build plan"" , serviceTemplate . getQName ( ) . toString ( ) ) ; final BPELPlan newBuildPlan = buildPlan ( csarName , definitions , serviceTemplate ) ; if ( newBuildPlan != null ) { BPELSituationAwareBuildProcessBuilder . LOG . debug ( ""Created BuildPlan "" + newBuildPlan . getBpelProcessElement ( ) . getAttribute ( ""name"" ) ) ; plans . add ( newBuildPlan ) ; } } else { BPELSituationAwareBuildProcessBuilder . LOG . debug ( ""ServiceTemplate {} has BuildPlan, no generation needed"" , serviceTemplate . getQName ( ) . toString ( ) ) ; } } if ( ! plans . isEmpty ( ) ) { LOG . info ( ""Created {} situation-aware build plans for CSAR {}"" , String . valueOf ( plans . size ( ) ) , csarName ) ; } return plans ; } }","public class A { @ Override public List < AbstractPlan > buildPlans ( final String csarName , final AbstractDefinitions definitions ) { final List < AbstractPlan > plans = new ArrayList < > ( ) ; for ( final AbstractServiceTemplate serviceTemplate : definitions . getServiceTemplates ( ) ) { if ( ! serviceTemplate . hasBuildPlan ( ) ) { BPELSituationAwareBuildProcessBuilder . LOG . debug ( ""ServiceTemplate {} has no BuildPlan, generating BuildPlan"" , serviceTemplate . getQName ( ) . toString ( ) ) ; final BPELPlan newBuildPlan = buildPlan ( csarName , definitions , serviceTemplate ) ; if ( newBuildPlan != null ) { BPELSituationAwareBuildProcessBuilder . LOG . debug ( ""Created BuildPlan "" + newBuildPlan . getBpelProcessElement ( ) . getAttribute ( ""name"" ) ) ; plans . add ( newBuildPlan ) ; } } else { BPELSituationAwareBuildProcessBuilder . LOG . debug ( ""ServiceTemplate {} has BuildPlan, no generation needed"" , serviceTemplate . getQName ( ) . toString ( ) ) ; } } if ( ! plans . isEmpty ( ) ) { LOG . info ( ""Created {} situation-aware build plans for CSAR {}"" , String . valueOf ( plans . size ( ) ) , csarName ) ; } return plans ; } }","BPELSituationAwareBuildProcessBuilder . LOG . debug ( ""ServiceTemplate {} has no BuildPlan, generating BuildPlan"" , serviceTemplate . getQName ( ) . toString ( ) ) ;",Same Information
"public class A { @ Override public Response getLatestBundle ( ) { _log . info ( ""Starting getLatestBundle."" ) ; Bundle latestBundle = getLatestBundleAsBundle ( ) ; if ( latestBundle != null ) { try { JSONObject response = new JSONObject ( ) ; response . put ( ""id"" , latestBundle . getId ( ) ) ; response . put ( ""dataset"" , latestBundle . getDataset ( ) ) ; response . put ( ""name"" , latestBundle . getName ( ) ) ; return Response . ok ( response . toString ( ) ) . build ( ) ; } catch ( Exception e ) { _log . error ( ""Error in getLatestBundle with id "" + latestBundle . getId ( ) , e ) ; } } return Response . ok ( ""Error: No bundles deployed."" ) . build ( ) ; } }","public class A { @ Override public Response getLatestBundle ( ) { _log . info ( ""Starting getLatestBundle."" ) ; Bundle latestBundle = getLatestBundleAsBundle ( ) ; if ( latestBundle != null ) { try { JSONObject response = new JSONObject ( ) ; response . put ( ""id"" , latestBundle . getId ( ) ) ; response . put ( ""dataset"" , latestBundle . getDataset ( ) ) ; response . put ( ""name"" , latestBundle . getName ( ) ) ; return Response . ok ( response . toString ( ) ) . build ( ) ; } catch ( Exception e ) { _log . error ( ""Error reading latest bundle: "" + e ) ; } } return Response . ok ( ""Error: No bundles deployed."" ) . build ( ) ; } }","_log . error ( ""Error reading latest bundle: "" + e ) ;",Same Information
"public class A { @ Override public List < MediaPackageElement > distributeSync ( String channelId , MediaPackage mediaPackage , Set < String > elementIds ) throws DistributionException { if ( getStreamingURLforCurrentOrg ( ) == null ) { logger . warn ( String . format ( ""Trying to distribute to streaming from tenant where streaming url or port aren't set."" , securityService . getOrganization ( ) . getId ( ) ) ) ; return Collections . emptyList ( ) ; } if ( distributionDirectory == null ) { logger . warn ( String . format ( ""Trying to distribute to streaming from tenant where distribution directory isn't set."" ) ) ; return Collections . emptyList ( ) ; } URI streamingURL = getStreamingURLforCurrentOrg ( ) ; return distributeElements ( channelId , mediaPackage , elementIds , streamingURL ) ; } }","public class A { @ Override public List < MediaPackageElement > distributeSync ( String channelId , MediaPackage mediaPackage , Set < String > elementIds ) throws DistributionException { if ( getStreamingURLforCurrentOrg ( ) == null ) { logger . warn ( String . format ( ""Trying to distribute to streaming from tenant where streaming url or port aren't set."" , securityService . getOrganization ( ) . getId ( ) ) ) ; return Collections . emptyList ( ) ; } if ( distributionDirectory == null ) { logger . warn ( ""Streaming distribution directory isn't set (org.opencastproject.streaming.directory)"" ) ; return Collections . emptyList ( ) ; } URI streamingURL = getStreamingURLforCurrentOrg ( ) ; return distributeElements ( channelId , mediaPackage , elementIds , streamingURL ) ; } }","logger . warn ( ""Streaming distribution directory isn't set (org.opencastproject.streaming.directory)"" ) ;",Same Information
"public class A { @ ApiOperation ( value = ""Returns all KIE Server templates"" , response = ServerTemplateList . class ) @ ApiResponses ( value = { @ ApiResponse ( code = 200 , message = ""KIE Server templates"" , examples = @ Example ( value = { @ ExampleProperty ( mediaType = JSON , value = SERVER_TEMPLATE_LIST_JSON ) , @ ExampleProperty ( mediaType = XML , value = SERVER_TEMPLATE_LIST_XML ) } ) ) , @ ApiResponse ( code = 404 , message = ""KIE Server template not found"" ) , @ ApiResponse ( code = 400 , message = ""Controller exception"" ) , @ ApiResponse ( code = 500 , message = ""Unexpected error"" ) } ) @ GET @ Path ( ""servers"" ) @ Produces ( { MediaType . APPLICATION_XML , MediaType . APPLICATION_JSON } ) @ Consumes ( { MediaType . APPLICATION_XML , MediaType . APPLICATION_JSON } ) public Response listServerTemplates ( @ Context HttpHeaders headers ) { String contentType = getContentType ( headers ) ; try { logger . debug ( ""Received get server templates"" ) ; String response = marshal ( contentType , specManagementService . listServerTemplates ( ) ) ; logger . debug ( ""Returning response with content '{}'"" , response ) ; return createCorrectVariant ( response , headers , Response . Status . OK ) ; } catch ( KieServerControllerIllegalArgumentException e ) { return createCorrectVariant ( e . getMessage ( ) , headers , Response . Status . NOT_FOUND ) ; } catch ( KieServerControllerException e ) { return createCorrectVariant ( REQUEST_FAILED_TOBE_PROCESSED + e . getMessage ( ) , headers , Response . Status . BAD_REQUEST ) ; } catch ( Exception e ) { logger . error ( ""Get server templates failed due to {}"" , e . getMessage ( ) , e ) ; return createCorrectVariant ( ""Unknown error "" + e . getMessage ( ) , headers , Response . Status . INTERNAL_SERVER_ERROR ) ; } } }","public class A { @ ApiOperation ( value = ""Returns all KIE Server templates"" , response = ServerTemplateList . class ) @ ApiResponses ( value = { @ ApiResponse ( code = 200 , message = ""KIE Server templates"" , examples = @ Example ( value = { @ ExampleProperty ( mediaType = JSON , value = SERVER_TEMPLATE_LIST_JSON ) , @ ExampleProperty ( mediaType = XML , value = SERVER_TEMPLATE_LIST_XML ) } ) ) , @ ApiResponse ( code = 404 , message = ""KIE Server template not found"" ) , @ ApiResponse ( code = 400 , message = ""Controller exception"" ) , @ ApiResponse ( code = 500 , message = ""Unexpected error"" ) } ) @ GET @ Path ( ""servers"" ) @ Produces ( { MediaType . APPLICATION_XML , MediaType . APPLICATION_JSON } ) @ Consumes ( { MediaType . APPLICATION_XML , MediaType . APPLICATION_JSON } ) public Response listServerTemplates ( @ Context HttpHeaders headers ) { String contentType = getContentType ( headers ) ; try { logger . debug ( ""Received get server templates"" ) ; String response = marshal ( contentType , specManagementService . listServerTemplates ( ) ) ; logger . debug ( ""Returning response for get server templates: {}"" , response ) ; return createCorrectVariant ( response , headers , Response . Status . OK ) ; } catch ( KieServerControllerIllegalArgumentException e ) { return createCorrectVariant ( e . getMessage ( ) , headers , Response . Status . NOT_FOUND ) ; } catch ( KieServerControllerException e ) { return createCorrectVariant ( REQUEST_FAILED_TOBE_PROCESSED + e . getMessage ( ) , headers , Response . Status . BAD_REQUEST ) ; } catch ( Exception e ) { logger . error ( ""Get server templates failed due to {}"" , e . getMessage ( ) , e ) ; return createCorrectVariant ( ""Unknown error "" + e . getMessage ( ) , headers , Response . Status . INTERNAL_SERVER_ERROR ) ; } } }","logger . debug ( ""Returning response for get server templates: {}"" , response ) ;",Same Information
"public class A { private void init ( ) throws Exception { this . depResolver = new DependencyResolver ( zconf . getString ( ConfVars . ZEPPELIN_INTERPRETER_LOCALREPO ) ) ; InterpreterOutput . limit = zconf . getInt ( ConfVars . ZEPPELIN_INTERPRETER_OUTPUT_LIMIT ) ; HeliumApplicationFactory heliumApplicationFactory = new HeliumApplicationFactory ( ) ; HeliumVisualizationFactory heliumVisualizationFactory ; if ( isBinaryPackage ( zconf ) ) { heliumVisualizationFactory = new HeliumVisualizationFactory ( zconf , new File ( zconf . getRelativeDir ( ConfVars . ZEPPELIN_DEP_LOCALREPO ) ) , new File ( zconf . getRelativeDir ( ""lib/node_modules/zeppelin-tabledata"" ) ) , new File ( zconf . getRelativeDir ( ""lib/node_modules/zeppelin-vis"" ) ) ) ; } else { heliumVisualizationFactory = new HeliumVisualizationFactory ( zconf , new File ( zconf . getRelativeDir ( ConfVars . ZEPPELIN_DEP_LOCALREPO ) ) , new File ( zconf . getRelativeDir ( ""smart-zeppelin/zeppelin-web/src/app/tabledata"" ) ) , new File ( zconf . getRelativeDir ( ""smart-zeppelin/zeppelin-web/src/app/visualization"" ) ) ) ; } this . helium = new Helium ( zconf . getHeliumConfPath ( ) , zconf . getHeliumDefaultLocalRegistryPath ( ) , heliumVisualizationFactory , heliumApplicationFactory ) ; try { heliumVisualizationFactory . bundle ( helium . getVisualizationPackagesToBundle ( ) ) ; } catch ( Exception e ) { LOGGER . error ( e . getMessage ( ) , e ) ; } this . schedulerFactory = new SchedulerFactory ( ) ; this . interpreterSettingManager = new InterpreterSettingManager ( zconf , depResolver , new InterpreterOption ( true ) ) ; this . noteSearchService = new LuceneSearch ( ) ; this . notebookAuthorization = NotebookAuthorization . init ( zconf ) ; this . credentials = new Credentials ( zconf . credentialsPersist ( ) , zconf . getCredentialsPath ( ) ) ; } }","public class A { private void init ( ) throws Exception { this . depResolver = new DependencyResolver ( zconf . getString ( ConfVars . ZEPPELIN_INTERPRETER_LOCALREPO ) ) ; InterpreterOutput . limit = zconf . getInt ( ConfVars . ZEPPELIN_INTERPRETER_OUTPUT_LIMIT ) ; HeliumApplicationFactory heliumApplicationFactory = new HeliumApplicationFactory ( ) ; HeliumVisualizationFactory heliumVisualizationFactory ; if ( isBinaryPackage ( zconf ) ) { heliumVisualizationFactory = new HeliumVisualizationFactory ( zconf , new File ( zconf . getRelativeDir ( ConfVars . ZEPPELIN_DEP_LOCALREPO ) ) , new File ( zconf . getRelativeDir ( ""lib/node_modules/zeppelin-tabledata"" ) ) , new File ( zconf . getRelativeDir ( ""lib/node_modules/zeppelin-vis"" ) ) ) ; } else { heliumVisualizationFactory = new HeliumVisualizationFactory ( zconf , new File ( zconf . getRelativeDir ( ConfVars . ZEPPELIN_DEP_LOCALREPO ) ) , new File ( zconf . getRelativeDir ( ""smart-zeppelin/zeppelin-web/src/app/tabledata"" ) ) , new File ( zconf . getRelativeDir ( ""smart-zeppelin/zeppelin-web/src/app/visualization"" ) ) ) ; } this . helium = new Helium ( zconf . getHeliumConfPath ( ) , zconf . getHeliumDefaultLocalRegistryPath ( ) , heliumVisualizationFactory , heliumApplicationFactory ) ; try { heliumVisualizationFactory . bundle ( helium . getVisualizationPackagesToBundle ( ) ) ; } catch ( Exception e ) { LOG . error ( e . getMessage ( ) , e ) ; } this . schedulerFactory = new SchedulerFactory ( ) ; this . interpreterSettingManager = new InterpreterSettingManager ( zconf , depResolver , new InterpreterOption ( true ) ) ; this . noteSearchService = new LuceneSearch ( ) ; this . notebookAuthorization = NotebookAuthorization . init ( zconf ) ; this . credentials = new Credentials ( zconf . credentialsPersist ( ) , zconf . getCredentialsPath ( ) ) ; } }","LOG . error ( e . getMessage ( ) , e ) ;",Same Information
"public class A { protected List < String > getBaseNamingContexts ( InitialDirContext ctx ) { List < String > dNs = new ArrayList < String > ( ) ; try { SearchControls ctls = new SearchControls ( ) ; ctls . setReturningObjFlag ( true ) ; ctls . setSearchScope ( SearchControls . OBJECT_SCOPE ) ; ctls . setReturningAttributes ( new String [ ] { BASE_DN_ATTRIBUTE } ) ; NamingEnumeration < SearchResult > objResults = ctx . search ( """" , ""objectclass=*"" , ctls ) ; while ( objResults != null && objResults . hasMore ( ) ) { final SearchResult objEntry = objResults . nextElement ( ) ; final Attributes objAttributes = objEntry . getAttributes ( ) ; if ( objAttributes != null ) { final Attribute objAttribute = objAttributes . get ( BASE_DN_ATTRIBUTE ) ; NamingEnumeration < ? extends Object > allValues = objAttribute . getAll ( ) ; while ( allValues . hasMoreElements ( ) ) dNs . add ( ( String ) allValues . nextElement ( ) ) ; } } if ( dNs . isEmpty ( ) ) LOGGER . warn ( ""No base DNs could be located for LDAP context"" ) ; } catch ( Exception e ) { LOGGER . warn ( ""Error while retrieving base DNs from LDAP context: "" , e ) ; } return dNs ; } }","public class A { protected List < String > getBaseNamingContexts ( InitialDirContext ctx ) { List < String > dNs = new ArrayList < String > ( ) ; try { SearchControls ctls = new SearchControls ( ) ; ctls . setReturningObjFlag ( true ) ; ctls . setSearchScope ( SearchControls . OBJECT_SCOPE ) ; ctls . setReturningAttributes ( new String [ ] { BASE_DN_ATTRIBUTE } ) ; NamingEnumeration < SearchResult > objResults = ctx . search ( """" , ""objectclass=*"" , ctls ) ; while ( objResults != null && objResults . hasMore ( ) ) { final SearchResult objEntry = objResults . nextElement ( ) ; final Attributes objAttributes = objEntry . getAttributes ( ) ; if ( objAttributes != null ) { final Attribute objAttribute = objAttributes . get ( BASE_DN_ATTRIBUTE ) ; NamingEnumeration < ? extends Object > allValues = objAttribute . getAll ( ) ; while ( allValues . hasMoreElements ( ) ) dNs . add ( ( String ) allValues . nextElement ( ) ) ; } } if ( dNs . isEmpty ( ) ) LOGGER . warn ( ""No base DNs could be located for LDAP context"" ) ; } catch ( Exception e ) { LOGGER . warn ( ""ERROR looking up base DNs for LDAP context"" , e ) ; } return dNs ; } }","LOGGER . warn ( ""ERROR looking up base DNs for LDAP context"" , e ) ;",Same Information
"public class A { public synchronized void cleanup ( boolean deleteTemporary ) { if ( deleteTemporary && tempDirectory != null && tempDirectory . exists ( ) ) { log . debug ( ""Deleting temp directory "" + tempDirectory ) ; FileUtils . delete ( tempDirectory ) ; tempDirectory = null ; } } }","public class A { public synchronized void cleanup ( boolean deleteTemporary ) { if ( deleteTemporary && tempDirectory != null && tempDirectory . exists ( ) ) { LOGGER . debug ( ""Deleting ALL temporary files from `{}`"" , tempDirectory . toString ( ) ) ; FileUtils . delete ( tempDirectory ) ; tempDirectory = null ; } } }","LOGGER . debug ( ""Deleting ALL temporary files from `{}`"" , tempDirectory . toString ( ) ) ;",Same Information
"public class A { private static void startJsonDump ( CommandLineArgs args ) { try { final String filename = args . getJsonDump ( ) ; final JsonDumper jsonDumper = new JsonDumper ( filename , args . getLanguages ( ) , args . getExtraTags ( ) ) ; NominatimConnector nominatimConnector = new NominatimConnector ( args . getHost ( ) , args . getPort ( ) , args . getDatabase ( ) , args . getUser ( ) , args . getPassword ( ) ) ; nominatimConnector . setImporter ( jsonDumper ) ; nominatimConnector . readEntireDatabase ( args . getCountryCodes ( ) . split ( "","" ) ) ; log . info ( ""finished JSON dump."" ) ; } catch ( FileNotFoundException e ) { log . error ( ""cannot create dump"" , e ) ; } } }","public class A { private static void startJsonDump ( CommandLineArgs args ) { try { final String filename = args . getJsonDump ( ) ; final JsonDumper jsonDumper = new JsonDumper ( filename , args . getLanguages ( ) , args . getExtraTags ( ) ) ; NominatimConnector nominatimConnector = new NominatimConnector ( args . getHost ( ) , args . getPort ( ) , args . getDatabase ( ) , args . getUser ( ) , args . getPassword ( ) ) ; nominatimConnector . setImporter ( jsonDumper ) ; nominatimConnector . readEntireDatabase ( args . getCountryCodes ( ) . split ( "","" ) ) ; log . info ( ""json dump was created: "" + filename ) ; } catch ( FileNotFoundException e ) { log . error ( ""cannot create dump"" , e ) ; } } }","log . info ( ""json dump was created: "" + filename ) ;",Same Information
"public class A { protected String getPosterImageFileUrl ( String posterimageUrlOpt ) { if ( StringUtils . isBlank ( posterimageUrlOpt ) ) { return null ; } URL url = null ; try { url = new URL ( posterimageUrlOpt ) ; } catch ( Exception e ) { logger . debug ( ""Failed to load poster image URL '{}'"" , posterimageUrlOpt , e ) ; } if ( url == null ) { return null ; } if ( ""file"" . equals ( url . getProtocol ( ) ) ) { return url . toExternalForm ( ) ; } try { File coverImageFile = getWorkspace ( ) . get ( url . toURI ( ) ) ; return coverImageFile . getPath ( ) ; } catch ( NotFoundException e ) { logger . warn ( ""Poster image could not be found at '{}'"" , url ) ; return null ; } catch ( IOException e ) { logger . warn ( ""Error getting poster image: {}"" , e . getMessage ( ) ) ; return null ; } catch ( URISyntaxException e ) { logger . warn ( ""Given URL '{}' is not a valid URI"" , url ) ; return null ; } } }","public class A { protected String getPosterImageFileUrl ( String posterimageUrlOpt ) { if ( StringUtils . isBlank ( posterimageUrlOpt ) ) { return null ; } URL url = null ; try { url = new URL ( posterimageUrlOpt ) ; } catch ( Exception e ) { logger . debug ( ""Given poster image URI '{}' is not valid"" , posterimageUrlOpt ) ; } if ( url == null ) { return null ; } if ( ""file"" . equals ( url . getProtocol ( ) ) ) { return url . toExternalForm ( ) ; } try { File coverImageFile = getWorkspace ( ) . get ( url . toURI ( ) ) ; return coverImageFile . getPath ( ) ; } catch ( NotFoundException e ) { logger . warn ( ""Poster image could not be found at '{}'"" , url ) ; return null ; } catch ( IOException e ) { logger . warn ( ""Error getting poster image: {}"" , e . getMessage ( ) ) ; return null ; } catch ( URISyntaxException e ) { logger . warn ( ""Given URL '{}' is not a valid URI"" , url ) ; return null ; } } }","logger . debug ( ""Given poster image URI '{}' is not valid"" , posterimageUrlOpt ) ;",Same Information
"public class A { @ Test void testSplitDocumentation ( ) throws Exception { try ( CamelContext context = new DefaultCamelContext ( ) ) { String json = context . adapt ( CatalogCamelContext . class ) . getEipParameterJsonSchema ( ""split"" ) ; LOG . info ( ""Got split json: {}"" , json ) ; assertNotNull ( ""Should have found json for split"" , json ) ; assertTrue ( json . contains ( ""\""name\"": \""split\"""" ) ) ; assertTrue ( json . contains ( ""If enabled then processing each splitted messages occurs concurrently."" ) ) ; assertTrue ( json . contains ( ""\""outputs\"": { \""kind\"": \""element\"", \""displayName\"": \""Outputs\"", \""required\"": true, \""type\"": \""array\"", \""javaType\"""" ) ) ; } } }","public class A { @ Test void testSplitDocumentation ( ) throws Exception { try ( CamelContext context = new DefaultCamelContext ( ) ) { String json = context . adapt ( CatalogCamelContext . class ) . getEipParameterJsonSchema ( ""split"" ) ; LOG . info ( json ) ; assertNotNull ( ""Should have found json for split"" , json ) ; assertTrue ( json . contains ( ""\""name\"": \""split\"""" ) ) ; assertTrue ( json . contains ( ""If enabled then processing each splitted messages occurs concurrently."" ) ) ; assertTrue ( json . contains ( ""\""outputs\"": { \""kind\"": \""element\"", \""displayName\"": \""Outputs\"", \""required\"": true, \""type\"": \""array\"", \""javaType\"""" ) ) ; } } }",LOG . info ( json ) ;,Same Information
"public class A { private void setupDeviceManagementSchema ( DataSourceConfig config ) throws CertificateManagementException { CertificateMgtSchemaInitializer initializer = new CertificateMgtSchemaInitializer ( config ) ; String checkSql = ""select * from DM_DEVICE_CERTIFICATE"" ; try { if ( ! initializer . isDatabaseStructureCreated ( checkSql ) ) { log . info ( ""Initializing Certificate management repository database schema"" ) ; initializer . createRegistryDatabase ( ) ; } else { log . info ( ""Certificate management repository database already exists. Not creating a new database."" ) ; } } catch ( Exception e ) { throw new CertificateManagementException ( ""Error occurred while initializing Certificate Management database schema"" , e ) ; } if ( log . isDebugEnabled ( ) ) { log . debug ( ""Certificate management repository database initialized successfully."" ) ; } } }","public class A { private void setupDeviceManagementSchema ( DataSourceConfig config ) throws CertificateManagementException { CertificateMgtSchemaInitializer initializer = new CertificateMgtSchemaInitializer ( config ) ; String checkSql = ""select * from DM_DEVICE_CERTIFICATE"" ; try { if ( ! initializer . isDatabaseStructureCreated ( checkSql ) ) { log . info ( ""Initializing Certificate management repository database schema"" ) ; initializer . createRegistryDatabase ( ) ; } else { log . info ( ""Certificate management repository database already exists. Not creating a new database."" ) ; } } catch ( Exception e ) { throw new CertificateManagementException ( ""Error occurred while initializing Certificate Management database schema"" , e ) ; } if ( log . isDebugEnabled ( ) ) { log . debug ( ""Certificate management metadata repository schema has been successfully initialized"" ) ; } } }","log . debug ( ""Certificate management metadata repository schema has been successfully initialized"" ) ;",Same Information
"public class A { public String resetMethodStatus ( ) { try { String result = this . checkMethod ( ) ; if ( null != result ) return result ; this . resetMethodStatus ( this . extractMethod ( ) ) ; } catch ( Throwable t ) { _logger . error ( ""error in resetMethodStatus"" , t ) ; return FAILURE ; } return SUCCESS ; } }","public class A { public String resetMethodStatus ( ) { try { String result = this . checkMethod ( ) ; if ( null != result ) return result ; this . resetMethodStatus ( this . extractMethod ( ) ) ; } catch ( Throwable t ) { _logger . error ( ""Error resetting method status"" , t ) ; return FAILURE ; } return SUCCESS ; } }","_logger . error ( ""Error resetting method status"" , t ) ;",Same Information
"public class A { @ Override public final Object fromBytes ( Class targetClass , byte [ ] bytes ) { try { if ( bytes == null ) { return null ; } if ( targetClass != null && targetClass . equals ( byte [ ] . class ) ) { return bytes ; } ObjectInputStream ois ; ois = new ObjectInputStream ( new ByteArrayInputStream ( bytes ) ) ; Object o = ois . readObject ( ) ; ois . close ( ) ; return o ; } catch ( IOException e ) { log . error ( ""I/O error while deserializing."" , e ) ; throw new PropertyAccessException ( e ) ; } catch ( ClassNotFoundException e ) { log . error ( ""Class not found exception, Caused by {}."" , e ) ; throw new PropertyAccessException ( e ) ; } } }","public class A { @ Override public final Object fromBytes ( Class targetClass , byte [ ] bytes ) { try { if ( bytes == null ) { return null ; } if ( targetClass != null && targetClass . equals ( byte [ ] . class ) ) { return bytes ; } ObjectInputStream ois ; ois = new ObjectInputStream ( new ByteArrayInputStream ( bytes ) ) ; Object o = ois . readObject ( ) ; ois . close ( ) ; return o ; } catch ( IOException e ) { log . error ( ""IO exception, Caused by {}."" , e ) ; throw new PropertyAccessException ( e ) ; } catch ( ClassNotFoundException e ) { log . error ( ""Class not found exception, Caused by {}."" , e ) ; throw new PropertyAccessException ( e ) ; } } }","log . error ( ""IO exception, Caused by {}."" , e ) ;",Same Information
"public class A { private Boolean meetsConditions ( Session session , PlanDefinition . PlanDefinitionActionComponent action ) { for ( PlanDefinition . PlanDefinitionActionConditionComponent condition : action . getCondition ( ) ) { if ( condition . hasDescription ( ) ) { logger . info ( ""Description: "" + condition . getDescription ( ) ) ; } if ( condition . getKind ( ) == PlanDefinition . ActionConditionKind . APPLICABILITY ) { if ( ! condition . getLanguage ( ) . equals ( ""text/cql"" ) ) { logger . warn ( ""An action language other than CQL was found: "" + condition . getLanguage ( ) ) ; continue ; } if ( ! condition . hasExpression ( ) ) { logger . error ( ""Missing condition expression"" ) ; throw new RuntimeException ( ""Missing condition expression"" ) ; } logger . info ( ""Evaluating action condition expression "" + condition . getExpression ( ) ) ; String cql = condition . getExpression ( ) ; Object result = executionProvider . evaluateInContext ( session . getPlanDefinition ( ) , cql , session . getPatientId ( ) ) ; if ( result == null ) { logger . warn ( ""Expression Returned null"" ) ; return false ; } if ( ! ( result instanceof Boolean ) ) { logger . warn ( ""The condition returned a non-boolean value: "" + result . getClass ( ) . getSimpleName ( ) ) ; continue ; } if ( ! ( Boolean ) result ) { logger . info ( ""The result of condition expression %s is false"" , condition . getExpression ( ) ) ; return false ; } } } return true ; } }","public class A { private Boolean meetsConditions ( Session session , PlanDefinition . PlanDefinitionActionComponent action ) { for ( PlanDefinition . PlanDefinitionActionConditionComponent condition : action . getCondition ( ) ) { if ( condition . hasDescription ( ) ) { logger . info ( ""Resolving condition with description: "" + condition . getDescription ( ) ) ; } if ( condition . getKind ( ) == PlanDefinition . ActionConditionKind . APPLICABILITY ) { if ( ! condition . getLanguage ( ) . equals ( ""text/cql"" ) ) { logger . warn ( ""An action language other than CQL was found: "" + condition . getLanguage ( ) ) ; continue ; } if ( ! condition . hasExpression ( ) ) { logger . error ( ""Missing condition expression"" ) ; throw new RuntimeException ( ""Missing condition expression"" ) ; } logger . info ( ""Evaluating action condition expression "" + condition . getExpression ( ) ) ; String cql = condition . getExpression ( ) ; Object result = executionProvider . evaluateInContext ( session . getPlanDefinition ( ) , cql , session . getPatientId ( ) ) ; if ( result == null ) { logger . warn ( ""Expression Returned null"" ) ; return false ; } if ( ! ( result instanceof Boolean ) ) { logger . warn ( ""The condition returned a non-boolean value: "" + result . getClass ( ) . getSimpleName ( ) ) ; continue ; } if ( ! ( Boolean ) result ) { logger . info ( ""The result of condition expression %s is false"" , condition . getExpression ( ) ) ; return false ; } } } return true ; } }","logger . info ( ""Resolving condition with description: "" + condition . getDescription ( ) ) ;",Same Information
"public class A { public synchronized void bindHook ( ServiceReference < DeploymentHook > hook ) { final Object rawHookId = hook . getProperty ( ConfigurationService . KURA_SERVICE_PID ) ; if ( ! ( rawHookId instanceof String ) ) { logger . warn ( ""Received invalid request for {}"" , ConfigurationService . KURA_SERVICE_PID ) ; return ; } final String hookId = ( String ) rawHookId ; if ( registeredHooks . containsKey ( hookId ) ) { logger . warn ( ""Found duplicated hook with id {}, not registering"" , ConfigurationService . KURA_SERVICE_PID ) ; return ; } this . registeredHooks . put ( hookId , getBundleContext ( ) . getService ( hook ) ) ; logger . info ( ""Hook registered: {}"" , hookId ) ; updateAssociations ( ) ; } }","public class A { public synchronized void bindHook ( ServiceReference < DeploymentHook > hook ) { final Object rawHookId = hook . getProperty ( ConfigurationService . KURA_SERVICE_PID ) ; if ( ! ( rawHookId instanceof String ) ) { logger . warn ( ""Found hook with invalid {}, not registering"" , ConfigurationService . KURA_SERVICE_PID ) ; return ; } final String hookId = ( String ) rawHookId ; if ( registeredHooks . containsKey ( hookId ) ) { logger . warn ( ""Found duplicated hook with id {}, not registering"" , ConfigurationService . KURA_SERVICE_PID ) ; return ; } this . registeredHooks . put ( hookId , getBundleContext ( ) . getService ( hook ) ) ; logger . info ( ""Hook registered: {}"" , hookId ) ; updateAssociations ( ) ; } }","logger . warn ( ""Found hook with invalid {}, not registering"" , ConfigurationService . KURA_SERVICE_PID ) ;",Same Information
"public class A { private int getAndSetNetworkTimeout ( final Connection connection , final long timeoutMs ) { if ( isNetworkTimeoutSupported != FALSE ) { try { final int originalTimeout = connection . getNetworkTimeout ( ) ; connection . setNetworkTimeout ( netTimeoutExecutor , ( int ) timeoutMs ) ; isNetworkTimeoutSupported = TRUE ; return originalTimeout ; } catch ( Exception | AbstractMethodError e ) { if ( isNetworkTimeoutSupported == UNINITIALIZED ) { isNetworkTimeoutSupported = FALSE ; logger . info ( ""{} - Driver does not support get/set network timeout for connections. ({})"" , poolName , e . getMessage ( ) ) ; if ( validationTimeout < SECONDS . toMillis ( 1 ) ) { logger . warn ( ""{} - WARNING: Cannot set the network timeout for connections. ({})"" , poolName , e . getMessage ( ) ) ; } else if ( validationTimeout % SECONDS . toMillis ( 1 ) != 0 ) { logger . warn ( ""{} - A validationTimeout with fractional second granularity cannot be honored on drivers without setNetworkTimeout() support."" , poolName ) ; } } } } return 0 ; } }","public class A { private int getAndSetNetworkTimeout ( final Connection connection , final long timeoutMs ) { if ( isNetworkTimeoutSupported != FALSE ) { try { final int originalTimeout = connection . getNetworkTimeout ( ) ; connection . setNetworkTimeout ( netTimeoutExecutor , ( int ) timeoutMs ) ; isNetworkTimeoutSupported = TRUE ; return originalTimeout ; } catch ( Exception | AbstractMethodError e ) { if ( isNetworkTimeoutSupported == UNINITIALIZED ) { isNetworkTimeoutSupported = FALSE ; logger . info ( ""{} - Driver does not support get/set network timeout for connections. ({})"" , poolName , e . getMessage ( ) ) ; if ( validationTimeout < SECONDS . toMillis ( 1 ) ) { logger . warn ( ""{} - A validationTimeout of less than 1 second cannot be honored on drivers without setNetworkTimeout() support."" , poolName ) ; } else if ( validationTimeout % SECONDS . toMillis ( 1 ) != 0 ) { logger . warn ( ""{} - A validationTimeout with fractional second granularity cannot be honored on drivers without setNetworkTimeout() support."" , poolName ) ; } } } } return 0 ; } }","logger . warn ( ""{} - A validationTimeout of less than 1 second cannot be honored on drivers without setNetworkTimeout() support."" , poolName ) ;",Same Information
"public class A { private synchronized CswSubscription deleteCswSubscription ( String subscriptionId ) throws CswException { String methodName = ""deleteCswSubscription"" ; LogSanitizer logSanitizedId = LogSanitizer . sanitize ( subscriptionId ) ; LOGGER . trace ( ENTERING_STR , methodName ) ; LOGGER . trace ( ""subscriptionId = {}"" , logSanitizedId ) ; if ( StringUtils . isEmpty ( subscriptionId ) ) { throw new CswException ( ""Unable to delete subscription because subscription ID is null or empty"" ) ; } CswSubscription subscription = getSubscription ( subscriptionId ) ; try { LOGGER . debug ( ""Removing (unregistering) subscription: {}"" , logSanitizedId ) ; ServiceRegistration sr = registeredSubscriptions . remove ( subscriptionId ) ; if ( sr != null ) { sr . unregister ( ) ; } else { LOGGER . debug ( ""No ServiceRegistration found for subscription: {}"" , logSanitizedId ) ; } Configuration subscriptionConfig = getSubscriptionConfiguration ( subscriptionId ) ; try { if ( subscriptionConfig != null ) { LOGGER . debug ( ""Deleting subscription for subscriptionId = {}"" , logSanitizedId ) ; subscriptionConfig . delete ( ) ; } else { LOGGER . debug ( ""subscriptionConfig is NULL for ID = {}"" , logSanitizedId ) ; } } catch ( IOException e ) { LOGGER . debug ( ""IOException trying to delete subscription's configuration for subscription ID {}"" , subscriptionId , e ) ; } LOGGER . debug ( ""Subscription removal complete"" ) ; } catch ( Exception e ) { LOGGER . debug ( ""Exception trying to delete subscription's configuration for subscription ID {}"" , logSanitizedId , e ) ; } LOGGER . trace ( ""EXITING: {} (status = {})"" , methodName , false ) ; return subscription ; } }","public class A { private synchronized CswSubscription deleteCswSubscription ( String subscriptionId ) throws CswException { String methodName = ""deleteCswSubscription"" ; LogSanitizer logSanitizedId = LogSanitizer . sanitize ( subscriptionId ) ; LOGGER . trace ( ENTERING_STR , methodName ) ; LOGGER . trace ( ""subscriptionId = {}"" , logSanitizedId ) ; if ( StringUtils . isEmpty ( subscriptionId ) ) { throw new CswException ( ""Unable to delete subscription because subscription ID is null or empty"" ) ; } CswSubscription subscription = getSubscription ( subscriptionId ) ; try { LOGGER . debug ( ""Removing (unregistering) subscription: {}"" , logSanitizedId ) ; ServiceRegistration sr = registeredSubscriptions . remove ( subscriptionId ) ; if ( sr != null ) { sr . unregister ( ) ; } else { LOGGER . debug ( ""No ServiceRegistration found for subscription: {}"" , logSanitizedId ) ; } Configuration subscriptionConfig = getSubscriptionConfiguration ( subscriptionId ) ; try { if ( subscriptionConfig != null ) { LOGGER . debug ( ""Deleting subscription for subscriptionId = {}"" , logSanitizedId ) ; subscriptionConfig . delete ( ) ; } else { LOGGER . debug ( ""subscriptionConfig is NULL for ID = {}"" , logSanitizedId ) ; } } catch ( IOException e ) { LOGGER . debug ( ""IOException trying to delete subscription's configuration for subscription ID {}"" , subscriptionId , e ) ; } LOGGER . debug ( ""Subscription removal complete"" ) ; } catch ( Exception e ) { LOGGER . debug ( ""Could not delete subscription for {}"" , logSanitizedId , e ) ; } LOGGER . trace ( ""EXITING: {} (status = {})"" , methodName , false ) ; return subscription ; } }","LOGGER . debug ( ""Could not delete subscription for {}"" , logSanitizedId , e ) ;",Same Information
"public class A { private void createVertexCompositeIndexWithSystemProperty ( AtlasGraphManagement management , Class propertyClass , AtlasPropertyKey propertyKey , final String systemPropertyKey , AtlasCardinality cardinality , boolean isUnique ) { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""Creating composite index for property {} of type {} and {}"" , propertyClass . getName ( ) , propertyClass . getName ( ) , systemPropertyKey ) ; } AtlasPropertyKey typePropertyKey = management . getPropertyKey ( systemPropertyKey ) ; if ( typePropertyKey == null ) { typePropertyKey = management . makePropertyKey ( systemPropertyKey , String . class , cardinality ) ; } final String indexName = propertyKey . getName ( ) + systemPropertyKey ; AtlasGraphIndex existingIndex = management . getGraphIndex ( indexName ) ; if ( existingIndex == null ) { List < AtlasPropertyKey > keys = new ArrayList < > ( 2 ) ; keys . add ( typePropertyKey ) ; keys . add ( propertyKey ) ; management . createVertexCompositeIndex ( indexName , isUnique , keys ) ; LOG . info ( ""Created composite index for property {} of type {} and {}"" , propertyKey . getName ( ) , propertyClass . getName ( ) , systemPropertyKey ) ; } } }","public class A { private void createVertexCompositeIndexWithSystemProperty ( AtlasGraphManagement management , Class propertyClass , AtlasPropertyKey propertyKey , final String systemPropertyKey , AtlasCardinality cardinality , boolean isUnique ) { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""Creating composite index for property {} of type {} and {}"" , propertyKey . getName ( ) , propertyClass . getName ( ) , systemPropertyKey ) ; } AtlasPropertyKey typePropertyKey = management . getPropertyKey ( systemPropertyKey ) ; if ( typePropertyKey == null ) { typePropertyKey = management . makePropertyKey ( systemPropertyKey , String . class , cardinality ) ; } final String indexName = propertyKey . getName ( ) + systemPropertyKey ; AtlasGraphIndex existingIndex = management . getGraphIndex ( indexName ) ; if ( existingIndex == null ) { List < AtlasPropertyKey > keys = new ArrayList < > ( 2 ) ; keys . add ( typePropertyKey ) ; keys . add ( propertyKey ) ; management . createVertexCompositeIndex ( indexName , isUnique , keys ) ; LOG . info ( ""Created composite index for property {} of type {} and {}"" , propertyKey . getName ( ) , propertyClass . getName ( ) , systemPropertyKey ) ; } } }","LOG . debug ( ""Creating composite index for property {} of type {} and {}"" , propertyKey . getName ( ) , propertyClass . getName ( ) , systemPropertyKey ) ;",Same Information
"public class A { @ Override public boolean visitEnter ( final HierComposite node ) { log . info ( ""visitEnter()"" ) ; if ( node == null ) { throw new IllegalArgumentException ( ""null node"" ) ; } log . debug ( ""checking control rod"" ) ; aborted = checkControlRod ( node ) ; if ( aborted ) { log . info ( ""aborted!"" ) ; return false ; } log . info ( ""obtaining metadata for:{}"" , node ) ; try { List < MetaDataAndDomainData > metadata = collectionAO . findMetadataValuesForCollection ( node . getAbsolutePath ( ) , 0 ) ; metadataRollup . getMetadata ( ) . push ( metadata ) ; log . info ( ""pushed metadata in the stack...now filter and then delegate to visitEnterWithMetadata() in the impl class to make any determinations"" ) ; if ( ! checkIfIndexable ( node , metadataRollup ) ) { log . info ( ""not indexable by filter, short circuit"" ) ; return false ; } boolean shortCircuit = visitEnterWithMetadata ( node , metadataRollup ) ; return shortCircuit ; } catch ( JargonException | JargonQueryException e ) { log . error ( ""error getting metadata"" , e ) ; throw new JargonRuntimeException ( ""error getting metadata"" , e ) ; } } }","public class A { @ Override public boolean visitEnter ( final HierComposite node ) { log . info ( ""visitEnter()"" ) ; if ( node == null ) { throw new IllegalArgumentException ( ""null node"" ) ; } log . debug ( ""checking control rod"" ) ; aborted = checkControlRod ( node ) ; if ( aborted ) { log . info ( ""aborted!"" ) ; return false ; } log . info ( ""obtaining metadata for:{}"" , node ) ; try { List < MetaDataAndDomainData > metadata = collectionAO . findMetadataValuesForCollection ( node . getAbsolutePath ( ) , 0 ) ; metadataRollup . getMetadata ( ) . push ( metadata ) ; log . info ( ""pushed metadata in the stack...now filter and then delegate to visitEnterWithMetadata() in the impl class to make any determinations"" ) ; if ( ! checkIfIndexable ( node , metadataRollup ) ) { log . info ( ""not indexable by filter, short circuit"" ) ; return false ; } boolean shortCircuit = visitEnterWithMetadata ( node , metadataRollup ) ; return shortCircuit ; } catch ( JargonException | JargonQueryException e ) { log . error ( ""error in obtaining metadata"" , e ) ; throw new JargonRuntimeException ( ""error getting metadata"" , e ) ; } } }","log . error ( ""error in obtaining metadata"" , e ) ;",Same Information
"public class A { @ Path ( ""/list"" ) @ GET @ Produces ( MediaType . APPLICATION_JSON ) @ Operation ( summary = ""Get all phasing only entities by using first/last index"" , description = ""Get all phasing only entities by using first/last index"" , tags = { ""accounts"" } , responses = { @ ApiResponse ( responseCode = ""200"" , description = ""Successful execution"" , content = @ Content ( mediaType = ""application/json"" , schema = @ Schema ( implementation = AccountControlPhasingResponse . class ) ) ) } ) @ PermitAll public Response getAllPhasingOnlyControls ( @ Parameter ( description = ""A zero-based index to the first, last asset ID to retrieve (optional)."" , schema = @ Schema ( implementation = FirstLastIndexBeanParam . class ) ) @ BeanParam FirstLastIndexBeanParam indexBeanParam ) { ResponseBuilder response = ResponseBuilder . startTiming ( ) ; indexBeanParam . adjustIndexes ( maxAPIFetchRecords ) ; log . trace ( ""Started getAllPhasingOnlyControls : \t indexBeanParam={}"" , indexBeanParam ) ; AccountControlPhasingResponse dto = new AccountControlPhasingResponse ( ) ; dto . phasingOnlyControls = accountControlPhasingService . getAllStream ( indexBeanParam . getFirstIndex ( ) , indexBeanParam . getLastIndex ( ) ) . map ( item -> accountControlPhasingConverter . convert ( item ) ) . collect ( Collectors . toList ( ) ) ; log . trace ( ""Result of getAllPhasingOnlyControls: {}"" , dto ) ; return response . bind ( dto ) . build ( ) ; } }","public class A { @ Path ( ""/list"" ) @ GET @ Produces ( MediaType . APPLICATION_JSON ) @ Operation ( summary = ""Get all phasing only entities by using first/last index"" , description = ""Get all phasing only entities by using first/last index"" , tags = { ""accounts"" } , responses = { @ ApiResponse ( responseCode = ""200"" , description = ""Successful execution"" , content = @ Content ( mediaType = ""application/json"" , schema = @ Schema ( implementation = AccountControlPhasingResponse . class ) ) ) } ) @ PermitAll public Response getAllPhasingOnlyControls ( @ Parameter ( description = ""A zero-based index to the first, last asset ID to retrieve (optional)."" , schema = @ Schema ( implementation = FirstLastIndexBeanParam . class ) ) @ BeanParam FirstLastIndexBeanParam indexBeanParam ) { ResponseBuilder response = ResponseBuilder . startTiming ( ) ; indexBeanParam . adjustIndexes ( maxAPIFetchRecords ) ; log . trace ( ""Started getAllPhasingOnlyControls : \t indexBeanParam={}"" , indexBeanParam ) ; AccountControlPhasingResponse dto = new AccountControlPhasingResponse ( ) ; dto . phasingOnlyControls = accountControlPhasingService . getAllStream ( indexBeanParam . getFirstIndex ( ) , indexBeanParam . getLastIndex ( ) ) . map ( item -> accountControlPhasingConverter . convert ( item ) ) . collect ( Collectors . toList ( ) ) ; log . trace ( ""getAllPhasingOnlyControls result: {}"" , dto ) ; return response . bind ( dto ) . build ( ) ; } }","log . trace ( ""getAllPhasingOnlyControls result: {}"" , dto ) ;",Same Information
"public class A { @ Override public void close ( ) { super . close ( ) ; log . debug ( ""Closing ShardQueryLogic: "" + System . identityHashCode ( this ) ) ; if ( null == scannerFactory ) { log . debug ( ""ScannerFactory was never initialized because, therefore there are no connections to close: "" + System . identityHashCode ( this ) ) ; } else { log . debug ( ""Closing ShardQueryLogic scannerFactory: "" + System . identityHashCode ( this ) ) ; try { int nClosed = 0 ; scannerFactory . lockdown ( ) ; for ( ScannerBase bs : Lists . newArrayList ( scannerFactory . currentScanners ( ) ) ) { scannerFactory . close ( bs ) ; ++ nClosed ; } if ( log . isDebugEnabled ( ) ) { log . debug ( ""Cleaned up "" + nClosed + "" batch scanners associated with this query logic."" ) ; } nClosed = 0 ; for ( ScannerSession bs : Lists . newArrayList ( scannerFactory . currentSessions ( ) ) ) { scannerFactory . close ( bs ) ; ++ nClosed ; } if ( log . isDebugEnabled ( ) ) { log . debug ( ""Cleaned up "" + nClosed + "" scanner sessions."" ) ; } } catch ( Exception e ) { log . error ( ""Caught exception trying to close scannerFactory"" , e ) ; } } if ( null != this . planner ) { try { log . debug ( ""Closing ShardQueryLogic planner: "" + System . identityHashCode ( this ) + '(' + ( this . getSettings ( ) == null ? ""empty"" : this . getSettings ( ) . getId ( ) ) + ')' ) ; this . planner . close ( getConfig ( ) , this . getSettings ( ) ) ; } catch ( Exception e ) { log . error ( ""Caught exception trying to close QueryPlanner"" , e ) ; } } if ( null != this . queries ) { try { log . debug ( ""Closing ShardQueryLogic queries: "" + System . identityHashCode ( this ) ) ; this . queries . close ( ) ; } catch ( IOException e ) { log . error ( ""Caught exception trying to close CloseableIterable of queries"" , e ) ; } } if ( null != this . scheduler ) { try { log . debug ( ""Closing ShardQueryLogic scheduler: "" + System . identityHashCode ( this ) ) ; this . scheduler . close ( ) ; ScanSessionStats stats = this . scheduler . getSchedulerStats ( ) ; if ( null != stats ) { stats . logSummary ( log ) ; } } catch ( IOException e ) { log . error ( ""Caught exception trying to close scheduler"" , e ) ; } } } }","public class A { @ Override public void close ( ) { super . close ( ) ; log . debug ( ""Closing ShardQueryLogic: "" + System . identityHashCode ( this ) ) ; if ( null == scannerFactory ) { log . debug ( ""ScannerFactory was never initialized because, therefore there are no connections to close: "" + System . identityHashCode ( this ) ) ; } else { log . debug ( ""Closing ShardQueryLogic scannerFactory: "" + System . identityHashCode ( this ) ) ; try { int nClosed = 0 ; scannerFactory . lockdown ( ) ; for ( ScannerBase bs : Lists . newArrayList ( scannerFactory . currentScanners ( ) ) ) { scannerFactory . close ( bs ) ; ++ nClosed ; } if ( log . isDebugEnabled ( ) ) { log . debug ( ""Cleaned up "" + nClosed + "" batch scanners associated with this query logic."" ) ; } nClosed = 0 ; for ( ScannerSession bs : Lists . newArrayList ( scannerFactory . currentSessions ( ) ) ) { scannerFactory . close ( bs ) ; ++ nClosed ; } if ( log . isDebugEnabled ( ) ) { log . debug ( ""Cleaned up "" + nClosed + "" scanner sessions."" ) ; } } catch ( Exception e ) { log . error ( ""Caught exception trying to close scannerFactory"" , e ) ; } } if ( null != this . planner ) { try { log . debug ( ""Closing ShardQueryLogic planner: "" + System . identityHashCode ( this ) + '(' + ( this . getSettings ( ) == null ? ""empty"" : this . getSettings ( ) . getId ( ) ) + ')' ) ; this . planner . close ( getConfig ( ) , this . getSettings ( ) ) ; } catch ( Exception e ) { log . error ( ""Caught exception trying to close QueryPlanner"" , e ) ; } } if ( null != this . queries ) { try { log . debug ( ""Closing ShardQueryLogic queries: "" + System . identityHashCode ( this ) ) ; this . queries . close ( ) ; } catch ( IOException e ) { log . error ( ""Caught exception trying to close CloseableIterable of queries"" , e ) ; } } if ( null != this . scheduler ) { try { log . debug ( ""Closing ShardQueryLogic scheduler: "" + System . identityHashCode ( this ) ) ; this . scheduler . close ( ) ; ScanSessionStats stats = this . scheduler . getSchedulerStats ( ) ; if ( null != stats ) { stats . logSummary ( log ) ; } } catch ( IOException e ) { log . error ( ""Caught exception trying to close Scheduler"" , e ) ; } } } }","log . error ( ""Caught exception trying to close Scheduler"" , e ) ;",Same Information
"public class A { @ Test public void parseBsonArrayWithValues ( ) throws IOException { BsonValue a = new BsonString ( ""stest"" ) ; BsonValue b = new BsonDouble ( 111 ) ; BsonValue c = new BsonBoolean ( true ) ; BsonDocument document = new BsonDocument ( ) . append ( ""int32"" , new BsonInt32 ( 12 ) ) . append ( ""int64"" , new BsonInt64 ( 77L ) ) . append ( ""bo\""olean"" , new BsonBoolean ( true ) ) . append ( ""date"" , new BsonDateTime ( new Date ( ) . getTime ( ) ) ) . append ( ""double"" , new BsonDouble ( 12.3 ) ) . append ( ""string"" , new BsonString ( ""pinpoint"" ) ) . append ( ""objectId"" , new BsonObjectId ( new ObjectId ( ) ) ) . append ( ""code"" , new BsonJavaScript ( ""int i = 10;"" ) ) . append ( ""codeWithScope"" , new BsonJavaScriptWithScope ( ""int x = y"" , new BsonDocument ( ""y"" , new BsonInt32 ( 1 ) ) ) ) . append ( ""regex"" , new BsonRegularExpression ( ""^test.*regex.*xyz$"" , ""big"" ) ) . append ( ""symbol"" , new BsonSymbol ( ""wow"" ) ) . append ( ""timestamp"" , new BsonTimestamp ( 0x12345678 , 5 ) ) . append ( ""undefined"" , new BsonUndefined ( ) ) . append ( ""binary1"" , new BsonBinary ( new byte [ ] { ( byte ) 0xe0 , 0x4f , ( byte ) 0xd0 , 0x20 } ) ) . append ( ""oldBinary"" , new BsonBinary ( BsonBinarySubType . OLD_BINARY , new byte [ ] { 1 , 1 , 1 , 1 , 1 } ) ) . append ( ""arrayInt"" , new BsonArray ( Arrays . asList ( a , b , c , new BsonInt32 ( 7 ) ) ) ) . append ( ""document"" , new BsonDocument ( ""a"" , new BsonInt32 ( 77 ) ) ) . append ( ""dbPointer"" , new BsonDbPointer ( ""db.coll"" , new ObjectId ( ) ) ) . append ( ""null"" , new BsonNull ( ) ) . append ( ""decimal128"" , new BsonDecimal128 ( new Decimal128 ( 55 ) ) ) ; BasicDBObject query = new BasicDBObject ( ) ; query . put ( ""ComplexBson"" , document ) ; logger . debug ( ""document:{}"" , document ) ; NormalizedBson stringStringValue = MongoUtil . parseBson ( new Object [ ] { query } , true ) ; logger . debug ( ""parsedArrayWithValues:{}"" , stringStringValue ) ; List list = objectMapper . readValue ( ""["" + stringStringValue . getNormalizedBson ( ) + ""]"" , List . class ) ; Assert . assertEquals ( list . size ( ) , 1 ) ; Map < String , ? > query1Map = ( Map < String , ? > ) list . get ( 0 ) ; checkValue ( query1Map ) ; } }","public class A { @ Test public void parseBsonArrayWithValues ( ) throws IOException { BsonValue a = new BsonString ( ""stest"" ) ; BsonValue b = new BsonDouble ( 111 ) ; BsonValue c = new BsonBoolean ( true ) ; BsonDocument document = new BsonDocument ( ) . append ( ""int32"" , new BsonInt32 ( 12 ) ) . append ( ""int64"" , new BsonInt64 ( 77L ) ) . append ( ""bo\""olean"" , new BsonBoolean ( true ) ) . append ( ""date"" , new BsonDateTime ( new Date ( ) . getTime ( ) ) ) . append ( ""double"" , new BsonDouble ( 12.3 ) ) . append ( ""string"" , new BsonString ( ""pinpoint"" ) ) . append ( ""objectId"" , new BsonObjectId ( new ObjectId ( ) ) ) . append ( ""code"" , new BsonJavaScript ( ""int i = 10;"" ) ) . append ( ""codeWithScope"" , new BsonJavaScriptWithScope ( ""int x = y"" , new BsonDocument ( ""y"" , new BsonInt32 ( 1 ) ) ) ) . append ( ""regex"" , new BsonRegularExpression ( ""^test.*regex.*xyz$"" , ""big"" ) ) . append ( ""symbol"" , new BsonSymbol ( ""wow"" ) ) . append ( ""timestamp"" , new BsonTimestamp ( 0x12345678 , 5 ) ) . append ( ""undefined"" , new BsonUndefined ( ) ) . append ( ""binary1"" , new BsonBinary ( new byte [ ] { ( byte ) 0xe0 , 0x4f , ( byte ) 0xd0 , 0x20 } ) ) . append ( ""oldBinary"" , new BsonBinary ( BsonBinarySubType . OLD_BINARY , new byte [ ] { 1 , 1 , 1 , 1 , 1 } ) ) . append ( ""arrayInt"" , new BsonArray ( Arrays . asList ( a , b , c , new BsonInt32 ( 7 ) ) ) ) . append ( ""document"" , new BsonDocument ( ""a"" , new BsonInt32 ( 77 ) ) ) . append ( ""dbPointer"" , new BsonDbPointer ( ""db.coll"" , new ObjectId ( ) ) ) . append ( ""null"" , new BsonNull ( ) ) . append ( ""decimal128"" , new BsonDecimal128 ( new Decimal128 ( 55 ) ) ) ; BasicDBObject query = new BasicDBObject ( ) ; query . put ( ""ComplexBson"" , document ) ; logger . debug ( ""document:{}"" , document ) ; NormalizedBson stringStringValue = MongoUtil . parseBson ( new Object [ ] { query } , true ) ; logger . debug ( ""val:{}"" , stringStringValue ) ; List list = objectMapper . readValue ( ""["" + stringStringValue . getNormalizedBson ( ) + ""]"" , List . class ) ; Assert . assertEquals ( list . size ( ) , 1 ) ; Map < String , ? > query1Map = ( Map < String , ? > ) list . get ( 0 ) ; checkValue ( query1Map ) ; } }","logger . debug ( ""val:{}"" , stringStringValue ) ;",Same Information
"public class A { private String loadTemplateFile ( File templateFile ) { if ( ! templateFile . exists ( ) && ! templateFile . canRead ( ) ) { return null ; } char [ ] chars ; int length ; try { chars = new char [ ( int ) templateFile . length ( ) ] ; FileInputStream stream = new FileInputStream ( templateFile ) ; InputStreamReader reader = new InputStreamReader ( stream , ""UTF-8"" ) ; length = reader . read ( chars ) ; } catch ( Exception noprob ) { log . error ( ""Exception reading theme ["" + this . getName ( ) + ""] template file ["" + templateFile + ""]"" ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( noprob . getMessage ( ) ) ; } return null ; } return new String ( chars , 0 , length ) ; } }","public class A { private String loadTemplateFile ( File templateFile ) { if ( ! templateFile . exists ( ) && ! templateFile . canRead ( ) ) { return null ; } char [ ] chars ; int length ; try { chars = new char [ ( int ) templateFile . length ( ) ] ; FileInputStream stream = new FileInputStream ( templateFile ) ; InputStreamReader reader = new InputStreamReader ( stream , ""UTF-8"" ) ; length = reader . read ( chars ) ; } catch ( Exception noprob ) { log . error ( ""Exception reading theme ["" + this . getName ( ) + ""] template file ["" + templateFile + ""]"" ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( noprob ) ; } return null ; } return new String ( chars , 0 , length ) ; } }",log . debug ( noprob ) ;,Same Information
"public class A { @ Override public HandlerResult handle ( ProcessState state , ProcessInstance process ) { Secret secret = ( Secret ) state . getResource ( ) ; String secretValue = secret . getValue ( ) ; if ( StringUtils . isNotBlank ( secretValue ) ) { try { secretsService . delete ( secret . getAccountId ( ) , secret . getValue ( ) ) ; } catch ( IOException e ) { logger . error ( ""Error deleting secret {}: {}"" , secret . getId ( ) , e . getMessage ( ) ) ; throw new IllegalStateException ( e ) ; } } return null ; } }","public class A { @ Override public HandlerResult handle ( ProcessState state , ProcessInstance process ) { Secret secret = ( Secret ) state . getResource ( ) ; String secretValue = secret . getValue ( ) ; if ( StringUtils . isNotBlank ( secretValue ) ) { try { secretsService . delete ( secret . getAccountId ( ) , secret . getValue ( ) ) ; } catch ( IOException e ) { log . error ( ""Failed to delete secret from storage [{}]"" , secret . getId ( ) , e ) ; throw new IllegalStateException ( e ) ; } } return null ; } }","log . error ( ""Failed to delete secret from storage [{}]"" , secret . getId ( ) , e ) ;",Same Information
"public class A { @ Test public void testNumericAndRange ( ) throws Exception { log . info ( ""------ testNumericAndRange ------"" ) ; String query = ""("" + CityField . NUM . name ( ) + GTE_OP + ""30)"" + AND_OP + ""("" + CityField . NUM . name ( ) + LTE_OP + ""105)"" ; runTest ( ""((_Bounded_ = true) && ("" + query + ""))"" , query ) ; } }","public class A { @ Test public void testNumericAndRange ( ) throws Exception { log . info ( ""------ testNumericAndRange ------"" ) ; String query = ""("" + CityField . NUM . name ( ) + GTE_OP + ""30)"" + AND_OP + ""("" + CityField . NUM . name ( ) + LTE_OP + ""105)"" ; runTest ( ""((_Bounded_ = true) && ("" + query + ""))"" , query ) ; } }","log . info ( ""------ testNumericAndRange ------"" ) ;",Same Information
"public class A { @ Override public void forciblyInheritFingerprint ( final String fingerprint ) throws AuthorizationAccessException { if ( fingerprint == null || fingerprint . trim ( ) . isEmpty ( ) ) { logger . info ( ""Inheriting Empty Users & Groups. Will backup existing Policies, Users & Groups"" ) ; backupPoliciesUsersAndGroups ( ) ; purgePoliciesUsersAndGroups ( ) ; return ; } final PoliciesUsersAndGroups policiesUsersAndGroups = parsePoliciesUsersAndGroups ( fingerprint ) ; if ( isInheritable ( policiesUsersAndGroups ) ) { logger . debug ( ""Inheriting Polciies, Users & Groups"" ) ; inheritPoliciesUsersAndGroups ( policiesUsersAndGroups ) ; } else { logger . info ( ""Cannot directly inherit Policies, Users & Groups. Will backup existing Policies, Users & Groups, and then replace with proposed configuration"" ) ; backupPoliciesUsersAndGroups ( ) ; purgePoliciesUsersAndGroups ( ) ; addPoliciesUsersAndGroups ( policiesUsersAndGroups ) ; } } }","public class A { @ Override public void forciblyInheritFingerprint ( final String fingerprint ) throws AuthorizationAccessException { if ( fingerprint == null || fingerprint . trim ( ) . isEmpty ( ) ) { logger . info ( ""Inheriting Empty Policies, Users & Groups. Will backup existing Policies, Users & Groups first."" ) ; backupPoliciesUsersAndGroups ( ) ; purgePoliciesUsersAndGroups ( ) ; return ; } final PoliciesUsersAndGroups policiesUsersAndGroups = parsePoliciesUsersAndGroups ( fingerprint ) ; if ( isInheritable ( policiesUsersAndGroups ) ) { logger . debug ( ""Inheriting Polciies, Users & Groups"" ) ; inheritPoliciesUsersAndGroups ( policiesUsersAndGroups ) ; } else { logger . info ( ""Cannot directly inherit Policies, Users & Groups. Will backup existing Policies, Users & Groups, and then replace with proposed configuration"" ) ; backupPoliciesUsersAndGroups ( ) ; purgePoliciesUsersAndGroups ( ) ; addPoliciesUsersAndGroups ( policiesUsersAndGroups ) ; } } }","logger . info ( ""Inheriting Empty Policies, Users & Groups. Will backup existing Policies, Users & Groups first."" ) ;",Same Information
"public class A { @ SuppressFBWarnings ( ""DM_EXIT"" ) public static void main ( String [ ] args ) { final SplashScreen splash = SplashScreen . getSplashScreen ( ) ; if ( splash != null ) { splash . createGraphics ( ) ; splash . update ( ) ; } int code ; long lastLaunch = 0L ; do { code = new Launch ( ) . run ( ) ; if ( System . currentTimeMillis ( ) - lastLaunch < RELAUNCH_EXCLUSION_WINDOW_MILLIS ) { LOGGER . info ( ""Encountered a subsequent failure. Giving up."" ) ; break ; } else if ( code == 2 ) { LOGGER . info ( ""User requested restart."" ) ; } else { if ( code != 0 ) { logErrorFile ( ) ; } if ( code != 0 && ourRestart ) { LOGGER . info ( ""Restarted."" ) ; } else { LOGGER . info ( ""Exiting with status "" + code ) ; break ; } } lastLaunch = System . currentTimeMillis ( ) ; } while ( true ) ; System . exit ( code ) ; } }","public class A { @ SuppressFBWarnings ( ""DM_EXIT"" ) public static void main ( String [ ] args ) { final SplashScreen splash = SplashScreen . getSplashScreen ( ) ; if ( splash != null ) { splash . createGraphics ( ) ; splash . update ( ) ; } int code ; long lastLaunch = 0L ; do { code = new Launch ( ) . run ( ) ; if ( System . currentTimeMillis ( ) - lastLaunch < RELAUNCH_EXCLUSION_WINDOW_MILLIS ) { LOGGER . info ( ""Encountered a subsequent failure. Giving up."" ) ; break ; } else if ( code == 2 ) { LOGGER . info ( ""User requested restart."" ) ; } else { if ( code != 0 ) { logErrorFile ( ) ; } if ( code != 0 && ourRestart ) { LOGGER . info ( ""Unexpected exit. Restarting..."" ) ; } else { LOGGER . info ( ""Exiting with status "" + code ) ; break ; } } lastLaunch = System . currentTimeMillis ( ) ; } while ( true ) ; System . exit ( code ) ; } }","LOGGER . info ( ""Unexpected exit. Restarting..."" ) ;",Same Information
"public class A { protected void execQueryUsingH2 ( String queryFolder , boolean needSort ) throws Exception { logger . info ( ""---------- Running H2 queries: "" + queryFolder ) ; List < File > sqlFiles = getFilesFromFolder ( new File ( queryFolder ) , "".sql"" ) ; for ( File sqlFile : sqlFiles ) { String queryName = StringUtils . split ( sqlFile . getName ( ) , '.' ) [ 0 ] ; String sql = getTextFromFile ( sqlFile ) ; logger . info ( ""Running query: "" + queryName + "" with sql: "" + sql ) ; executeQuery ( newH2Connection ( ) , queryName , sql , needSort ) ; } } }","public class A { protected void execQueryUsingH2 ( String queryFolder , boolean needSort ) throws Exception { logger . info ( ""---------- Running H2 queries: "" + queryFolder ) ; List < File > sqlFiles = getFilesFromFolder ( new File ( queryFolder ) , "".sql"" ) ; for ( File sqlFile : sqlFiles ) { String queryName = StringUtils . split ( sqlFile . getName ( ) , '.' ) [ 0 ] ; String sql = getTextFromFile ( sqlFile ) ; logger . info ( ""Query Result from H2 - "" + queryName ) ; executeQuery ( newH2Connection ( ) , queryName , sql , needSort ) ; } } }","logger . info ( ""Query Result from H2 - "" + queryName ) ;",Same Information
"public class A { private void preLoadClass ( String agentId , long agentStartTimestamp , AgentStatMetricCollector < AgentStatMetricSnapshot > agentStatCollector ) { logger . debug ( ""pre-load class start"" ) ; CollectJob collectJob = new CollectJob ( EmptyDataSender . INSTANCE , agentId , agentStartTimestamp , agentStatCollector , 1 ) ; collectJob . run ( ) ; collectJob . run ( ) ; logger . debug ( ""pre-load class finish"" ) ; } }","public class A { private void preLoadClass ( String agentId , long agentStartTimestamp , AgentStatMetricCollector < AgentStatMetricSnapshot > agentStatCollector ) { logger . debug ( ""pre-load class start"" ) ; CollectJob collectJob = new CollectJob ( EmptyDataSender . INSTANCE , agentId , agentStartTimestamp , agentStatCollector , 1 ) ; collectJob . run ( ) ; collectJob . run ( ) ; logger . debug ( ""pre-load class end"" ) ; } }","logger . debug ( ""pre-load class end"" ) ;",Same Information
"public class A { @ Override public JsonObject getLeaderBoard ( String usecase ) throws InsightsCustomException { try { AutoMLConfig mlConfig = autoMLConfigDAL . getMLConfigByUsecase ( usecase ) ; String modelId = mlConfig . getModelId ( ) ; return h2oApiCommunicator . getLeaderBoard ( modelId ) ; } catch ( Exception e ) { log . error ( ""Error getting leaderboard: {}"" , usecase ) ; throw new InsightsCustomException ( ""Error getting leaderboard: "" + usecase ) ; } } }","public class A { @ Override public JsonObject getLeaderBoard ( String usecase ) throws InsightsCustomException { try { AutoMLConfig mlConfig = autoMLConfigDAL . getMLConfigByUsecase ( usecase ) ; String modelId = mlConfig . getModelId ( ) ; return h2oApiCommunicator . getLeaderBoard ( modelId ) ; } catch ( Exception e ) { log . error ( ""Error getting leaderboard: {} "" , usecase ) ; throw new InsightsCustomException ( ""Error getting leaderboard: "" + usecase ) ; } } }","log . error ( ""Error getting leaderboard: {} "" , usecase ) ;",Same Information
"public class A { public static boolean isUpdateStatement ( Statement statement ) { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( String . format ( ""Is '%s' Statement ? "" , statement . toString ( ) ) ) ; } if ( isSimpleStatement ( statement ) ) { return UPDATE_PATTERN . matcher ( maybeGetNormalizedQueryString ( statement ) ) . matches ( ) ; } else { return statement instanceof Update . Where || statement instanceof Update . Options ; } } }","public class A { public static boolean isUpdateStatement ( Statement statement ) { if ( LOGGER . isTraceEnabled ( ) ) { LOGGER . trace ( String . format ( ""Is '%s' Update statement ? "" , statement . toString ( ) ) ) ; } if ( isSimpleStatement ( statement ) ) { return UPDATE_PATTERN . matcher ( maybeGetNormalizedQueryString ( statement ) ) . matches ( ) ; } else { return statement instanceof Update . Where || statement instanceof Update . Options ; } } }","LOGGER . trace ( String . format ( ""Is '%s' Update statement ? "" , statement . toString ( ) ) ) ;",Same Information
"public class A { public void create ( final boolean baseline ) { callbackExecutor . onEvent ( Event . CREATE_SCHEMA ) ; int retries = 0 ; while ( true ) { try { ExecutionTemplateFactory . createExecutionTemplate ( connection . getJdbcConnection ( ) , database ) . execute ( ( ) -> { List < Schema > createdSchemas = new ArrayList < > ( ) ; for ( Schema schema : schemas ) { if ( ! schema . exists ( ) ) { if ( schema . getName ( ) == null ) { throw new FlywayException ( ""Unable to determine schema for the schema history table."" + "" Set a default schema for the connection or specify one using the defaultSchema property!"" ) ; } LOG . debug ( ""Creating schema: "" + schema ) ; schema . create ( ) ; createdSchemas . add ( schema ) ; } else { LOG . debug ( ""Schema {} already exists."" , schema . getName ( ) ) ; } } if ( ! createdSchemas . isEmpty ( ) ) { schemaHistory . create ( baseline ) ; schemaHistory . addSchemasMarker ( createdSchemas . toArray ( new Schema [ 0 ] ) ) ; } return null ; } ) ; return ; } catch ( RuntimeException e ) { if ( ++ retries >= 10 ) { throw e ; } try { LOG . debug ( ""Schema creation failed. Retrying in 1 sec ..."" ) ; Thread . sleep ( 1000 ) ; } catch ( InterruptedException e1 ) { } } } } }","public class A { public void create ( final boolean baseline ) { callbackExecutor . onEvent ( Event . CREATE_SCHEMA ) ; int retries = 0 ; while ( true ) { try { ExecutionTemplateFactory . createExecutionTemplate ( connection . getJdbcConnection ( ) , database ) . execute ( ( ) -> { List < Schema > createdSchemas = new ArrayList < > ( ) ; for ( Schema schema : schemas ) { if ( ! schema . exists ( ) ) { if ( schema . getName ( ) == null ) { throw new FlywayException ( ""Unable to determine schema for the schema history table."" + "" Set a default schema for the connection or specify one using the defaultSchema property!"" ) ; } LOG . debug ( ""Creating schema: "" + schema ) ; schema . create ( ) ; createdSchemas . add ( schema ) ; } else { LOG . debug ( ""Skipping creation of existing schema: "" + schema ) ; } } if ( ! createdSchemas . isEmpty ( ) ) { schemaHistory . create ( baseline ) ; schemaHistory . addSchemasMarker ( createdSchemas . toArray ( new Schema [ 0 ] ) ) ; } return null ; } ) ; return ; } catch ( RuntimeException e ) { if ( ++ retries >= 10 ) { throw e ; } try { LOG . debug ( ""Schema creation failed. Retrying in 1 sec ..."" ) ; Thread . sleep ( 1000 ) ; } catch ( InterruptedException e1 ) { } } } } }","LOG . debug ( ""Skipping creation of existing schema: "" + schema ) ;",Same Information
"public class A { public void open ( ) { if ( isConnected ( ) ) { logger . debug ( ""Socket already connected"" ) ; } else { sslContextFactory . setTrustAll ( true ) ; sslContextFactory . setEndpointIdentificationAlgorithm ( null ) ; WebSocketClient client = this . client ; if ( client == null ) { client = new WebSocketClient ( sslContextFactory , websocketExecutor ) ; client . setMaxIdleTimeout ( 600 * 1000 ) ; this . client = client ; } TibberWebSocketListener socket = this . socket ; if ( socket == null ) { socket = new TibberWebSocketListener ( ) ; this . socket = socket ; } ClientUpgradeRequest newRequest = new ClientUpgradeRequest ( ) ; newRequest . setHeader ( ""Authorization"" , ""Bearer "" + tibberConfig . getToken ( ) ) ; newRequest . setSubProtocols ( ""graphql-subscriptions"" ) ; try { logger . debug ( ""Starting Websocket connection"" ) ; client . start ( ) ; } catch ( Exception e ) { logger . warn ( ""Websocket Start Exception: {}"" , e . getMessage ( ) ) ; } try { logger . debug ( ""Connecting Websocket connection"" ) ; sessionFuture = client . connect ( socket , new URI ( SUBSCRIPTION_URL ) , newRequest ) ; } catch ( IOException e ) { logger . warn ( ""Websocket Connect Exception: {}"" , e . getMessage ( ) ) ; } catch ( URISyntaxException e ) { logger . warn ( ""Websocket URI Exception: {}"" , e . getMessage ( ) ) ; } } } }","public class A { public void open ( ) { if ( isConnected ( ) ) { logger . debug ( ""Open: connection is already open"" ) ; } else { sslContextFactory . setTrustAll ( true ) ; sslContextFactory . setEndpointIdentificationAlgorithm ( null ) ; WebSocketClient client = this . client ; if ( client == null ) { client = new WebSocketClient ( sslContextFactory , websocketExecutor ) ; client . setMaxIdleTimeout ( 600 * 1000 ) ; this . client = client ; } TibberWebSocketListener socket = this . socket ; if ( socket == null ) { socket = new TibberWebSocketListener ( ) ; this . socket = socket ; } ClientUpgradeRequest newRequest = new ClientUpgradeRequest ( ) ; newRequest . setHeader ( ""Authorization"" , ""Bearer "" + tibberConfig . getToken ( ) ) ; newRequest . setSubProtocols ( ""graphql-subscriptions"" ) ; try { logger . debug ( ""Starting Websocket connection"" ) ; client . start ( ) ; } catch ( Exception e ) { logger . warn ( ""Websocket Start Exception: {}"" , e . getMessage ( ) ) ; } try { logger . debug ( ""Connecting Websocket connection"" ) ; sessionFuture = client . connect ( socket , new URI ( SUBSCRIPTION_URL ) , newRequest ) ; } catch ( IOException e ) { logger . warn ( ""Websocket Connect Exception: {}"" , e . getMessage ( ) ) ; } catch ( URISyntaxException e ) { logger . warn ( ""Websocket URI Exception: {}"" , e . getMessage ( ) ) ; } } } }","logger . debug ( ""Open: connection is already open"" ) ;",Same Information
"public class A { public RiskIncidence createRiskIncidence ( RiskIncidence riskIncidence , boolean commit ) throws AlreadyExistsException , NotFoundException , AuthorizationDeniedException , GenericException { RodaCoreFactory . checkIfWriteIsAllowedAndIfFalseThrowException ( nodeType ) ; try { riskIncidence . setId ( IdUtils . createUUID ( ) ) ; riskIncidence . setDetectedOn ( new Date ( ) ) ; String riskIncidenceAsJson = JsonUtils . getJsonFromObject ( riskIncidence ) ; StoragePath riskIncidencePath = ModelUtils . getRiskIncidenceStoragePath ( riskIncidence . getId ( ) ) ; storage . createBinary ( riskIncidencePath , new StringContentPayload ( riskIncidenceAsJson ) , false ) ; } catch ( GenericException | RequestNotValidException | AuthorizationDeniedException | NotFoundException | AlreadyExistsException e ) { LOGGER . error ( ""Error creating riskIncidence"" , e ) ; } notifyRiskIncidenceCreatedOrUpdated ( riskIncidence , commit ) . failOnError ( ) ; return riskIncidence ; } }","public class A { public RiskIncidence createRiskIncidence ( RiskIncidence riskIncidence , boolean commit ) throws AlreadyExistsException , NotFoundException , AuthorizationDeniedException , GenericException { RodaCoreFactory . checkIfWriteIsAllowedAndIfFalseThrowException ( nodeType ) ; try { riskIncidence . setId ( IdUtils . createUUID ( ) ) ; riskIncidence . setDetectedOn ( new Date ( ) ) ; String riskIncidenceAsJson = JsonUtils . getJsonFromObject ( riskIncidence ) ; StoragePath riskIncidencePath = ModelUtils . getRiskIncidenceStoragePath ( riskIncidence . getId ( ) ) ; storage . createBinary ( riskIncidencePath , new StringContentPayload ( riskIncidenceAsJson ) , false ) ; } catch ( GenericException | RequestNotValidException | AuthorizationDeniedException | NotFoundException | AlreadyExistsException e ) { LOGGER . error ( ""Error creating risk incidence in storage"" , e ) ; } notifyRiskIncidenceCreatedOrUpdated ( riskIncidence , commit ) . failOnError ( ) ; return riskIncidence ; } }","LOGGER . error ( ""Error creating risk incidence in storage"" , e ) ;",Same Information
"public class A { @ Secured ( { ServicesData . ROLE_GET_SECURITY_PROFILES } ) @ PostMapping ( CommonConstants . PATH_CHECK ) public ResponseEntity < Void > check ( @ RequestBody SecurityProfileDto accessContractDto , @ RequestHeader ( value = CommonConstants . X_TENANT_ID_HEADER ) Integer tenant ) { LOGGER . debug ( ""check security profile={}"" , accessContractDto ) ; final boolean exist = securityProfileExternalService . check ( accessContractDto ) ; return RestUtils . buildBooleanResponse ( exist ) ; } }","public class A { @ Secured ( { ServicesData . ROLE_GET_SECURITY_PROFILES } ) @ PostMapping ( CommonConstants . PATH_CHECK ) public ResponseEntity < Void > check ( @ RequestBody SecurityProfileDto accessContractDto , @ RequestHeader ( value = CommonConstants . X_TENANT_ID_HEADER ) Integer tenant ) { LOGGER . debug ( ""check exist accessContract={}"" , accessContractDto ) ; final boolean exist = securityProfileExternalService . check ( accessContractDto ) ; return RestUtils . buildBooleanResponse ( exist ) ; } }","LOGGER . debug ( ""check exist accessContract={}"" , accessContractDto ) ;",Same Information
"public class A { private void getServerProfileTransformation ( ) { ServerProfile serverProfile = this . serverProfileClient . getByName ( SERVER_PROFILE_NAME ) . get ( 0 ) ; String enclosureGroupUri = enclosureGroupClient . getByName ( EnclosureGroupClientSample . ENCLOSURE_GROUP_NAME ) . get ( 0 ) . getUri ( ) ; ServerProfile serverProfileUpdated = serverProfileClient . getTransformation ( serverProfile . getResourceId ( ) , ServerHardwareTypeClientSample . SERVER_HARDWARE_TYPE_URI , enclosureGroupUri ) ; LOGGER . info ( ""Server Profile transformation returned to client: {}"" , serverProfileUpdated . toJsonString ( ) ) ; } }","public class A { private void getServerProfileTransformation ( ) { ServerProfile serverProfile = this . serverProfileClient . getByName ( SERVER_PROFILE_NAME ) . get ( 0 ) ; String enclosureGroupUri = enclosureGroupClient . getByName ( EnclosureGroupClientSample . ENCLOSURE_GROUP_NAME ) . get ( 0 ) . getUri ( ) ; ServerProfile serverProfileUpdated = serverProfileClient . getTransformation ( serverProfile . getResourceId ( ) , ServerHardwareTypeClientSample . SERVER_HARDWARE_TYPE_URI , enclosureGroupUri ) ; LOGGER . info ( ""ServerProfile object returned to client : "" + serverProfileUpdated . toJsonString ( ) ) ; } }","LOGGER . info ( ""ServerProfile object returned to client : "" + serverProfileUpdated . toJsonString ( ) ) ;",Same Information
"public class A { protected boolean restoreLowerBoundIdOrSkipTable ( Connection sourceConnect , TableOperationParams operationParams , ShardRecovery recoveryValue ) throws SQLException { Objects . requireNonNull ( sourceConnect , ""source connection is NULL"" ) ; Objects . requireNonNull ( operationParams , ""operationParams is NULL"" ) ; Objects . requireNonNull ( recoveryValue , ""recoveryValue is NULL"" ) ; if ( recoveryValue . getObjectName ( ) == null || recoveryValue . getObjectName ( ) . isEmpty ( ) ) { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( ""START object '{}' from = {}"" , operationParams . tableName , recoveryValue ) ; } else if ( recoveryValue . getProcessedObject ( ) != null && ! recoveryValue . getProcessedObject ( ) . isEmpty ( ) && isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { log . debug ( ""Already '{}' in = {}"" , operationParams . tableName , recoveryValue ) ; return true ; } else { if ( recoveryValue . getObjectName ( ) != null && recoveryValue . getObjectName ( ) . equalsIgnoreCase ( currentTableName ) && ! isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { this . lowerBoundIdValue = recoveryValue . getLastColumnValue ( ) ; log . debug ( ""RESTORED object '{}' from = {}"" , operationParams . tableName , recoveryValue ) ; } else { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( ""GO NEXT object '{}' by = {}"" , operationParams . tableName , recoveryValue ) ; } } return false ; } }","public class A { protected boolean restoreLowerBoundIdOrSkipTable ( Connection sourceConnect , TableOperationParams operationParams , ShardRecovery recoveryValue ) throws SQLException { Objects . requireNonNull ( sourceConnect , ""source connection is NULL"" ) ; Objects . requireNonNull ( operationParams , ""operationParams is NULL"" ) ; Objects . requireNonNull ( recoveryValue , ""recoveryValue is NULL"" ) ; if ( recoveryValue . getObjectName ( ) == null || recoveryValue . getObjectName ( ) . isEmpty ( ) ) { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( ""START object '{}' from = {}"" , operationParams . tableName , recoveryValue ) ; } else if ( recoveryValue . getProcessedObject ( ) != null && ! recoveryValue . getProcessedObject ( ) . isEmpty ( ) && isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { log . debug ( ""SKIP object '{}' by = {}"" , operationParams . tableName , recoveryValue ) ; return true ; } else { if ( recoveryValue . getObjectName ( ) != null && recoveryValue . getObjectName ( ) . equalsIgnoreCase ( currentTableName ) && ! isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { this . lowerBoundIdValue = recoveryValue . getLastColumnValue ( ) ; log . debug ( ""RESTORED object '{}' from = {}"" , operationParams . tableName , recoveryValue ) ; } else { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( ""GO NEXT object '{}' by = {}"" , operationParams . tableName , recoveryValue ) ; } } return false ; } }","log . debug ( ""SKIP object '{}' by = {}"" , operationParams . tableName , recoveryValue ) ;",Same Information
"public class A { @ Override public Model getConciseBoundedDescription ( String resource , int depth , boolean withTypesForLeafs ) { logger . debug ( ""Creating CBD for resource {} with depth {}"" , resource , depth ) ; Model cbd = ModelFactory . createDefaultModel ( ) ; cbd . add ( getIncomingModel ( resource , depth ) ) ; cbd . add ( getOutgoingModel ( resource , depth ) ) ; logger . debug ( ""CBD size: {}"" , cbd . size ( ) ) ; return cbd ; } }","public class A { @ Override public Model getConciseBoundedDescription ( String resource , int depth , boolean withTypesForLeafs ) { logger . debug ( ""computing CBD of depth {} for {} ..."" , resource , depth ) ; Model cbd = ModelFactory . createDefaultModel ( ) ; cbd . add ( getIncomingModel ( resource , depth ) ) ; cbd . add ( getOutgoingModel ( resource , depth ) ) ; logger . debug ( ""CBD size: {}"" , cbd . size ( ) ) ; return cbd ; } }","logger . debug ( ""computing CBD of depth {} for {} ..."" , resource , depth ) ;",Same Information
"public class A { @ Test public void test ( ) throws Exception { String publicWebappURL = OneRecordingServer . getPublicWebappUrl ( ) ; log . debug ( ""Start uploading content"" ) ; File fileToUpload = new File ( ""test-files/logo.png"" ) ; uploadFileWithCURL ( publicWebappURL + ""repository_servlet/video-upload"" , fileToUpload ) ; log . debug ( ""Waiting 10 seconds to auto-termination..."" ) ; Thread . sleep ( 10 * 1000 ) ; File downloadedFile = new File ( ""test-files/sampleDownload.txt"" ) ; log . debug ( ""Start downloading file"" ) ; downloadFromURL ( publicWebappURL + ""repository_servlet/video-download"" , downloadedFile ) ; boolean equalFiles = TestUtils . equalFiles ( fileToUpload , downloadedFile ) ; if ( equalFiles ) { log . debug ( ""The uploadad and downloaded files are equal"" ) ; } else { log . debug ( ""The uploadad and downloaded files are not equal"" ) ; } assertTrue ( ""The uploadad and downloaded files are different"" , equalFiles ) ; } }","public class A { @ Test public void test ( ) throws Exception { String publicWebappURL = OneRecordingServer . getPublicWebappUrl ( ) ; log . debug ( ""Start uploading content"" ) ; File fileToUpload = new File ( ""test-files/logo.png"" ) ; uploadFileWithCURL ( publicWebappURL + ""repository_servlet/video-upload"" , fileToUpload ) ; log . debug ( ""Waiting 10 seconds to auto-termination..."" ) ; Thread . sleep ( 10 * 1000 ) ; File downloadedFile = new File ( ""test-files/sampleDownload.txt"" ) ; log . debug ( ""Start downloading file"" ) ; downloadFromURL ( publicWebappURL + ""repository_servlet/video-download"" , downloadedFile ) ; boolean equalFiles = TestUtils . equalFiles ( fileToUpload , downloadedFile ) ; if ( equalFiles ) { log . debug ( ""The uploadad and downloaded files are equal"" ) ; } else { log . debug ( ""The uploadad and downloaded files are different"" ) ; } assertTrue ( ""The uploadad and downloaded files are different"" , equalFiles ) ; } }","log . debug ( ""The uploadad and downloaded files are different"" ) ;",Same Information
"public class A { private void insertObservations ( ) throws UnsupportedEncodingException , IOException , XmlException , OwsExceptionReport { ExecutorService threadPool = Executors . newFixedThreadPool ( 5 , new GroupedAndNamedThreadFactory ( ""52n-sample-data-insert-observations"" ) ) ; for ( File observationFile : getFilesBySuffix ( OBS_XML_FILE_ENDING ) ) { threadPool . submit ( new InsertObservationTask ( observationFile ) ) ; } try { threadPool . shutdown ( ) ; while ( ! threadPool . isTerminated ( ) ) { Thread . sleep ( THREADPOOL_SLEEP_BETWEEN_CHECKS ) ; } } catch ( InterruptedException e ) { LOG . error ( ""Thread was interrupted!"" , e ) ; } exceptions . throwIfNotEmpty ( ) ; } }","public class A { private void insertObservations ( ) throws UnsupportedEncodingException , IOException , XmlException , OwsExceptionReport { ExecutorService threadPool = Executors . newFixedThreadPool ( 5 , new GroupedAndNamedThreadFactory ( ""52n-sample-data-insert-observations"" ) ) ; for ( File observationFile : getFilesBySuffix ( OBS_XML_FILE_ENDING ) ) { threadPool . submit ( new InsertObservationTask ( observationFile ) ) ; } try { threadPool . shutdown ( ) ; while ( ! threadPool . isTerminated ( ) ) { Thread . sleep ( THREADPOOL_SLEEP_BETWEEN_CHECKS ) ; } } catch ( InterruptedException e ) { LOG . error ( ""Insert obervations thread was interrupted!"" , e ) ; } exceptions . throwIfNotEmpty ( ) ; } }","LOG . error ( ""Insert obervations thread was interrupted!"" , e ) ;",Same Information
"public class A { private static boolean checkLineContingency ( Contingency contingency , LineContingency element , Network network ) { Line line = network . getLine ( element . getId ( ) ) ; if ( line == null || ( element . getVoltageLevelId ( ) != null && ! ( element . getVoltageLevelId ( ) . equals ( line . getTerminal1 ( ) . getVoltageLevel ( ) . getId ( ) ) || element . getVoltageLevelId ( ) . equals ( line . getTerminal2 ( ) . getVoltageLevel ( ) . getId ( ) ) ) ) ) { LOGGER . warn ( ""HVEND_LINK_NOT_FOUND: {}"" , element . getId ( ) ) ; return false ; } return true ; } }","public class A { private static boolean checkLineContingency ( Contingency contingency , LineContingency element , Network network ) { Line line = network . getLine ( element . getId ( ) ) ; if ( line == null || ( element . getVoltageLevelId ( ) != null && ! ( element . getVoltageLevelId ( ) . equals ( line . getTerminal1 ( ) . getVoltageLevel ( ) . getId ( ) ) || element . getVoltageLevelId ( ) . equals ( line . getTerminal2 ( ) . getVoltageLevel ( ) . getId ( ) ) ) ) ) { LOGGER . warn ( ""Line '{}' of contingency '{}' not found"" , element . getId ( ) , contingency . getId ( ) ) ; return false ; } return true ; } }","LOGGER . warn ( ""Line '{}' of contingency '{}' not found"" , element . getId ( ) , contingency . getId ( ) ) ;",Meaningless
"public class A { @ Override public void unscheduleRunOnceJob ( String subject , String externalId ) { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( ""Task: ["" + subject + ""]"" ) ; } JobId jobId = new RunOnceJobId ( subject , externalId ) ; logObjectIfNotNull ( jobId ) ; unscheduleJob ( jobId . value ( ) ) ; } }","public class A { @ Override public void unscheduleRunOnceJob ( String subject , String externalId ) { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( format ( ""unscheduling run once job: "" + LOG_SUBJECT_EXTERNAL_ID , subject , externalId ) ) ; } JobId jobId = new RunOnceJobId ( subject , externalId ) ; logObjectIfNotNull ( jobId ) ; unscheduleJob ( jobId . value ( ) ) ; } }","LOGGER . debug ( format ( ""unscheduling run once job: "" + LOG_SUBJECT_EXTERNAL_ID , subject , externalId ) ) ;",Meaningless
"public class A { @ Test public void testQuery003 ( ) throws Exception { log . info ( ""------ Test (a AND b), where all terms are supported ------"" ) ; Set < String > expected = new HashSet < > ( ) ; String query = CarField . COLOR . name ( ) + "" =~ 'bl.*s.*' and "" + CarField . WHEELS . name ( ) + "" == '4'"" ; runTest ( query , expected ) ; } }","public class A { @ Test public void testQuery003 ( ) throws Exception { log . info ( ""------ Test a*.* AND b ------"" ) ; Set < String > expected = new HashSet < > ( ) ; String query = CarField . COLOR . name ( ) + "" =~ 'bl.*s.*' and "" + CarField . WHEELS . name ( ) + "" == '4'"" ; runTest ( query , expected ) ; } }","log . info ( ""------ Test a*.* AND b ------"" ) ;",Meaningless
"public class A { @ Override public void paint ( Graphics2D g2d ) { super . paint ( g2d ) ; if ( LOGGER . isTraceEnabled ( ) ) { for ( TimelineLayer layer : myLayers ) { long t0 = System . nanoTime ( ) ; layer . paint ( g2d ) ; LOGGER . trace ( StringUtilities . formatTimingMessage ( ""Time to paint timeline: "" , ( t0 - t0 ) / 1000.0 ) ) ; } } else { for ( TimelineLayer layer : myLayers ) { layer . paint ( g2d ) ; } } } }","public class A { @ Override public void paint ( Graphics2D g2d ) { super . paint ( g2d ) ; if ( LOGGER . isTraceEnabled ( ) ) { for ( TimelineLayer layer : myLayers ) { long t0 = System . nanoTime ( ) ; layer . paint ( g2d ) ; LOGGER . trace ( StringUtilities . formatTimingMessage ( ""Time to paint layer "" + layer . getClass ( ) . getSimpleName ( ) + "": "" , System . nanoTime ( ) - t0 ) ) ; } } else { for ( TimelineLayer layer : myLayers ) { layer . paint ( g2d ) ; } } } }","LOGGER . trace ( StringUtilities . formatTimingMessage ( ""Time to paint layer "" + layer . getClass ( ) . getSimpleName ( ) + "": "" , System . nanoTime ( ) - t0 ) ) ;",Meaningless
"public class A { public void stop ( ) { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Stopping JMS Connection"" ) ; } if ( serializedExecutor != null ) { serializedExecutor . shutdownNow ( ) ; } serializedExecutor = null ; } }","public class A { public void stop ( ) { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""{} Stopping leadership election..."" , logPrefix ( ) ) ; } if ( serializedExecutor != null ) { serializedExecutor . shutdownNow ( ) ; } serializedExecutor = null ; } }","logger . debug ( ""{} Stopping leadership election..."" , logPrefix ( ) ) ;",Meaningless
"public class A { void rescheduleLeftOverJobs ( ) { final Iterator < Long > jobs = jobDaoProvider . get ( ) . getAllJobIds ( JobStatus . SUBMITTED ) ; int resubmitcount = 0 ; while ( jobs . hasNext ( ) ) { long oldID = jobs . next ( ) ; long newID = jobDaoProvider . get ( ) . rescheduleJob ( oldID ) ; log . info ( ""Rescheduling job {} of start over {} times."" , oldID , newID ) ; ++ resubmitcount ; } log . info ( ""{} jobs has been resubmitted."" , resubmitcount ) ; } }","public class A { void rescheduleLeftOverJobs ( ) { final Iterator < Long > jobs = jobDaoProvider . get ( ) . getAllJobIds ( JobStatus . SUBMITTED ) ; int resubmitcount = 0 ; while ( jobs . hasNext ( ) ) { long oldID = jobs . next ( ) ; long newID = jobDaoProvider . get ( ) . rescheduleJob ( oldID ) ; log . info ( ""Resubmitting old job {} as {}"" , oldID , newID ) ; ++ resubmitcount ; } log . info ( ""{} jobs has been resubmitted."" , resubmitcount ) ; } }","log . info ( ""Resubmitting old job {} as {}"" , oldID , newID ) ;",Meaningless
"public class A { private boolean acquire ( ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Attempting to acquire leader lease..."" ) ; } Duration retryPeriod = config . getRetryPeriod ( ) ; long retryPeriodMillis = retryPeriod . toMillis ( ) ; AtomicBoolean acquired = new AtomicBoolean ( false ) ; ScheduledFuture scheduledFuture = scheduledWorkers . scheduleWithFixedDelay ( ( ) -> { Future < Boolean > future = leaseWorkers . submit ( this :: tryAcquireOrRenew ) ; try { Boolean success = future . get ( retryPeriodMillis , TimeUnit . MILLISECONDS ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( ""The tryAcquireOrRenew result is {}"" , success ) ; } acquired . set ( success ) ; } catch ( CancellationException e ) { log . info ( ""Retry lease has been cancelled"" ) ; } catch ( Throwable t ) { this . exceptionHandler . accept ( t ) ; future . cancel ( true ) ; } finally { maybeReportTransition ( ) ; } } , 0 , Double . valueOf ( retryPeriodMillis * ( JITTER_FACTOR * Math . random ( ) + 1 ) ) . longValue ( ) , TimeUnit . MILLISECONDS ) ; try { while ( ! acquired . get ( ) ) { Thread . sleep ( retryPeriodMillis ) ; } } catch ( InterruptedException e ) { log . error ( ""LeaderElection acquire loop gets interrupted"" , e ) ; return false ; } finally { scheduledFuture . cancel ( true ) ; } return true ; } }","public class A { private boolean acquire ( ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Attempting to acquire leader lease..."" ) ; } Duration retryPeriod = config . getRetryPeriod ( ) ; long retryPeriodMillis = retryPeriod . toMillis ( ) ; AtomicBoolean acquired = new AtomicBoolean ( false ) ; ScheduledFuture scheduledFuture = scheduledWorkers . scheduleWithFixedDelay ( ( ) -> { Future < Boolean > future = leaseWorkers . submit ( this :: tryAcquireOrRenew ) ; try { Boolean success = future . get ( retryPeriodMillis , TimeUnit . MILLISECONDS ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( ""The tryAcquireOrRenew result is {}"" , success ) ; } acquired . set ( success ) ; } catch ( CancellationException e ) { log . info ( ""Processing tryAcquireOrRenew successfully canceled"" ) ; } catch ( Throwable t ) { this . exceptionHandler . accept ( t ) ; future . cancel ( true ) ; } finally { maybeReportTransition ( ) ; } } , 0 , Double . valueOf ( retryPeriodMillis * ( JITTER_FACTOR * Math . random ( ) + 1 ) ) . longValue ( ) , TimeUnit . MILLISECONDS ) ; try { while ( ! acquired . get ( ) ) { Thread . sleep ( retryPeriodMillis ) ; } } catch ( InterruptedException e ) { log . error ( ""LeaderElection acquire loop gets interrupted"" , e ) ; return false ; } finally { scheduledFuture . cancel ( true ) ; } return true ; } }","log . info ( ""Processing tryAcquireOrRenew successfully canceled"" ) ;",Meaningful
"public class A { public void resumeMinorGC ( ) { if ( suspendMinorCompaction . compareAndSet ( true , false ) ) { log . info ( ""Resuming minor gc."" ) ; } } }","public class A { public void resumeMinorGC ( ) { if ( suspendMinorCompaction . compareAndSet ( true , false ) ) { LOG . info ( ""{} Minor Compaction back to normal since bookie has enough space now."" , Thread . currentThread ( ) . getName ( ) ) ; } } }","LOG . info ( ""{} Minor Compaction back to normal since bookie has enough space now."" , Thread . currentThread ( ) . getName ( ) ) ;",Meaningful
"public class A { @ Path ( ""/list"" ) @ GET @ Produces ( MediaType . APPLICATION_JSON ) @ Operation ( summary = ""Get all phasing only entities by using first/last index"" , description = ""Get all phasing only entities by using first/last index"" , tags = { ""accounts"" } , responses = { @ ApiResponse ( responseCode = ""200"" , description = ""Successful execution"" , content = @ Content ( mediaType = ""application/json"" , schema = @ Schema ( implementation = AccountControlPhasingResponse . class ) ) ) } ) @ PermitAll public Response getAllPhasingOnlyControls ( @ Parameter ( description = ""A zero-based index to the first, last asset ID to retrieve (optional)."" , schema = @ Schema ( implementation = FirstLastIndexBeanParam . class ) ) @ BeanParam FirstLastIndexBeanParam indexBeanParam ) { ResponseBuilder response = ResponseBuilder . startTiming ( ) ; indexBeanParam . adjustIndexes ( maxAPIFetchRecords ) ; log . trace ( ""Started getAllPhasingOnlyControls, startTiming: {}"" , response . startTiming ( ) ) ; AccountControlPhasingResponse dto = new AccountControlPhasingResponse ( ) ; dto . phasingOnlyControls = accountControlPhasingService . getAllStream ( indexBeanParam . getFirstIndex ( ) , indexBeanParam . getLastIndex ( ) ) . map ( item -> accountControlPhasingConverter . convert ( item ) ) . collect ( Collectors . toList ( ) ) ; log . trace ( ""getAllPhasingOnlyControls result: {}"" , dto ) ; return response . bind ( dto ) . build ( ) ; } }","public class A { @ Path ( ""/list"" ) @ GET @ Produces ( MediaType . APPLICATION_JSON ) @ Operation ( summary = ""Get all phasing only entities by using first/last index"" , description = ""Get all phasing only entities by using first/last index"" , tags = { ""accounts"" } , responses = { @ ApiResponse ( responseCode = ""200"" , description = ""Successful execution"" , content = @ Content ( mediaType = ""application/json"" , schema = @ Schema ( implementation = AccountControlPhasingResponse . class ) ) ) } ) @ PermitAll public Response getAllPhasingOnlyControls ( @ Parameter ( description = ""A zero-based index to the first, last asset ID to retrieve (optional)."" , schema = @ Schema ( implementation = FirstLastIndexBeanParam . class ) ) @ BeanParam FirstLastIndexBeanParam indexBeanParam ) { ResponseBuilder response = ResponseBuilder . startTiming ( ) ; indexBeanParam . adjustIndexes ( maxAPIFetchRecords ) ; log . trace ( ""Started getAllPhasingOnlyControls : \t indexBeanParam={}"" , indexBeanParam ) ; AccountControlPhasingResponse dto = new AccountControlPhasingResponse ( ) ; dto . phasingOnlyControls = accountControlPhasingService . getAllStream ( indexBeanParam . getFirstIndex ( ) , indexBeanParam . getLastIndex ( ) ) . map ( item -> accountControlPhasingConverter . convert ( item ) ) . collect ( Collectors . toList ( ) ) ; log . trace ( ""getAllPhasingOnlyControls result: {}"" , dto ) ; return response . bind ( dto ) . build ( ) ; } }","log . trace ( ""Started getAllPhasingOnlyControls : \t indexBeanParam={}"" , indexBeanParam ) ;",Meaningful
"public class A { public String getDateVsTaskInstanceCount ( String filters ) { String sortedResult = """" ; try { if ( BPMNAnalyticsCoreUtils . isDASAnalyticsActivated ( ) ) { JSONObject filterObj = new JSONObject ( filters ) ; long from = filterObj . getLong ( BPMNAnalyticsCoreConstants . START_TIME ) ; long to = filterObj . getLong ( BPMNAnalyticsCoreConstants . END_TIME ) ; String processId = filterObj . getString ( BPMNAnalyticsCoreConstants . PROCESS_ID ) ; JSONArray taskIdList = filterObj . getJSONArray ( BPMNAnalyticsCoreConstants . TASK_ID_LIST ) ; AggregateField countField = new AggregateField ( ) ; countField . setFieldName ( BPMNAnalyticsCoreConstants . ALL ) ; countField . setAggregate ( BPMNAnalyticsCoreConstants . COUNT ) ; countField . setAlias ( BPMNAnalyticsCoreConstants . TASK_INSTANCE_COUNT ) ; ArrayList < AggregateField > aggregateFields = new ArrayList < > ( ) ; aggregateFields . add ( countField ) ; AggregateQuery query = new AggregateQuery ( ) ; query . setTableName ( BPMNAnalyticsCoreConstants . TASK_USAGE_TABLE ) ; query . setGroupByField ( BPMNAnalyticsCoreConstants . FINISHED_TIME ) ; String queryStr = BPMNAnalyticsCoreUtils . getDateRangeQuery ( BPMNAnalyticsCoreConstants . COLUMN_FINISHED_TIME , from , to ) ; queryStr += ""AND "" + ""processDefinitionId:"" + ""\""'"" + processId + ""'\"""" ; if ( taskIdList . length ( ) != 0 ) { queryStr += "" AND "" ; for ( int i = 0 ; i < taskIdList . length ( ) ; i ++ ) { if ( i == 0 ) { queryStr += ""(taskDefinitionKey:"" + ""\""'"" + taskIdList . getString ( i ) + ""'\"""" ; } else { queryStr += "" OR "" + ""taskDefinitionKey:"" + ""\""'"" + taskIdList . getString ( i ) + ""'\"""" ; } if ( i == taskIdList . length ( ) - 1 ) { queryStr += "")"" ; } } } query . setQuery ( queryStr ) ; query . setAggregateFields ( aggregateFields ) ; String result = BPMNAnalyticsCoreRestClient . post ( BPMNAnalyticsCoreUtils . getURL ( BPMNAnalyticsCoreConstants . ANALYTICS_AGGREGATE ) , BPMNAnalyticsCoreUtils . getJSONString ( query ) ) ; JSONArray unsortedResultArray = new JSONArray ( result ) ; Hashtable < Long , Integer > table = new Hashtable < > ( ) ; if ( unsortedResultArray . length ( ) != 0 ) { for ( int i = 0 ; i < unsortedResultArray . length ( ) ; i ++ ) { JSONObject jsonObj = unsortedResultArray . getJSONObject ( i ) ; JSONObject values = jsonObj . getJSONObject ( BPMNAnalyticsCoreConstants . VALUES ) ; long completedTime = Long . parseLong ( values . getJSONArray ( BPMNAnalyticsCoreConstants . FINISHED_TIME ) . getString ( 0 ) ) ; int taskInstanceCount = values . getInt ( BPMNAnalyticsCoreConstants . TASK_INSTANCE_COUNT ) ; table . put ( completedTime , taskInstanceCount ) ; } sortedResult = BPMNAnalyticsCoreUtils . getLongKeySortedList ( table , BPMNAnalyticsCoreConstants . FINISHED_TIME , BPMNAnalyticsCoreConstants . TASK_INSTANCE_COUNT ) ; } } } catch ( JSONException | IOException | XMLStreamException e ) { log . error ( ""Error while getting Task Instance Count Result:"" , e ) ; } if ( log . isDebugEnabled ( ) ) { log . debug ( ""Date Vs Task Instance Count Result:"" + sortedResult ) ; } return sortedResult ; } }","public class A { public String getDateVsTaskInstanceCount ( String filters ) { String sortedResult = """" ; try { if ( BPMNAnalyticsCoreUtils . isDASAnalyticsActivated ( ) ) { JSONObject filterObj = new JSONObject ( filters ) ; long from = filterObj . getLong ( BPMNAnalyticsCoreConstants . START_TIME ) ; long to = filterObj . getLong ( BPMNAnalyticsCoreConstants . END_TIME ) ; String processId = filterObj . getString ( BPMNAnalyticsCoreConstants . PROCESS_ID ) ; JSONArray taskIdList = filterObj . getJSONArray ( BPMNAnalyticsCoreConstants . TASK_ID_LIST ) ; AggregateField countField = new AggregateField ( ) ; countField . setFieldName ( BPMNAnalyticsCoreConstants . ALL ) ; countField . setAggregate ( BPMNAnalyticsCoreConstants . COUNT ) ; countField . setAlias ( BPMNAnalyticsCoreConstants . TASK_INSTANCE_COUNT ) ; ArrayList < AggregateField > aggregateFields = new ArrayList < > ( ) ; aggregateFields . add ( countField ) ; AggregateQuery query = new AggregateQuery ( ) ; query . setTableName ( BPMNAnalyticsCoreConstants . TASK_USAGE_TABLE ) ; query . setGroupByField ( BPMNAnalyticsCoreConstants . FINISHED_TIME ) ; String queryStr = BPMNAnalyticsCoreUtils . getDateRangeQuery ( BPMNAnalyticsCoreConstants . COLUMN_FINISHED_TIME , from , to ) ; queryStr += ""AND "" + ""processDefinitionId:"" + ""\""'"" + processId + ""'\"""" ; if ( taskIdList . length ( ) != 0 ) { queryStr += "" AND "" ; for ( int i = 0 ; i < taskIdList . length ( ) ; i ++ ) { if ( i == 0 ) { queryStr += ""(taskDefinitionKey:"" + ""\""'"" + taskIdList . getString ( i ) + ""'\"""" ; } else { queryStr += "" OR "" + ""taskDefinitionKey:"" + ""\""'"" + taskIdList . getString ( i ) + ""'\"""" ; } if ( i == taskIdList . length ( ) - 1 ) { queryStr += "")"" ; } } } query . setQuery ( queryStr ) ; query . setAggregateFields ( aggregateFields ) ; String result = BPMNAnalyticsCoreRestClient . post ( BPMNAnalyticsCoreUtils . getURL ( BPMNAnalyticsCoreConstants . ANALYTICS_AGGREGATE ) , BPMNAnalyticsCoreUtils . getJSONString ( query ) ) ; JSONArray unsortedResultArray = new JSONArray ( result ) ; Hashtable < Long , Integer > table = new Hashtable < > ( ) ; if ( unsortedResultArray . length ( ) != 0 ) { for ( int i = 0 ; i < unsortedResultArray . length ( ) ; i ++ ) { JSONObject jsonObj = unsortedResultArray . getJSONObject ( i ) ; JSONObject values = jsonObj . getJSONObject ( BPMNAnalyticsCoreConstants . VALUES ) ; long completedTime = Long . parseLong ( values . getJSONArray ( BPMNAnalyticsCoreConstants . FINISHED_TIME ) . getString ( 0 ) ) ; int taskInstanceCount = values . getInt ( BPMNAnalyticsCoreConstants . TASK_INSTANCE_COUNT ) ; table . put ( completedTime , taskInstanceCount ) ; } sortedResult = BPMNAnalyticsCoreUtils . getLongKeySortedList ( table , BPMNAnalyticsCoreConstants . FINISHED_TIME , BPMNAnalyticsCoreConstants . TASK_INSTANCE_COUNT ) ; } } } catch ( JSONException | IOException | XMLStreamException e ) { log . error ( ""BPMN Analytics Core - Date Vs Task Instance Count TaskLevelMonitoring error."" , e ) ; } if ( log . isDebugEnabled ( ) ) { log . debug ( ""Date Vs Task Instance Count Result:"" + sortedResult ) ; } return sortedResult ; } }","log . error ( ""BPMN Analytics Core - Date Vs Task Instance Count TaskLevelMonitoring error."" , e ) ;",Meaningful
"public class A { @ Override public Operator run ( ) { long start = System . currentTimeMillis ( ) ; UResultOperator uResultOperator = new UResultOperator ( _planNode . run ( ) ) ; long end = System . currentTimeMillis ( ) ; logger . info ( ""Result plan: {}. App query: {}"" , uResultOperator . displayDetails ( ) , String . format ( "", Task query: %bms"" , end - start ) ) ; return uResultOperator ; } }","public class A { @ Override public Operator run ( ) { long start = System . currentTimeMillis ( ) ; UResultOperator uResultOperator = new UResultOperator ( _planNode . run ( ) ) ; long end = System . currentTimeMillis ( ) ; LOGGER . info ( ""InstanceResponsePlanNode.run took: "" + ( end - start ) ) ; return uResultOperator ; } }","LOGGER . info ( ""InstanceResponsePlanNode.run took: "" + ( end - start ) ) ;",Meaningful
"public class A { @ Nonnull private List < StdArrangementMatchRule > deserializeRules ( @ Nonnull Element element , @ Nullable final Set < StdArrangementRuleAliasToken > aliases ) { if ( aliases != null && myMixin instanceof MutableMixin ) { ( ( MutableMixin ) myMixin ) . setMyRuleAliases ( aliases ) ; } final List < StdArrangementMatchRule > rules = new ArrayList < StdArrangementMatchRule > ( ) ; for ( Object o : element . getChildren ( RULE_ELEMENT_NAME ) ) { Element ruleElement = ( Element ) o ; Element matcherElement = ruleElement . getChild ( MATCHER_ELEMENT_NAME ) ; if ( matcherElement == null ) { continue ; } StdArrangementEntryMatcher matcher = null ; for ( Object c : matcherElement . getChildren ( ) ) { matcher = myMatcherSerializer . deserialize ( ( Element ) c ) ; if ( matcher != null ) { break ; } } if ( matcher == null ) { return ContainerUtil . newSmartList ( ) ; } Element orderTypeElement = ruleElement . getChild ( ORDER_TYPE_ELEMENT_NAME ) ; ArrangementSettingsToken orderType = null ; if ( orderTypeElement != null ) { String orderTypeId = orderTypeElement . getText ( ) ; orderType = StdArrangementTokens . byId ( orderTypeId ) ; if ( orderType == null ) { orderType = myMixin . deserializeToken ( orderTypeId ) ; } if ( orderType == null ) { logger . warn ( ""Unable to deserialize order type {}. Unable to compare list of {}"" , orderTypeId , orderType ) ; } } if ( orderType == null ) { orderType = ArrangementMatchRule . DEFAULT_ORDER_TYPE ; } rules . add ( new StdArrangementMatchRule ( matcher , orderType ) ) ; } return rules ; } }","public class A { @ Nonnull private List < StdArrangementMatchRule > deserializeRules ( @ Nonnull Element element , @ Nullable final Set < StdArrangementRuleAliasToken > aliases ) { if ( aliases != null && myMixin instanceof MutableMixin ) { ( ( MutableMixin ) myMixin ) . setMyRuleAliases ( aliases ) ; } final List < StdArrangementMatchRule > rules = new ArrayList < StdArrangementMatchRule > ( ) ; for ( Object o : element . getChildren ( RULE_ELEMENT_NAME ) ) { Element ruleElement = ( Element ) o ; Element matcherElement = ruleElement . getChild ( MATCHER_ELEMENT_NAME ) ; if ( matcherElement == null ) { continue ; } StdArrangementEntryMatcher matcher = null ; for ( Object c : matcherElement . getChildren ( ) ) { matcher = myMatcherSerializer . deserialize ( ( Element ) c ) ; if ( matcher != null ) { break ; } } if ( matcher == null ) { return ContainerUtil . newSmartList ( ) ; } Element orderTypeElement = ruleElement . getChild ( ORDER_TYPE_ELEMENT_NAME ) ; ArrangementSettingsToken orderType = null ; if ( orderTypeElement != null ) { String orderTypeId = orderTypeElement . getText ( ) ; orderType = StdArrangementTokens . byId ( orderTypeId ) ; if ( orderType == null ) { orderType = myMixin . deserializeToken ( orderTypeId ) ; } if ( orderType == null ) { LOG . warn ( String . format ( ""Can't deserialize matching rule order type for id '%s'. Falling back to default (%s)"" , orderTypeId , ArrangementMatchRule . DEFAULT_ORDER_TYPE . getId ( ) ) ) ; } } if ( orderType == null ) { orderType = ArrangementMatchRule . DEFAULT_ORDER_TYPE ; } rules . add ( new StdArrangementMatchRule ( matcher , orderType ) ) ; } return rules ; } }","LOG . warn ( String . format ( ""Can't deserialize matching rule order type for id '%s'. Falling back to default (%s)"" , orderTypeId , ArrangementMatchRule . DEFAULT_ORDER_TYPE . getId ( ) ) ) ;",Meaningful
"public class A { @ Override public int addToInt ( Session session , int delta ) { SessionImpl impl = ( SessionImpl ) session ; IntVar var = impl . requireSet ( index , key ) ; int prev = var . intValue ( session ) ; if ( trace ) { log . trace ( ""#{} addToInt({}) -> {}"" , impl . uniqueId ( ) , key , prev , delta ) ; } var . set ( prev + delta ) ; return prev ; } }","public class A { @ Override public int addToInt ( Session session , int delta ) { SessionImpl impl = ( SessionImpl ) session ; IntVar var = impl . requireSet ( index , key ) ; int prev = var . intValue ( session ) ; if ( trace ) { log . trace ( ""#{} {} <- {}"" , impl . uniqueId ( ) , key , prev + delta ) ; } var . set ( prev + delta ) ; return prev ; } }","log . trace ( ""#{} {} <- {}"" , impl . uniqueId ( ) , key , prev + delta ) ;",Meaningful
"public class A { public OutstandingMigrationsProblem createOutstandingMigrationsProblem ( ) { return new OutstandingMigrationsProblem ( ) { private Set < TabletId > migrations = Collections . emptySet ( ) ; @ Override public void setMigrations ( Set < TabletId > migrations ) { this . migrations = migrations ; } @ Override public void report ( ) { log . warn ( ""Not balancing due to {} outstanding migrations."" , migrations . size ( ) ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( ""Successfully created migrations after {} outstanding migrations."" , migrations . size ( ) ) ; } migrations = Collections . emptySet ( ) ; } } ; } }","public class A { public OutstandingMigrationsProblem createOutstandingMigrationsProblem ( ) { return new OutstandingMigrationsProblem ( ) { private Set < TabletId > migrations = Collections . emptySet ( ) ; @ Override public void setMigrations ( Set < TabletId > migrations ) { this . migrations = migrations ; } @ Override public void report ( ) { log . warn ( ""Not balancing due to {} outstanding migrations."" , migrations . size ( ) ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( ""Sample up to 10 outstanding migrations: {}"" , migrations . stream ( ) . limit ( 10 ) . map ( String :: valueOf ) . collect ( Collectors . joining ( "", "" ) ) ) ; } migrations = Collections . emptySet ( ) ; } } ; } }","log . debug ( ""Sample up to 10 outstanding migrations: {}"" , migrations . stream ( ) . limit ( 10 ) . map ( String :: valueOf ) . collect ( Collectors . joining ( "", "" ) ) ) ;",Meaningful
"public class A { @ Override public void onServoSetSpeed ( ServoControl servo ) { int speed = - 1 ; if ( servo . getSpeed ( ) != null ) { speed = servo . getSpeed ( ) . intValue ( ) ; } log . info ( ""servoSetVelocity {} id {} velocity {}"" , servo . getName ( ) , getDeviceId ( servo ) , speed ) ; Integer i = getDeviceId ( servo ) ; if ( i == null ) { log . error ( ""servoSetSpeed is null"" ) ; return ; } msg . servoSetVelocity ( i , speed ) ; } }","public class A { @ Override public void onServoSetSpeed ( ServoControl servo ) { int speed = - 1 ; if ( servo . getSpeed ( ) != null ) { speed = servo . getSpeed ( ) . intValue ( ) ; } log . info ( ""servoSetVelocity {} id {} velocity {}"" , servo . getName ( ) , getDeviceId ( servo ) , speed ) ; Integer i = getDeviceId ( servo ) ; if ( i == null ) { log . error ( ""{} has null deviceId"" , servo ) ; return ; } msg . servoSetVelocity ( i , speed ) ; } }","log . error ( ""{} has null deviceId"" , servo ) ;",Meaningful
"public class A { @ Test public void testTraceWithNArguments ( ) { buf . setLength ( 0 ) ; final VitamUILogger logger = VitamUILoggerFactory . getInstance ( VitamUITraceLoggerTest . class ) ; final String message = ""message"" ; final String format = message + "" {} {} {}"" ; final Integer object1 = 1 ; final Integer object2 = 2 ; final Integer object3 = 3 ; logger . trace ( format , object1 , object2 ) ; assertTrue ( ""Log message should be written."" , buf . length ( ) > 0 ) ; assertTrue ( ""Log message should be written."" , buf . lastIndexOf ( message ) > 0 ) ; assertTrue ( ""Log message should be written."" , buf . lastIndexOf ( message + "" "" + object1 . toString ( ) + "" "" + object2 . toString ( ) + "" "" + object3 . toString ( ) ) > 0 ) ; } }","public class A { @ Test public void testTraceWithNArguments ( ) { buf . setLength ( 0 ) ; final VitamUILogger logger = VitamUILoggerFactory . getInstance ( VitamUITraceLoggerTest . class ) ; final String message = ""message"" ; final String format = message + "" {} {} {}"" ; final Integer object1 = 1 ; final Integer object2 = 2 ; final Integer object3 = 3 ; logger . trace ( format , object1 , object2 , object3 ) ; assertTrue ( ""Log message should be written."" , buf . length ( ) > 0 ) ; assertTrue ( ""Log message should be written."" , buf . lastIndexOf ( message ) > 0 ) ; assertTrue ( ""Log message should be written."" , buf . lastIndexOf ( message + "" "" + object1 . toString ( ) + "" "" + object2 . toString ( ) + "" "" + object3 . toString ( ) ) > 0 ) ; } }","logger . trace ( format , object1 , object2 , object3 ) ;",Meaningful
"public class A { @ Override public void onParentStatusEvent ( final MonitorStatusEvent statusEvent ) { Runnable monitoringRunnable = new Runnable ( ) { @ Override public void run ( ) { String instanceId = statusEvent . getInstanceId ( ) ; if ( statusEvent . getStatus ( ) == GroupStatus . Terminating || statusEvent . getStatus ( ) == ApplicationStatus . Terminating ) { if ( log . isInfoEnabled ( ) ) { log . info ( ""Sending cluster status update event for service "" + getServiceId ( ) + "" to "" + getClusterId ( ) + "", "" + statusEvent . getStatus ( ) + "", isTerminating"" ) ; } ClusterStatusEventPublisher . sendClusterStatusClusterTerminatingEvent ( getAppId ( ) , getServiceId ( ) , getClusterId ( ) , instanceId ) ; } } } ; executorService . execute ( monitoringRunnable ) ; } }","public class A { @ Override public void onParentStatusEvent ( final MonitorStatusEvent statusEvent ) { Runnable monitoringRunnable = new Runnable ( ) { @ Override public void run ( ) { String instanceId = statusEvent . getInstanceId ( ) ; if ( statusEvent . getStatus ( ) == GroupStatus . Terminating || statusEvent . getStatus ( ) == ApplicationStatus . Terminating ) { if ( log . isInfoEnabled ( ) ) { log . info ( ""Publishing Cluster terminating event for [application] "" + appId + "" [cluster] "" + getClusterId ( ) + "" [instance] "" + instanceId ) ; } ClusterStatusEventPublisher . sendClusterStatusClusterTerminatingEvent ( getAppId ( ) , getServiceId ( ) , getClusterId ( ) , instanceId ) ; } } } ; executorService . execute ( monitoringRunnable ) ; } }","log . info ( ""Publishing Cluster terminating event for [application] "" + appId + "" [cluster] "" + getClusterId ( ) + "" [instance] "" + instanceId ) ;",Meaningful
"public class A { public static void main ( String [ ] args ) throws Exception { String webappsPath = args [ 0 ] ; int port = Integer . parseInt ( args [ 1 ] ) ; File dataDir = Files . createTempDir ( ) ; dataDir . deleteOnExit ( ) ; Tomcat tomcat = new Tomcat ( ) ; tomcat . setBaseDir ( dataDir . getAbsolutePath ( ) ) ; tomcat . setPort ( port ) ; tomcat . getConnector ( ) . setAttribute ( ""maxThreads"" , ""1000"" ) ; tomcat . addWebapp ( ""/"" , new File ( webappsPath ) . getAbsolutePath ( ) ) ; log . info ( ""-----------------------------------------------------------------"" ) ; log . info ( ""Starting webapps-"" ) ; log . info ( ""-----------------------------------------------------------------"" ) ; tomcat . start ( ) ; while ( true ) { Thread . sleep ( 1000 ) ; } } }","public class A { public static void main ( String [ ] args ) throws Exception { String webappsPath = args [ 0 ] ; int port = Integer . parseInt ( args [ 1 ] ) ; File dataDir = Files . createTempDir ( ) ; dataDir . deleteOnExit ( ) ; Tomcat tomcat = new Tomcat ( ) ; tomcat . setBaseDir ( dataDir . getAbsolutePath ( ) ) ; tomcat . setPort ( port ) ; tomcat . getConnector ( ) . setAttribute ( ""maxThreads"" , ""1000"" ) ; tomcat . addWebapp ( ""/"" , new File ( webappsPath ) . getAbsolutePath ( ) ) ; log . info ( ""-----------------------------------------------------------------"" ) ; log . info ( ""Starting Tomcat port {} dir {}"" , port , webappsPath ) ; log . info ( ""-----------------------------------------------------------------"" ) ; tomcat . start ( ) ; while ( true ) { Thread . sleep ( 1000 ) ; } } }","log . info ( ""Starting Tomcat port {} dir {}"" , port , webappsPath ) ;",Meaningful
"public class A { @ Override public void saveKeyStore ( KeyStore keyStore ) throws Exception { File samlKeyStoreFile = new File ( getSamlKeyStorePath ( ) ) ; samlKeyStoreFile = samlKeyStoreFile . getAbsoluteFile ( ) ; if ( ! samlKeyStoreFile . exists ( ) ) { if ( _log . isInfoEnabled ( ) ) { _log . info ( ""Adding SamlKeyStoreFile to the keystore file"" ) ; } File parentDir = samlKeyStoreFile . getParentFile ( ) ; if ( ! parentDir . exists ( ) ) { parentDir . mkdirs ( ) ; } } monitorFile ( samlKeyStoreFile ) ; String samlKeyStorePassword = getSamlKeyStorePassword ( ) ; try ( FileOutputStream fileOutputStream = new FileOutputStream ( samlKeyStoreFile ) ) { _keyStore . store ( fileOutputStream , samlKeyStorePassword . toCharArray ( ) ) ; } } }","public class A { @ Override public void saveKeyStore ( KeyStore keyStore ) throws Exception { File samlKeyStoreFile = new File ( getSamlKeyStorePath ( ) ) ; samlKeyStoreFile = samlKeyStoreFile . getAbsoluteFile ( ) ; if ( ! samlKeyStoreFile . exists ( ) ) { if ( _log . isInfoEnabled ( ) ) { _log . info ( ""Creating a new SAML keystore at "" + samlKeyStoreFile ) ; } File parentDir = samlKeyStoreFile . getParentFile ( ) ; if ( ! parentDir . exists ( ) ) { parentDir . mkdirs ( ) ; } } monitorFile ( samlKeyStoreFile ) ; String samlKeyStorePassword = getSamlKeyStorePassword ( ) ; try ( FileOutputStream fileOutputStream = new FileOutputStream ( samlKeyStoreFile ) ) { _keyStore . store ( fileOutputStream , samlKeyStorePassword . toCharArray ( ) ) ; } } }","_log . info ( ""Creating a new SAML keystore at "" + samlKeyStoreFile ) ;",Meaningful
"public class A { public void disconnect ( ) { if ( client != null ) { try { client . close ( ) ; } catch ( IOException e ) { LOG . error ( ""Error closing client."" , e ) ; } } } }","public class A { public void disconnect ( ) { if ( client != null ) { try { client . close ( ) ; } catch ( IOException e ) { logger . error ( ""{}"" , e . getLocalizedMessage ( ) , e ) ; } } } }","logger . error ( ""{}"" , e . getLocalizedMessage ( ) , e ) ;",Meaningful
"public class A { @ Override public T localeResolver ( LocaleResolver localeResolver ) { if ( LOG . isDebugEnabled ( ) ) { if ( localeResolver != null ) { LOG . debug ( ""Setting locale resolver : {}"" , localeResolver ) ; } } this . localeResolver = localeResolver ; return thisAsT ( ) ; } }","public class A { @ Override public T localeResolver ( LocaleResolver localeResolver ) { if ( LOG . isDebugEnabled ( ) ) { if ( localeResolver != null ) { LOG . debug ( ""Setting custom LocaleResolver of type "" + localeResolver . getClass ( ) . getName ( ) ) ; } } this . localeResolver = localeResolver ; return thisAsT ( ) ; } }","LOG . debug ( ""Setting custom LocaleResolver of type "" + localeResolver . getClass ( ) . getName ( ) ) ;",Meaningful
"public class A { public void setHireDate ( NSTimestamp value ) { if ( _Employee . LOG . isDebugEnabled ( ) ) { _Employee . LOG . debug ( ""updating dates from "" + value + "" to "" + value ) ; } takeStoredValueForKey ( value , _Employee . HIRE_DATE_KEY ) ; } }","public class A { public void setHireDate ( NSTimestamp value ) { if ( _Employee . LOG . isDebugEnabled ( ) ) { _Employee . LOG . debug ( ""updating hireDate from "" + hireDate ( ) + "" to "" + value ) ; } takeStoredValueForKey ( value , _Employee . HIRE_DATE_KEY ) ; } }","_Employee . LOG . debug ( ""updating hireDate from "" + hireDate ( ) + "" to "" + value ) ;",Meaningful
"public class A { @ Override public boolean initMethod ( final PluginAnnotation < T > pluginAnnotation ) { LOGGER . debug ( ""Init {}"" , pluginAnnotation . getPlugin ( ) . getName ( ) ) ; ClassLoader classLoader = pluginManager . getPluginRegistry ( ) . getAppClassLoader ( pluginAnnotation . getPlugin ( ) ) ; try { registerResources ( pluginAnnotation , classLoader ) ; } catch ( IOException e ) { LOGGER . error ( ""Unable to register resources for annotation {} on method {} class {}"" , e , pluginAnnotation . getAnnotation ( ) , pluginAnnotation . getMethod ( ) . getName ( ) , pluginAnnotation . getMethod ( ) . getDeclaringClass ( ) . getName ( ) ) ; return false ; } return true ; } }","public class A { @ Override public boolean initMethod ( final PluginAnnotation < T > pluginAnnotation ) { LOGGER . debug ( ""Init for method "" + pluginAnnotation . getMethod ( ) ) ; ClassLoader classLoader = pluginManager . getPluginRegistry ( ) . getAppClassLoader ( pluginAnnotation . getPlugin ( ) ) ; try { registerResources ( pluginAnnotation , classLoader ) ; } catch ( IOException e ) { LOGGER . error ( ""Unable to register resources for annotation {} on method {} class {}"" , e , pluginAnnotation . getAnnotation ( ) , pluginAnnotation . getMethod ( ) . getName ( ) , pluginAnnotation . getMethod ( ) . getDeclaringClass ( ) . getName ( ) ) ; return false ; } return true ; } }","LOGGER . debug ( ""Init for method "" + pluginAnnotation . getMethod ( ) ) ;",Meaningful
"public class A { @ SuppressWarnings ( { ""rawtypes"" , ""unchecked"" } ) protected KStream < ? , ? > getKStream ( String inboundName , BindingProperties bindingProperties , KafkaStreamsConsumerProperties kafkaStreamsConsumerProperties , StreamsBuilder streamsBuilder , Serde < ? > keySerde , Serde < ? > valueSerde , Topology . AutoOffsetReset autoOffsetReset , boolean firstBuild ) { if ( firstBuild ) { addStateStoreBeans ( streamsBuilder ) ; } final boolean nativeDecoding = this . bindingServiceProperties . getConsumerProperties ( inboundName ) . isUseNativeDecoding ( ) ; if ( nativeDecoding ) { LOG . info ( ""Native decoding is enabled for "" + inboundName + "". Inbound deserialization done at the broker."" ) ; } else { LOG . info ( ""Native decoding is enabled for "" + inboundName + "". Inbound deserialization done at the broker."" ) ; } KStream < ? , ? > stream ; final Serde < ? > valueSerdeToUse = StringUtils . hasText ( kafkaStreamsConsumerProperties . getEventTypes ( ) ) ? new Serdes . BytesSerde ( ) : valueSerde ; final Consumed < ? , ? > consumed = getConsumed ( kafkaStreamsConsumerProperties , keySerde , valueSerdeToUse , autoOffsetReset ) ; if ( this . kafkaStreamsExtendedBindingProperties . getExtendedConsumerProperties ( inboundName ) . isDestinationIsPattern ( ) ) { final Pattern pattern = Pattern . compile ( this . bindingServiceProperties . getBindingDestination ( inboundName ) ) ; stream = streamsBuilder . stream ( pattern , consumed ) ; } else { String [ ] bindingTargets = StringUtils . commaDelimitedListToStringArray ( this . bindingServiceProperties . getBindingDestination ( inboundName ) ) ; stream = streamsBuilder . stream ( Arrays . asList ( bindingTargets ) , consumed ) ; } if ( StringUtils . hasText ( kafkaStreamsConsumerProperties . getEventTypes ( ) ) ) { AtomicBoolean matched = new AtomicBoolean ( ) ; stream . process ( ( ) -> eventTypeProcessor ( kafkaStreamsConsumerProperties , matched ) ) ; final KStream < ? , ? > [ ] branch = stream . branch ( ( key , value ) -> matched . getAndSet ( false ) ) ; final KStream < ? , Object > deserializedKStream = branch [ 0 ] . mapValues ( value -> valueSerde . deserializer ( ) . deserialize ( null , ( ( Bytes ) value ) . get ( ) ) ) ; return getkStream ( bindingProperties , deserializedKStream , nativeDecoding ) ; } return getkStream ( bindingProperties , stream , nativeDecoding ) ; } }","public class A { @ SuppressWarnings ( { ""rawtypes"" , ""unchecked"" } ) protected KStream < ? , ? > getKStream ( String inboundName , BindingProperties bindingProperties , KafkaStreamsConsumerProperties kafkaStreamsConsumerProperties , StreamsBuilder streamsBuilder , Serde < ? > keySerde , Serde < ? > valueSerde , Topology . AutoOffsetReset autoOffsetReset , boolean firstBuild ) { if ( firstBuild ) { addStateStoreBeans ( streamsBuilder ) ; } final boolean nativeDecoding = this . bindingServiceProperties . getConsumerProperties ( inboundName ) . isUseNativeDecoding ( ) ; if ( nativeDecoding ) { LOG . info ( ""Native decoding is enabled for "" + inboundName + "". Inbound deserialization done at the broker."" ) ; } else { LOG . info ( ""Native decoding is disabled for "" + inboundName + "". Inbound message conversion done by Spring Cloud Stream."" ) ; } KStream < ? , ? > stream ; final Serde < ? > valueSerdeToUse = StringUtils . hasText ( kafkaStreamsConsumerProperties . getEventTypes ( ) ) ? new Serdes . BytesSerde ( ) : valueSerde ; final Consumed < ? , ? > consumed = getConsumed ( kafkaStreamsConsumerProperties , keySerde , valueSerdeToUse , autoOffsetReset ) ; if ( this . kafkaStreamsExtendedBindingProperties . getExtendedConsumerProperties ( inboundName ) . isDestinationIsPattern ( ) ) { final Pattern pattern = Pattern . compile ( this . bindingServiceProperties . getBindingDestination ( inboundName ) ) ; stream = streamsBuilder . stream ( pattern , consumed ) ; } else { String [ ] bindingTargets = StringUtils . commaDelimitedListToStringArray ( this . bindingServiceProperties . getBindingDestination ( inboundName ) ) ; stream = streamsBuilder . stream ( Arrays . asList ( bindingTargets ) , consumed ) ; } if ( StringUtils . hasText ( kafkaStreamsConsumerProperties . getEventTypes ( ) ) ) { AtomicBoolean matched = new AtomicBoolean ( ) ; stream . process ( ( ) -> eventTypeProcessor ( kafkaStreamsConsumerProperties , matched ) ) ; final KStream < ? , ? > [ ] branch = stream . branch ( ( key , value ) -> matched . getAndSet ( false ) ) ; final KStream < ? , Object > deserializedKStream = branch [ 0 ] . mapValues ( value -> valueSerde . deserializer ( ) . deserialize ( null , ( ( Bytes ) value ) . get ( ) ) ) ; return getkStream ( bindingProperties , deserializedKStream , nativeDecoding ) ; } return getkStream ( bindingProperties , stream , nativeDecoding ) ; } }","LOG . info ( ""Native decoding is disabled for "" + inboundName + "". Inbound message conversion done by Spring Cloud Stream."" ) ;",Meaningful
"public class A { @ Override public final void run ( ) { this . running = true ; if ( ResourceManager . useCloud ( ) ) { try { Thread . sleep ( INITIAL_SLEEP ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; } } if ( ResourceManager . useCloud ( ) ) { RUNTIME_LOGGER . info ( ""[Resource Optimizer] Checking initial creations"" ) ; initialCreations ( ) ; } while ( this . running ) { try { doOperations ( ) ; periodicRemoveObsoletes ( ) ; try { synchronized ( this ) { if ( this . running ) { this . wait ( SLEEP_TIME ) ; } } } catch ( InterruptedException ie ) { } } catch ( Exception e ) { RUNTIME_LOGGER . error ( ""[Resource Optimizer] Error while executing live cleanup"" , e ) ; } } } }","public class A { @ Override public final void run ( ) { this . running = true ; if ( ResourceManager . useCloud ( ) ) { try { Thread . sleep ( INITIAL_SLEEP ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; } } if ( ResourceManager . useCloud ( ) ) { RUNTIME_LOGGER . info ( ""[Resource Optimizer] Checking initial creations"" ) ; initialCreations ( ) ; } while ( this . running ) { try { doOperations ( ) ; periodicRemoveObsoletes ( ) ; try { synchronized ( this ) { if ( this . running ) { this . wait ( SLEEP_TIME ) ; } } } catch ( InterruptedException ie ) { } } catch ( Exception e ) { RUNTIME_LOGGER . error ( ERROR_OPT_RES , e ) ; } } } }","RUNTIME_LOGGER . error ( ERROR_OPT_RES , e ) ;",Meaningful
"public class A { void unblockTo ( Address address ) { LOGGER . info ( ""Unblocking from {}"" , address ) ; LockPair lockPair = getLockPair ( address ) ; lockPair . unblockOutgoing ( ) ; } }","public class A { void unblockTo ( Address address ) { LOGGER . info ( ""Unblocked messages to "" + address ) ; LockPair lockPair = getLockPair ( address ) ; lockPair . unblockOutgoing ( ) ; } }","LOGGER . info ( ""Unblocked messages to "" + address ) ;",Meaningful
"public class A { @ Override public Node processExternalRequest ( Request request , Boolean isIpV6 ) { String headerKey = extractHeaderKey ( request ) ; Node node = null ; if ( balancerContext . regexMap != null && balancerContext . regexMap . size ( ) != 0 ) { if ( logger . isDebugEnabled ( ) ) logger . debug ( ""regexMap is not empty : "" + balancerContext . regexMap ) ; for ( Entry < String , KeySip > entry : balancerContext . regexMap . entrySet ( ) ) { Pattern r = Pattern . compile ( entry . getKey ( ) ) ; Matcher m = r . matcher ( headerKey ) ; if ( m . find ( ) ) { node = invocationContext . sipNodeMap ( isIpV6 ) . get ( entry . getValue ( ) ) ; if ( node != null ) { if ( logger . isDebugEnabled ( ) ) logger . debug ( ""Found node for pattern : "" + entry . getKey ( ) + "" and key :"" + entry . getValue ( ) ) ; return node ; } else { if ( logger . isDebugEnabled ( ) ) logger . debug ( ""Node not found in the map of nodes. It is null. For pattern: "" + entry . getKey ( ) + "" and key :"" + entry . getValue ( ) ) ; } } } } else { node = userToMap . get ( headerKey ) ; headerToTimestamps . put ( headerKey , System . currentTimeMillis ( ) ) ; } if ( node == null || invocationContext . sipNodeMap ( isIpV6 ) . get ( new KeySip ( node , isIpV6 ) ) . isGracefulShutdown ( ) ) { if ( lbConfig . getSipConfiguration ( ) . getTrafficRampupCyclePeriod ( ) != null && lbConfig . getSipConfiguration ( ) . getMaxWeightIndex ( ) != null ) node = getNextRampUpNode ( isIpV6 ) ; else node = nextAvailableNode ( isIpV6 ) ; if ( node == null ) return null ; userToMap . put ( headerKey , node ) ; if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Added new node to node set because adding it to node: "" + node ) ; } } else { if ( ! invocationContext . sipNodeMap ( isIpV6 ) . containsValue ( node ) ) { node = selectNewNode ( node , headerKey , isIpV6 ) ; } else { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""The assigned node in the affinity map is still alive: "" + node ) ; } } } return node ; } }","public class A { @ Override public Node processExternalRequest ( Request request , Boolean isIpV6 ) { String headerKey = extractHeaderKey ( request ) ; Node node = null ; if ( balancerContext . regexMap != null && balancerContext . regexMap . size ( ) != 0 ) { if ( logger . isDebugEnabled ( ) ) logger . debug ( ""regexMap is not empty : "" + balancerContext . regexMap ) ; for ( Entry < String , KeySip > entry : balancerContext . regexMap . entrySet ( ) ) { Pattern r = Pattern . compile ( entry . getKey ( ) ) ; Matcher m = r . matcher ( headerKey ) ; if ( m . find ( ) ) { node = invocationContext . sipNodeMap ( isIpV6 ) . get ( entry . getValue ( ) ) ; if ( node != null ) { if ( logger . isDebugEnabled ( ) ) logger . debug ( ""Found node for pattern : "" + entry . getKey ( ) + "" and key :"" + entry . getValue ( ) ) ; return node ; } else { if ( logger . isDebugEnabled ( ) ) logger . debug ( ""Node not found in the map of nodes. It is null. For pattern: "" + entry . getKey ( ) + "" and key :"" + entry . getValue ( ) ) ; } } } } else { node = userToMap . get ( headerKey ) ; headerToTimestamps . put ( headerKey , System . currentTimeMillis ( ) ) ; } if ( node == null || invocationContext . sipNodeMap ( isIpV6 ) . get ( new KeySip ( node , isIpV6 ) ) . isGracefulShutdown ( ) ) { if ( lbConfig . getSipConfiguration ( ) . getTrafficRampupCyclePeriod ( ) != null && lbConfig . getSipConfiguration ( ) . getMaxWeightIndex ( ) != null ) node = getNextRampUpNode ( isIpV6 ) ; else node = nextAvailableNode ( isIpV6 ) ; if ( node == null ) return null ; userToMap . put ( headerKey , node ) ; if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""No node found in the affinity map. It is null. We select new node: "" + node ) ; } } else { if ( ! invocationContext . sipNodeMap ( isIpV6 ) . containsValue ( node ) ) { node = selectNewNode ( node , headerKey , isIpV6 ) ; } else { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""The assigned node in the affinity map is still alive: "" + node ) ; } } } return node ; } }","logger . debug ( ""No node found in the affinity map. It is null. We select new node: "" + node ) ;",Meaningful
"public class A { @ Override public void handle ( CloseSessionEvent event , EventProcessor proc ) throws RemoteException { if ( event . cSID != null ) { log . debug ( ""Destroying container session : "" + event . sID . getLongId ( ) ) ; int count = event . session . destroySession ( event . cSID ) ; event . status . getOperatorMeasurement ( ) . changeActiveValue ( - count ) ; } else { log . debug ( ""Destroying session session : "" + event . session . getLongId ( ) ) ; int count = event . session . destroySession ( ) ; event . status . getOperatorMeasurement ( ) . changeActiveValue ( - count ) ; } } }","public class A { @ Override public void handle ( CloseSessionEvent event , EventProcessor proc ) throws RemoteException { if ( event . cSID != null ) { log . debug ( ""Destroying container session : "" + event . sID . getLongId ( ) ) ; int count = event . session . destroySession ( event . cSID ) ; event . status . getOperatorMeasurement ( ) . changeActiveValue ( - count ) ; } else { log . debug ( ""Destroying plan session : "" + event . sID . getLongId ( ) ) ; int count = event . session . destroySession ( ) ; event . status . getOperatorMeasurement ( ) . changeActiveValue ( - count ) ; } } }","log . debug ( ""Destroying plan session : "" + event . sID . getLongId ( ) ) ;",Meaningful
"public class A { @ SuppressWarnings ( ""unchecked"" ) private SipURI outboundInterface ( ServletContext context , String transport ) { SipURI result = null ; final List < SipURI > uris = ( List < SipURI > ) context . getAttribute ( OUTBOUND_INTERFACES ) ; if ( uris != null && uris . size ( ) > 0 ) { for ( final SipURI uri : uris ) { final String interfaceTransport = uri . getTransportParam ( ) ; if ( transport . equalsIgnoreCase ( interfaceTransport ) ) { result = uri ; } } if ( logger . isInfoEnabled ( ) ) { if ( result == null ) { logger . info ( ""ServletContext return null"" ) ; } else { logger . info ( ""Outbound interface found: "" + result . toString ( ) ) ; } } } else { if ( logger . isInfoEnabled ( ) ) { logger . info ( ""ServletContext return null or empty list of connectors"" ) ; } } return result ; } }","public class A { @ SuppressWarnings ( ""unchecked"" ) private SipURI outboundInterface ( ServletContext context , String transport ) { SipURI result = null ; final List < SipURI > uris = ( List < SipURI > ) context . getAttribute ( OUTBOUND_INTERFACES ) ; if ( uris != null && uris . size ( ) > 0 ) { for ( final SipURI uri : uris ) { final String interfaceTransport = uri . getTransportParam ( ) ; if ( transport . equalsIgnoreCase ( interfaceTransport ) ) { result = uri ; } } if ( logger . isInfoEnabled ( ) ) { if ( result == null ) { logger . info ( ""Outbound interface is NULL! Looks like there was no "" + transport + "" in the list of connectors"" ) ; } else { logger . info ( ""Outbound interface found: "" + result . toString ( ) ) ; } } } else { if ( logger . isInfoEnabled ( ) ) { logger . info ( ""ServletContext return null or empty list of connectors"" ) ; } } return result ; } }","logger . info ( ""Outbound interface is NULL! Looks like there was no "" + transport + "" in the list of connectors"" ) ;",Meaningful
"public class A { @ Override public void register ( Registration registration ) { if ( StringUtils . isEmpty ( registration . getServiceId ( ) ) ) { log . warn ( ""No service to register for nacos client..."" ) ; return ; } NamingService namingService = namingService ( ) ; String serviceId = registration . getServiceId ( ) ; String group = nacosDiscoveryProperties . getGroup ( ) ; Instance instance = getNacosInstanceFromRegistration ( registration ) ; try { namingService . registerInstance ( serviceId , group , instance ) ; log . info ( ""nacos registry, {} {} {}:{} register finished"" , group , serviceId , instance . getIp ( ) , instance . getPort ( ) ) ; } catch ( Exception e ) { log . error ( ""nacos registry exception, {} {} {}:{}"" , group , serviceId , instance . getIp ( ) , instance . getPort ( ) , e ) ; rethrowRuntimeException ( e ) ; } } }","public class A { @ Override public void register ( Registration registration ) { if ( StringUtils . isEmpty ( registration . getServiceId ( ) ) ) { log . warn ( ""No service to register for nacos client..."" ) ; return ; } NamingService namingService = namingService ( ) ; String serviceId = registration . getServiceId ( ) ; String group = nacosDiscoveryProperties . getGroup ( ) ; Instance instance = getNacosInstanceFromRegistration ( registration ) ; try { namingService . registerInstance ( serviceId , group , instance ) ; log . info ( ""nacos registry, {} {} {}:{} register finished"" , group , serviceId , instance . getIp ( ) , instance . getPort ( ) ) ; } catch ( Exception e ) { log . error ( ""nacos registry, {} register failed...{},"" , serviceId , registration . toString ( ) , e ) ; rethrowRuntimeException ( e ) ; } } }","log . error ( ""nacos registry, {} register failed...{},"" , serviceId , registration . toString ( ) , e ) ;",Meaningful
"public class A { public boolean cleanupNode ( final RegionAndId regionAndId ) { String instanceId = regionAndId . id ( ) ; InstanceStatus instanceStatus = Iterables . tryFind ( api . instanceApi ( ) . listInstanceStatus ( regionAndId . regionId ( ) ) . concat ( ) , new InstanceStatusPredicate ( instanceId ) ) . orNull ( ) ; if ( instanceStatus == null ) return true ; if ( InstanceStatus . Status . STOPPED != instanceStatus . status ( ) ) { logger . debug ( "">> power off %s ..."" , instanceId ) ; api . instanceApi ( ) . powerOff ( instanceId ) ; instanceSuspendedPredicate . apply ( RegionAndId . slashEncodeRegionAndId ( regionAndId ) ) ; } logger . debug ( "">> destroying %s ..."" , RegionAndId . slashEncodeRegionAndId ( regionAndId ) ) ; api . instanceApi ( ) . delete ( instanceId ) ; return instanceTerminatedPredicate . apply ( RegionAndId . slashEncodeRegionAndId ( regionAndId ) ) ; } }","public class A { public boolean cleanupNode ( final RegionAndId regionAndId ) { String instanceId = regionAndId . id ( ) ; InstanceStatus instanceStatus = Iterables . tryFind ( api . instanceApi ( ) . listInstanceStatus ( regionAndId . regionId ( ) ) . concat ( ) , new InstanceStatusPredicate ( instanceId ) ) . orNull ( ) ; if ( instanceStatus == null ) return true ; if ( InstanceStatus . Status . STOPPED != instanceStatus . status ( ) ) { logger . debug ( "">> powering off %s ..."" , RegionAndId . slashEncodeRegionAndId ( regionAndId ) ) ; api . instanceApi ( ) . powerOff ( instanceId ) ; instanceSuspendedPredicate . apply ( RegionAndId . slashEncodeRegionAndId ( regionAndId ) ) ; } logger . debug ( "">> destroying %s ..."" , RegionAndId . slashEncodeRegionAndId ( regionAndId ) ) ; api . instanceApi ( ) . delete ( instanceId ) ; return instanceTerminatedPredicate . apply ( RegionAndId . slashEncodeRegionAndId ( regionAndId ) ) ; } }","logger . debug ( "">> powering off %s ..."" , RegionAndId . slashEncodeRegionAndId ( regionAndId ) ) ;",Meaningful
"public class A { @ Override public boolean validate ( JSONObject message ) { LOG . debug ( ""Grok parser validating message: {}"" , message ) ; Object timestampObject = message . get ( Constants . Fields . TIMESTAMP . getName ( ) ) ; if ( timestampObject instanceof Long ) { Long timestamp = ( Long ) timestampObject ; if ( timestamp > 0 ) { LOG . debug ( ""Grok parser validated message: {}"" , message ) ; return true ; } } LOG . debug ( ""Invalid timestamp field found: {}"" , timestamp ) ; return false ; } }","public class A { @ Override public boolean validate ( JSONObject message ) { LOG . debug ( ""Grok parser validating message: {}"" , message ) ; Object timestampObject = message . get ( Constants . Fields . TIMESTAMP . getName ( ) ) ; if ( timestampObject instanceof Long ) { Long timestamp = ( Long ) timestampObject ; if ( timestamp > 0 ) { LOG . debug ( ""Grok parser validated message: {}"" , message ) ; return true ; } } LOG . debug ( ""Grok parser did not validate message: {}"" , message ) ; return false ; } }","LOG . debug ( ""Grok parser did not validate message: {}"" , message ) ;",Meaningful
"public class A { @ Override public void handleCommand ( ChannelUID channelUID , Command command ) { Group group = this . group ; if ( group == null ) { return ; } if ( command instanceof RefreshType ) { logger . debug ( ""Refresh type is a read-only channel"" ) ; } else { if ( channelUID . getId ( ) . equals ( CBusBindingConstants . CHANNEL_VALUE ) ) { logger . debug ( ""Channel Value command for {}: {}"" , channelUID , command ) ; try { if ( command instanceof DecimalType ) { group . TriggerEvent ( ( ( DecimalType ) command ) . intValue ( ) ) ; } } catch ( CGateException e ) { logger . debug ( ""Failed to send trigger command {} to {}"" , command , group , e ) ; updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . COMMUNICATION_ERROR , ""Communication Error"" ) ; } } } } }","public class A { @ Override public void handleCommand ( ChannelUID channelUID , Command command ) { Group group = this . group ; if ( group == null ) { return ; } if ( command instanceof RefreshType ) { logger . debug ( ""Refresh for Trigger group not implemented"" ) ; } else { if ( channelUID . getId ( ) . equals ( CBusBindingConstants . CHANNEL_VALUE ) ) { logger . debug ( ""Channel Value command for {}: {}"" , channelUID , command ) ; try { if ( command instanceof DecimalType ) { group . TriggerEvent ( ( ( DecimalType ) command ) . intValue ( ) ) ; } } catch ( CGateException e ) { logger . debug ( ""Failed to send trigger command {} to {}"" , command , group , e ) ; updateStatus ( ThingStatus . OFFLINE , ThingStatusDetail . COMMUNICATION_ERROR , ""Communication Error"" ) ; } } } } }","logger . debug ( ""Refresh for Trigger group not implemented"" ) ;",Meaningful
"public class A { static void verifyCertificateSigningRequest ( CertificateSigningRequestDTO request , String requesterCN , CAProperties caProperties ) { if ( request == null ) { logger . error ( ""CertificateSigningRequest cannot be null"" ) ; throw new InvalidParameterException ( ""CertificateSigningRequest cannot be null"" ) ; } if ( Utilities . isEmpty ( request . getEncodedCSR ( ) ) ) { logger . error ( ""CertificateSigningRequest cannot be empty"" ) ; throw new InvalidParameterException ( ""CertificateSigningRequest cannot be empty"" ) ; } if ( Utilities . isEmpty ( requesterCN ) ) { logger . error ( ""CertificateSigningRequest requester common name cannot be empty"" ) ; throw new InvalidParameterException ( ""CertificateSigningRequest requester common name cannot be empty"" ) ; } if ( caProperties == null ) { logger . error ( ""CertificateSigningRequest requesterCaProperties cannot be empty"" ) ; throw new InvalidParameterException ( ""CaProperties cannot be empty"" ) ; } final ZonedDateTime now = ZonedDateTime . now ( ) ; final ZonedDateTime validBefore = Utilities . parseUTCStringToLocalZonedDateTime ( request . getValidBefore ( ) ) ; if ( validBefore != null ) { final ZonedDateTime validBeforeLimit = getValidBeforeLimit ( now , caProperties ) ; if ( validBefore . compareTo ( validBeforeLimit ) > 0 ) { final String msg = ""Validity range parameter exceeds maximum validity range: validBefore (limit: "" + validBeforeLimit + "", got: "" + validBefore + "")"" ; logger . error ( msg ) ; throw new InvalidParameterException ( msg ) ; } } final ZonedDateTime validAfter = Utilities . parseUTCStringToLocalZonedDateTime ( request . getValidAfter ( ) ) ; if ( validAfter != null ) { final ZonedDateTime validAfterLimit = getValidAfterLimit ( now , caProperties ) ; if ( validAfter . compareTo ( validAfterLimit ) < 0 ) { final String msg = ""Validity range parameter exceeds maximum validity range: validAfter (limit: "" + validAfterLimit + "", got: "" + validAfter + "")"" ; logger . error ( msg ) ; throw new InvalidParameterException ( msg ) ; } } } }","public class A { static void verifyCertificateSigningRequest ( CertificateSigningRequestDTO request , String requesterCN , CAProperties caProperties ) { if ( request == null ) { logger . error ( ""CertificateSigningRequest cannot be null"" ) ; throw new InvalidParameterException ( ""CertificateSigningRequest cannot be null"" ) ; } if ( Utilities . isEmpty ( request . getEncodedCSR ( ) ) ) { logger . error ( ""CertificateSigningRequest cannot be empty"" ) ; throw new InvalidParameterException ( ""CertificateSigningRequest cannot be empty"" ) ; } if ( Utilities . isEmpty ( requesterCN ) ) { logger . error ( ""CertificateSigningRequest requester common name cannot be empty"" ) ; throw new InvalidParameterException ( ""CertificateSigningRequest requester common name cannot be empty"" ) ; } if ( caProperties == null ) { logger . error ( ""CaProperties cannot be empty"" ) ; throw new InvalidParameterException ( ""CaProperties cannot be empty"" ) ; } final ZonedDateTime now = ZonedDateTime . now ( ) ; final ZonedDateTime validBefore = Utilities . parseUTCStringToLocalZonedDateTime ( request . getValidBefore ( ) ) ; if ( validBefore != null ) { final ZonedDateTime validBeforeLimit = getValidBeforeLimit ( now , caProperties ) ; if ( validBefore . compareTo ( validBeforeLimit ) > 0 ) { final String msg = ""Validity range parameter exceeds maximum validity range: validBefore (limit: "" + validBeforeLimit + "", got: "" + validBefore + "")"" ; logger . error ( msg ) ; throw new InvalidParameterException ( msg ) ; } } final ZonedDateTime validAfter = Utilities . parseUTCStringToLocalZonedDateTime ( request . getValidAfter ( ) ) ; if ( validAfter != null ) { final ZonedDateTime validAfterLimit = getValidAfterLimit ( now , caProperties ) ; if ( validAfter . compareTo ( validAfterLimit ) < 0 ) { final String msg = ""Validity range parameter exceeds maximum validity range: validAfter (limit: "" + validAfterLimit + "", got: "" + validAfter + "")"" ; logger . error ( msg ) ; throw new InvalidParameterException ( msg ) ; } } } }","logger . error ( ""CaProperties cannot be empty"" ) ;",Meaningful
"public class A { private void logDebug ( OAuth2TokenResponse response ) { if ( ! LOGGER . isDebugEnabled ( ) ) { return ; } try { DecodedJwt decodedJwt = response . getDecodedAccessToken ( ) ; LOGGER . debug ( ""DecodedJwt: {}"" , decodedJwt ) ; } catch ( IllegalArgumentException e ) { LOGGER . debug ( ""Access token can not be logged. {}"" , e . getMessage ( ) ) ; } } }","public class A { private void logDebug ( OAuth2TokenResponse response ) { if ( ! LOGGER . isDebugEnabled ( ) ) { return ; } try { DecodedJwt decodedJwt = response . getDecodedAccessToken ( ) ; LOGGER . debug ( ""Access token: {}"" , decodedJwt ) ; } catch ( IllegalArgumentException e ) { LOGGER . debug ( ""Access token can not be logged. {}"" , e . getMessage ( ) ) ; } } }","LOGGER . debug ( ""Access token: {}"" , decodedJwt ) ;",Meaningful
"public class A { void setRedundancyZone ( InternalDistributedMember member , String redundancyZone ) { if ( redundancyZone != null && ! redundancyZone . equals ( """" ) ) { redundancyZones . put ( member , redundancyZone ) ; } if ( member != getDistributionManagerId ( ) ) { String relationship = areInSameZone ( getDistributionManagerId ( ) , member ) ? """" : ""not "" ; log . info ( ""Setting Redundancy zone to {} by user {}"" , member , redundancyZone ) ; } } }","public class A { void setRedundancyZone ( InternalDistributedMember member , String redundancyZone ) { if ( redundancyZone != null && ! redundancyZone . equals ( """" ) ) { redundancyZones . put ( member , redundancyZone ) ; } if ( member != getDistributionManagerId ( ) ) { String relationship = areInSameZone ( getDistributionManagerId ( ) , member ) ? """" : ""not "" ; logger . info ( ""Member {} is {}equivalent or in the same redundancy zone."" , member , relationship ) ; } } }","logger . info ( ""Member {} is {}equivalent or in the same redundancy zone."" , member , relationship ) ;",Meaningful
"public class A { private void createStreamingServer ( WPSCapabilitiesType capabilities , ComponentsFactory factory ) { ProcessOfferings offerings = capabilities . getProcessOfferings ( ) ; if ( offerings != null ) { Set < String > processNames = New . set ( ) ; for ( ProcessBriefType briefType : offerings . getProcess ( ) ) { processNames . add ( briefType . getIdentifier ( ) . getValue ( ) ) ; } boolean serverHasStreaming = processNames . contains ( StreamingConstants . SUBSCRIBE_PROCESS ) && processNames . contains ( StreamingConstants . UNSUBSCRIBE_PROCESS ) ; if ( serverHasStreaming ) { myServer = new WPSStreamingServer ( factory ) ; ServerProvider < StreamingServer > provider = myProviderRegistry . getProvider ( StreamingServer . class ) ; if ( provider instanceof ModifiableServerProvider ) { ( ( ModifiableServerProvider < StreamingServer > ) provider ) . addServer ( myServer ) ; } else { LOGGER . warn ( ""Cannot add StreamingServer to {}"" , myServer . getIdentifier ( ) . getValue ( ) ) ; } } } } }","public class A { private void createStreamingServer ( WPSCapabilitiesType capabilities , ComponentsFactory factory ) { ProcessOfferings offerings = capabilities . getProcessOfferings ( ) ; if ( offerings != null ) { Set < String > processNames = New . set ( ) ; for ( ProcessBriefType briefType : offerings . getProcess ( ) ) { processNames . add ( briefType . getIdentifier ( ) . getValue ( ) ) ; } boolean serverHasStreaming = processNames . contains ( StreamingConstants . SUBSCRIBE_PROCESS ) && processNames . contains ( StreamingConstants . UNSUBSCRIBE_PROCESS ) ; if ( serverHasStreaming ) { myServer = new WPSStreamingServer ( factory ) ; ServerProvider < StreamingServer > provider = myProviderRegistry . getProvider ( StreamingServer . class ) ; if ( provider instanceof ModifiableServerProvider ) { ( ( ModifiableServerProvider < StreamingServer > ) provider ) . addServer ( myServer ) ; } else { LOGGER . warn ( ""Could not find a streaming server provider."" ) ; } } } } }","LOGGER . warn ( ""Could not find a streaming server provider."" ) ;",Meaningful
"public class A { protected void writeCommand ( byte [ ] message ) throws SonyProjectorException { logger . debug ( ""writeCommand called with simu {}"" , simu ) ; if ( simu ) { return ; } OutputStream dataOut = this . dataOut ; if ( dataOut == null ) { throw new SonyProjectorException ( ""writeCommand failed: output stream is null"" ) ; } try { dataOut . write ( message ) ; dataOut . flush ( ) ; } catch ( IOException e ) { logger . debug ( ""writeCommand failed: {}"" , e . getMessage ( ) ) ; throw new SonyProjectorException ( ""writeCommand failed: "" + e . getMessage ( ) ) ; } } }","public class A { protected void writeCommand ( byte [ ] message ) throws SonyProjectorException { logger . debug ( ""writeCommand: {}"" , HexUtils . bytesToHex ( message ) ) ; if ( simu ) { return ; } OutputStream dataOut = this . dataOut ; if ( dataOut == null ) { throw new SonyProjectorException ( ""writeCommand failed: output stream is null"" ) ; } try { dataOut . write ( message ) ; dataOut . flush ( ) ; } catch ( IOException e ) { logger . debug ( ""writeCommand failed: {}"" , e . getMessage ( ) ) ; throw new SonyProjectorException ( ""writeCommand failed: "" + e . getMessage ( ) ) ; } } }","logger . debug ( ""writeCommand: {}"" , HexUtils . bytesToHex ( message ) ) ;",Meaningful
"public class A { public void configure ( final Configuration config ) throws ConfigurationException { try { setServerConfig ( config ) ; setAttachmentDir ( config ) ; setJWSDir ( config ) ; setSecurityProvider ( config ) ; setTransportName ( config ) ; setManagedServices ( config ) ; if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( ""Using configuration: "" + config . getConfigurationFile ( ) ) ; } } catch ( final Exception e ) { throw new ConfigurationException ( ""Error during configuration"" , e ) ; } } }","public class A { public void configure ( final Configuration config ) throws ConfigurationException { try { setServerConfig ( config ) ; setAttachmentDir ( config ) ; setJWSDir ( config ) ; setSecurityProvider ( config ) ; setTransportName ( config ) ; setManagedServices ( config ) ; if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( ""SoapServerImpl.configure() complete"" ) ; } } catch ( final Exception e ) { throw new ConfigurationException ( ""Error during configuration"" , e ) ; } } }","getLogger ( ) . debug ( ""SoapServerImpl.configure() complete"" ) ;",Meaningful
"public class A { @ Override public boolean validate ( JSONObject message ) { LOG . debug ( ""Grok parser validating message: {}"" , message ) ; Object timestampObject = message . get ( Constants . Fields . TIMESTAMP . getName ( ) ) ; if ( timestampObject instanceof Long ) { Long timestamp = ( Long ) timestampObject ; if ( timestamp > 0 ) { LOG . debug ( ""Received timestamp to be {}"" , timestamp ) ; return true ; } } LOG . debug ( ""Grok parser did not validate message: {}"" , message ) ; return false ; } }","public class A { @ Override public boolean validate ( JSONObject message ) { LOG . debug ( ""Grok parser validating message: {}"" , message ) ; Object timestampObject = message . get ( Constants . Fields . TIMESTAMP . getName ( ) ) ; if ( timestampObject instanceof Long ) { Long timestamp = ( Long ) timestampObject ; if ( timestamp > 0 ) { LOG . debug ( ""Grok parser validated message: {}"" , message ) ; return true ; } } LOG . debug ( ""Grok parser did not validate message: {}"" , message ) ; return false ; } }","LOG . debug ( ""Grok parser validated message: {}"" , message ) ;",Meaningful
"public class A { public void service ( HttpServletRequest req , HttpServletResponse res ) throws Exception { boolean debug = log . isDebugEnabled ( ) ; try { req . setCharacterEncoding ( ""UTF-8"" ) ; } catch ( UnsupportedEncodingException e ) { log . error ( ""Encoding not supported"" , e ) ; } String portalPath = req . getRequestURI ( ) . substring ( req . getContextPath ( ) . length ( ) ) ; Router router = routerRef . get ( ) ; if ( router != null ) { Iterator < Map < QualifiedName , String > > matcher = router . matcher ( portalPath , req . getParameterMap ( ) ) ; boolean started = false ; boolean processed = false ; try { while ( matcher . hasNext ( ) && ! processed ) { Map < QualifiedName , String > parameters = matcher . next ( ) ; String handlerKey = parameters . get ( HANDLER_PARAM ) ; if ( handlerKey != null ) { WebRequestHandler handler = handlers . get ( handlerKey ) ; if ( handler != null ) { if ( debug ) { log . debug ( ""Serving request path="" + portalPath + "", parameters="" + parameters + "" with handler "" + handler ) ; } if ( ! started && handler . getRequiresLifeCycle ( ) ) { if ( debug ) { log . debug ( ""Serving request path="" + portalPath + "", in max. Waiting for "" + handler . getRequiresLifeCycle ( ) + "" elements"" ) ; } RequestLifeCycle . begin ( ExoContainerContext . getCurrentContainer ( ) ) ; started = true ; } processed = handler . execute ( new ControllerContext ( this , router , req , res , parameters ) ) ; } else { if ( debug ) { log . debug ( ""No handler "" + handlerKey + "" for request path="" + portalPath + "", parameters="" + parameters ) ; } } } } } finally { if ( started ) { if ( debug ) { log . debug ( ""Finishing RequestLifeCycle for current request"" ) ; } RequestLifeCycle . end ( ) ; } } if ( ! processed ) { log . error ( ""Could not associate the request path="" + portalPath + "" with an handler"" ) ; res . sendError ( HttpServletResponse . SC_NOT_FOUND ) ; } } else { log . error ( ""Missing valid router configuration "" + configurationPathRef . get ( ) ) ; res . sendError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR ) ; } } }","public class A { public void service ( HttpServletRequest req , HttpServletResponse res ) throws Exception { boolean debug = log . isDebugEnabled ( ) ; try { req . setCharacterEncoding ( ""UTF-8"" ) ; } catch ( UnsupportedEncodingException e ) { log . error ( ""Encoding not supported"" , e ) ; } String portalPath = req . getRequestURI ( ) . substring ( req . getContextPath ( ) . length ( ) ) ; Router router = routerRef . get ( ) ; if ( router != null ) { Iterator < Map < QualifiedName , String > > matcher = router . matcher ( portalPath , req . getParameterMap ( ) ) ; boolean started = false ; boolean processed = false ; try { while ( matcher . hasNext ( ) && ! processed ) { Map < QualifiedName , String > parameters = matcher . next ( ) ; String handlerKey = parameters . get ( HANDLER_PARAM ) ; if ( handlerKey != null ) { WebRequestHandler handler = handlers . get ( handlerKey ) ; if ( handler != null ) { if ( debug ) { log . debug ( ""Serving request path="" + portalPath + "", parameters="" + parameters + "" with handler "" + handler ) ; } if ( ! started && handler . getRequiresLifeCycle ( ) ) { if ( debug ) { log . debug ( ""Starting RequestLifeCycle for handler "" + handler ) ; } RequestLifeCycle . begin ( ExoContainerContext . getCurrentContainer ( ) ) ; started = true ; } processed = handler . execute ( new ControllerContext ( this , router , req , res , parameters ) ) ; } else { if ( debug ) { log . debug ( ""No handler "" + handlerKey + "" for request path="" + portalPath + "", parameters="" + parameters ) ; } } } } } finally { if ( started ) { if ( debug ) { log . debug ( ""Finishing RequestLifeCycle for current request"" ) ; } RequestLifeCycle . end ( ) ; } } if ( ! processed ) { log . error ( ""Could not associate the request path="" + portalPath + "" with an handler"" ) ; res . sendError ( HttpServletResponse . SC_NOT_FOUND ) ; } } else { log . error ( ""Missing valid router configuration "" + configurationPathRef . get ( ) ) ; res . sendError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR ) ; } } }","log . debug ( ""Starting RequestLifeCycle for handler "" + handler ) ;",Meaningful
"public class A { @ SuppressWarnings ( ""null"" ) @ RuleAction ( label = ""send a raw command"" , description = ""Send a raw command to the receiver."" ) public void sendRawCommand ( @ ActionInput ( name = ""command"" ) @ Nullable String command , @ ActionInput ( name = ""command"" ) @ Nullable String value ) { logger . debug ( ""sendRawCommand called with raw command: {} value: {}"" , command , value ) ; if ( handler == null ) { logger . warn ( ""No handler available to handle command."" ) ; return ; } handler . sendRawCommand ( command , value ) ; } }","public class A { @ SuppressWarnings ( ""null"" ) @ RuleAction ( label = ""send a raw command"" , description = ""Send a raw command to the receiver."" ) public void sendRawCommand ( @ ActionInput ( name = ""command"" ) @ Nullable String command , @ ActionInput ( name = ""command"" ) @ Nullable String value ) { logger . debug ( ""sendRawCommand called with raw command: {} value: {}"" , command , value ) ; if ( handler == null ) { logger . warn ( ""Onkyo Action service ThingHandler is null!"" ) ; return ; } handler . sendRawCommand ( command , value ) ; } }","logger . warn ( ""Onkyo Action service ThingHandler is null!"" ) ;",Meaningful
"public class A { private void validateInMemoryResultSet ( InMemoryQueryResult result ) { int size = result . getRows ( ) . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { logger . info ( ""Input result: {}"" , i ) ; } Assert . assertEquals ( size , 2 , ""Wrong result"" ) ; Assert . assertEquals ( result . getRows ( ) . get ( 0 ) . getValues ( ) . get ( 0 ) , 2 , ""Wrong result"" ) ; Assert . assertEquals ( result . getRows ( ) . get ( 0 ) . getValues ( ) . get ( 1 ) , ""second"" , ""Wrong result"" ) ; Assert . assertEquals ( result . getRows ( ) . get ( 1 ) . getValues ( ) . get ( 0 ) , 3 , ""Wrong result"" ) ; Assert . assertEquals ( result . getRows ( ) . get ( 1 ) . getValues ( ) . get ( 1 ) , ""third"" , ""Wrong result"" ) ; } }","public class A { private void validateInMemoryResultSet ( InMemoryQueryResult result ) { int size = result . getRows ( ) . size ( ) ; for ( int i = 0 ; i < size ; i ++ ) { logger . info ( result . getRows ( ) . get ( i ) . getValues ( ) . get ( 0 ) + "" "" + result . getRows ( ) . get ( i ) . getValues ( ) . get ( 1 ) ) ; } Assert . assertEquals ( size , 2 , ""Wrong result"" ) ; Assert . assertEquals ( result . getRows ( ) . get ( 0 ) . getValues ( ) . get ( 0 ) , 2 , ""Wrong result"" ) ; Assert . assertEquals ( result . getRows ( ) . get ( 0 ) . getValues ( ) . get ( 1 ) , ""second"" , ""Wrong result"" ) ; Assert . assertEquals ( result . getRows ( ) . get ( 1 ) . getValues ( ) . get ( 0 ) , 3 , ""Wrong result"" ) ; Assert . assertEquals ( result . getRows ( ) . get ( 1 ) . getValues ( ) . get ( 1 ) , ""third"" , ""Wrong result"" ) ; } }","logger . info ( result . getRows ( ) . get ( i ) . getValues ( ) . get ( 0 ) + "" "" + result . getRows ( ) . get ( i ) . getValues ( ) . get ( 1 ) ) ;",Meaningful
"public class A { @ Test public void parseBsonArrayWithValues ( ) throws IOException { BsonValue a = new BsonString ( ""stest"" ) ; BsonValue b = new BsonDouble ( 111 ) ; BsonValue c = new BsonBoolean ( true ) ; BsonDocument document = new BsonDocument ( ) . append ( ""int32"" , new BsonInt32 ( 12 ) ) . append ( ""int64"" , new BsonInt64 ( 77L ) ) . append ( ""bo\""olean"" , new BsonBoolean ( true ) ) . append ( ""date"" , new BsonDateTime ( new Date ( ) . getTime ( ) ) ) . append ( ""double"" , new BsonDouble ( 12.3 ) ) . append ( ""string"" , new BsonString ( ""pinpoint"" ) ) . append ( ""objectId"" , new BsonObjectId ( new ObjectId ( ) ) ) . append ( ""code"" , new BsonJavaScript ( ""int i = 10;"" ) ) . append ( ""codeWithScope"" , new BsonJavaScriptWithScope ( ""int x = y"" , new BsonDocument ( ""y"" , new BsonInt32 ( 1 ) ) ) ) . append ( ""regex"" , new BsonRegularExpression ( ""^test.*regex.*xyz$"" , ""big"" ) ) . append ( ""symbol"" , new BsonSymbol ( ""wow"" ) ) . append ( ""timestamp"" , new BsonTimestamp ( 0x12345678 , 5 ) ) . append ( ""undefined"" , new BsonUndefined ( ) ) . append ( ""binary1"" , new BsonBinary ( new byte [ ] { ( byte ) 0xe0 , 0x4f , ( byte ) 0xd0 , 0x20 } ) ) . append ( ""oldBinary"" , new BsonBinary ( BsonBinarySubType . OLD_BINARY , new byte [ ] { 1 , 1 , 1 , 1 , 1 } ) ) . append ( ""arrayInt"" , new BsonArray ( Arrays . asList ( a , b , c , new BsonInt32 ( 7 ) ) ) ) . append ( ""document"" , new BsonDocument ( ""a"" , new BsonInt32 ( 77 ) ) ) . append ( ""dbPointer"" , new BsonDbPointer ( ""db.coll"" , new ObjectId ( ) ) ) . append ( ""null"" , new BsonNull ( ) ) . append ( ""decimal128"" , new BsonDecimal128 ( new Decimal128 ( 55 ) ) ) ; BasicDBObject query = new BasicDBObject ( ) ; query . put ( ""ComplexBson"" , document ) ; logger . debug ( ""query:{}"" , query ) ; NormalizedBson stringStringValue = MongoUtil . parseBson ( new Object [ ] { query } , true ) ; logger . debug ( ""val:{}"" , stringStringValue ) ; List list = objectMapper . readValue ( ""["" + stringStringValue . getNormalizedBson ( ) + ""]"" , List . class ) ; Assert . assertEquals ( list . size ( ) , 1 ) ; Map < String , ? > query1Map = ( Map < String , ? > ) list . get ( 0 ) ; checkValue ( query1Map ) ; } }","public class A { @ Test public void parseBsonArrayWithValues ( ) throws IOException { BsonValue a = new BsonString ( ""stest"" ) ; BsonValue b = new BsonDouble ( 111 ) ; BsonValue c = new BsonBoolean ( true ) ; BsonDocument document = new BsonDocument ( ) . append ( ""int32"" , new BsonInt32 ( 12 ) ) . append ( ""int64"" , new BsonInt64 ( 77L ) ) . append ( ""bo\""olean"" , new BsonBoolean ( true ) ) . append ( ""date"" , new BsonDateTime ( new Date ( ) . getTime ( ) ) ) . append ( ""double"" , new BsonDouble ( 12.3 ) ) . append ( ""string"" , new BsonString ( ""pinpoint"" ) ) . append ( ""objectId"" , new BsonObjectId ( new ObjectId ( ) ) ) . append ( ""code"" , new BsonJavaScript ( ""int i = 10;"" ) ) . append ( ""codeWithScope"" , new BsonJavaScriptWithScope ( ""int x = y"" , new BsonDocument ( ""y"" , new BsonInt32 ( 1 ) ) ) ) . append ( ""regex"" , new BsonRegularExpression ( ""^test.*regex.*xyz$"" , ""big"" ) ) . append ( ""symbol"" , new BsonSymbol ( ""wow"" ) ) . append ( ""timestamp"" , new BsonTimestamp ( 0x12345678 , 5 ) ) . append ( ""undefined"" , new BsonUndefined ( ) ) . append ( ""binary1"" , new BsonBinary ( new byte [ ] { ( byte ) 0xe0 , 0x4f , ( byte ) 0xd0 , 0x20 } ) ) . append ( ""oldBinary"" , new BsonBinary ( BsonBinarySubType . OLD_BINARY , new byte [ ] { 1 , 1 , 1 , 1 , 1 } ) ) . append ( ""arrayInt"" , new BsonArray ( Arrays . asList ( a , b , c , new BsonInt32 ( 7 ) ) ) ) . append ( ""document"" , new BsonDocument ( ""a"" , new BsonInt32 ( 77 ) ) ) . append ( ""dbPointer"" , new BsonDbPointer ( ""db.coll"" , new ObjectId ( ) ) ) . append ( ""null"" , new BsonNull ( ) ) . append ( ""decimal128"" , new BsonDecimal128 ( new Decimal128 ( 55 ) ) ) ; BasicDBObject query = new BasicDBObject ( ) ; query . put ( ""ComplexBson"" , document ) ; logger . debug ( ""document:{}"" , document ) ; NormalizedBson stringStringValue = MongoUtil . parseBson ( new Object [ ] { query } , true ) ; logger . debug ( ""val:{}"" , stringStringValue ) ; List list = objectMapper . readValue ( ""["" + stringStringValue . getNormalizedBson ( ) + ""]"" , List . class ) ; Assert . assertEquals ( list . size ( ) , 1 ) ; Map < String , ? > query1Map = ( Map < String , ? > ) list . get ( 0 ) ; checkValue ( query1Map ) ; } }","logger . debug ( ""document:{}"" , document ) ;",Meaningful
"public class A { @ Override public void onRegistrationSuccess ( ServerIdentity server , RegisterRequest request , String registrationID ) { LOGGER . info ( ""registered instance success, server:{}, registrationId:{}, endpoint:{}"" , server , registrationID , server , server . getEndpointId ( ) ) ; } }","public class A { @ Override public void onRegistrationSuccess ( ServerIdentity server , RegisterRequest request , String registrationID ) { log . info ( ""ClientObserver -> onRegistrationSuccess... EndpointName [{}] [{}]"" , request . getEndpointName ( ) , registrationID ) ; } }","log . info ( ""ClientObserver -> onRegistrationSuccess... EndpointName [{}] [{}]"" , request . getEndpointName ( ) , registrationID ) ;",Meaningful
"public class A { @ Override public List < AggregateResult > calcResult ( long curStartTime , long curEndTime ) throws IOException { List < ByteBuffer > aggrBuffers ; try { if ( ClusterDescriptor . getInstance ( ) . getConfig ( ) . isUseAsyncServer ( ) ) { AsyncDataClient client = metaGroupMember . getClientProvider ( ) . getAsyncDataClient ( source , RaftServer . getReadOperationTimeoutMS ( ) ) ; aggrBuffers = SyncClientAdaptor . getGroupByResult ( client , header , executorId , curStartTime , curEndTime ) ; } else { try ( SyncDataClient syncDataClient = metaGroupMember . getClientProvider ( ) . getSyncDataClient ( source , RaftServer . getReadOperationTimeoutMS ( ) ) ) { aggrBuffers = syncDataClient . getGroupByResult ( header , executorId , curStartTime , curEndTime ) ; } } } catch ( TException e ) { throw new IOException ( e ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; throw new IOException ( e ) ; } resetAggregateResults ( ) ; if ( aggrBuffers != null ) { for ( int i = 0 ; i < aggrBuffers . size ( ) ; i ++ ) { AggregateResult result = AggregateResult . deserializeFrom ( aggrBuffers . get ( i ) ) ; results . get ( i ) . merge ( result ) ; } } logger . debug ( aggrBuffers . size ( ) + "" fetching results of aggr buffers in cluster: "" + aggrBuffers . size ( ) ) ; return results ; } }","public class A { @ Override public List < AggregateResult > calcResult ( long curStartTime , long curEndTime ) throws IOException { List < ByteBuffer > aggrBuffers ; try { if ( ClusterDescriptor . getInstance ( ) . getConfig ( ) . isUseAsyncServer ( ) ) { AsyncDataClient client = metaGroupMember . getClientProvider ( ) . getAsyncDataClient ( source , RaftServer . getReadOperationTimeoutMS ( ) ) ; aggrBuffers = SyncClientAdaptor . getGroupByResult ( client , header , executorId , curStartTime , curEndTime ) ; } else { try ( SyncDataClient syncDataClient = metaGroupMember . getClientProvider ( ) . getSyncDataClient ( source , RaftServer . getReadOperationTimeoutMS ( ) ) ) { aggrBuffers = syncDataClient . getGroupByResult ( header , executorId , curStartTime , curEndTime ) ; } } } catch ( TException e ) { throw new IOException ( e ) ; } catch ( InterruptedException e ) { Thread . currentThread ( ) . interrupt ( ) ; throw new IOException ( e ) ; } resetAggregateResults ( ) ; if ( aggrBuffers != null ) { for ( int i = 0 ; i < aggrBuffers . size ( ) ; i ++ ) { AggregateResult result = AggregateResult . deserializeFrom ( aggrBuffers . get ( i ) ) ; results . get ( i ) . merge ( result ) ; } } logger . debug ( ""Fetched group by result from {} of [{}, {}]: {}"" , source , curStartTime , curEndTime , results ) ; return results ; } }","logger . debug ( ""Fetched group by result from {} of [{}, {}]: {}"" , source , curStartTime , curEndTime , results ) ;",Meaningful
"public class A { @ Override public boolean stopKubernetesCluster ( long kubernetesClusterId ) throws CloudRuntimeException { if ( ! KubernetesServiceEnabled . value ( ) ) { logAndThrow ( Level . ERROR , ""Kubernetes Service plugin is disabled"" ) ; } final KubernetesClusterVO kubernetesCluster = kubernetesClusterDao . findById ( kubernetesClusterId ) ; if ( kubernetesCluster == null ) { throw new InvalidParameterValueException ( ""Failed to find Kubernetes cluster with given ID"" ) ; } if ( kubernetesCluster . getRemoved ( ) != null ) { throw new InvalidParameterValueException ( String . format ( ""Kubernetes cluster : %s is already deleted"" , kubernetesCluster . getName ( ) ) ) ; } accountManager . checkAccess ( CallContext . current ( ) . getCallingAccount ( ) , SecurityChecker . AccessType . OperateEntry , false , kubernetesCluster ) ; if ( kubernetesCluster . getState ( ) . equals ( KubernetesCluster . State . Stopped ) ) { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( String . format ( ""Kubernetes cluster : %s is already stopped"" , kubernetesCluster . getName ( ) ) ) ; } return true ; } if ( kubernetesCluster . getState ( ) . equals ( KubernetesCluster . State . Stopping ) ) { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( String . format ( ""Kubernetes cluster : %s is already stopped, but Kubernetes cluster state is %s"" , kubernetesCluster . getName ( ) , kubernetesCluster . getState ( ) ) ) ; } return true ; } KubernetesClusterStopWorker stopWorker = new KubernetesClusterStopWorker ( kubernetesCluster , this ) ; stopWorker = ComponentContext . inject ( stopWorker ) ; return stopWorker . stop ( ) ; } }","public class A { @ Override public boolean stopKubernetesCluster ( long kubernetesClusterId ) throws CloudRuntimeException { if ( ! KubernetesServiceEnabled . value ( ) ) { logAndThrow ( Level . ERROR , ""Kubernetes Service plugin is disabled"" ) ; } final KubernetesClusterVO kubernetesCluster = kubernetesClusterDao . findById ( kubernetesClusterId ) ; if ( kubernetesCluster == null ) { throw new InvalidParameterValueException ( ""Failed to find Kubernetes cluster with given ID"" ) ; } if ( kubernetesCluster . getRemoved ( ) != null ) { throw new InvalidParameterValueException ( String . format ( ""Kubernetes cluster : %s is already deleted"" , kubernetesCluster . getName ( ) ) ) ; } accountManager . checkAccess ( CallContext . current ( ) . getCallingAccount ( ) , SecurityChecker . AccessType . OperateEntry , false , kubernetesCluster ) ; if ( kubernetesCluster . getState ( ) . equals ( KubernetesCluster . State . Stopped ) ) { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( String . format ( ""Kubernetes cluster : %s is already stopped"" , kubernetesCluster . getName ( ) ) ) ; } return true ; } if ( kubernetesCluster . getState ( ) . equals ( KubernetesCluster . State . Stopping ) ) { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( String . format ( ""Kubernetes cluster : %s is getting stopped"" , kubernetesCluster . getName ( ) ) ) ; } return true ; } KubernetesClusterStopWorker stopWorker = new KubernetesClusterStopWorker ( kubernetesCluster , this ) ; stopWorker = ComponentContext . inject ( stopWorker ) ; return stopWorker . stop ( ) ; } }","LOGGER . debug ( String . format ( ""Kubernetes cluster : %s is getting stopped"" , kubernetesCluster . getName ( ) ) ) ;",Meaningful
"public class A { @ Override public SegmentMetadata load ( File segmentDir ) throws Exception { final SegmentMetadata segmentMetadata = new SegmentMetadataImpl ( segmentDir ) ; LOGGER . info ( ""Loading segment metadata: {}"" , segmentMetadata ) ; return segmentMetadata ; } }","public class A { @ Override public SegmentMetadata load ( File segmentDir ) throws Exception { final SegmentMetadata segmentMetadata = new SegmentMetadataImpl ( segmentDir ) ; LOGGER . info ( ""Loaded segment metadata for segment : "" + segmentMetadata . getName ( ) ) ; return segmentMetadata ; } }","LOGGER . info ( ""Loaded segment metadata for segment : "" + segmentMetadata . getName ( ) ) ;",Meaningful
"public class A { @ Activate public void activate ( ComponentContext cc ) throws Exception { logger . debug ( ""Activating admin group loader"" ) ; BundleContext bundleCtx = cc . getBundleContext ( ) ; adminUserName = StringUtils . trimToNull ( bundleCtx . getProperty ( SecurityConstants . GLOBAL_ADMIN_USER_PROPERTY ) ) ; adminPassword = StringUtils . trimToNull ( bundleCtx . getProperty ( OPT_ADMIN_PASSWORD ) ) ; adminEmail = StringUtils . trimToNull ( bundleCtx . getProperty ( OPT_ADMIN_EMAIL ) ) ; adminRoles = StringUtils . trimToNull ( bundleCtx . getProperty ( OPT_ADMIN_ROLES ) ) ; if ( DEFAULT_ADMIN_PASSWORD_CONFIGURATION . equals ( adminPassword ) ) { logger . warn ( ""Users {} option of default admin password is set to {}"" , getConfiguration ( ) . getString ( ""System-admin.properties"" ) , DEFAULT_ADMIN_PASSWORD_CONFIGURATION ) ; } componentCtx = cc ; for ( final Organization organization : organizationDirectoryService . getOrganizations ( ) ) { createSystemAdministratorUserAndGroup ( organization ) ; } } }","public class A { @ Activate public void activate ( ComponentContext cc ) throws Exception { logger . debug ( ""Activating admin group loader"" ) ; BundleContext bundleCtx = cc . getBundleContext ( ) ; adminUserName = StringUtils . trimToNull ( bundleCtx . getProperty ( SecurityConstants . GLOBAL_ADMIN_USER_PROPERTY ) ) ; adminPassword = StringUtils . trimToNull ( bundleCtx . getProperty ( OPT_ADMIN_PASSWORD ) ) ; adminEmail = StringUtils . trimToNull ( bundleCtx . getProperty ( OPT_ADMIN_EMAIL ) ) ; adminRoles = StringUtils . trimToNull ( bundleCtx . getProperty ( OPT_ADMIN_ROLES ) ) ; if ( DEFAULT_ADMIN_PASSWORD_CONFIGURATION . equals ( adminPassword ) ) { logger . warn ( ""\n"" + ""######################################################\n"" + ""# #\n"" + ""# WARNING: Opencast still uses the default admin #\n"" + ""# credentials. Never do this in production. #\n"" + ""# #\n"" + ""# To change the password, edit the key #\n"" + ""# org.opencastproject.security.admin.pass #\n"" + ""# in custom.properties. #\n"" + ""# #\n"" + ""######################################################"" ) ; } componentCtx = cc ; for ( final Organization organization : organizationDirectoryService . getOrganizations ( ) ) { createSystemAdministratorUserAndGroup ( organization ) ; } } }","logger . warn ( ""\n"" + ""######################################################\n"" + ""# #\n"" + ""# WARNING: Opencast still uses the default admin #\n"" + ""# credentials. Never do this in production. #\n"" + ""# #\n"" + ""# To change the password, edit the key #\n"" + ""# org.opencastproject.security.admin.pass #\n"" + ""# in custom.properties. #\n"" + ""# #\n"" + ""######################################################"" ) ;",Meaningful
"public class A { public void deployTarballResource ( String url , String targetName ) { try { doDeployTarballResource ( url , targetName ) ; } catch ( RuntimeException e ) { LOG . error ( ""Error deploying tarball resource {}"" , targetName , e ) ; throw Throwables . propagate ( e ) ; } } }","public class A { public void deployTarballResource ( String url , String targetName ) { try { doDeployTarballResource ( url , targetName ) ; } catch ( RuntimeException e ) { LOG . error ( ""Error deploying '"" + url + ""' on "" + toString ( ) + ""; rethrowing..."" , e ) ; throw Throwables . propagate ( e ) ; } } }","LOG . error ( ""Error deploying '"" + url + ""' on "" + toString ( ) + ""; rethrowing..."" , e ) ;",Meaningful
"public class A { private synchronized void monitor ( ) { try { monitorRequestPending . set ( false ) ; if ( this . modems . isEmpty ( ) ) { return ; } final HashMap < String , InterfaceState > newInterfaceStatuses = new HashMap < > ( ) ; logger . debug ( ""tracked modems: {}"" , modems . keySet ( ) ) ; for ( final Entry < String , MonitoredModem > e : this . modems . entrySet ( ) ) { logger . debug ( ""monitoring monitor {}"" , e . getKey ( ) ) ; processMonitor ( newInterfaceStatuses , e . getKey ( ) , e . getValue ( ) ) ; } checkStatusChange ( this . interfaceStatuses , newInterfaceStatuses ) ; this . interfaceStatuses = newInterfaceStatuses ; } catch ( Exception ex ) { logger . error ( ""Unexpected error during monitoring"" , ex ) ; } } }","public class A { private synchronized void monitor ( ) { try { monitorRequestPending . set ( false ) ; if ( this . modems . isEmpty ( ) ) { return ; } final HashMap < String , InterfaceState > newInterfaceStatuses = new HashMap < > ( ) ; logger . debug ( ""tracked modems: {}"" , modems . keySet ( ) ) ; for ( final Entry < String , MonitoredModem > e : this . modems . entrySet ( ) ) { logger . debug ( ""processing modem {}"" , e . getKey ( ) ) ; processMonitor ( newInterfaceStatuses , e . getKey ( ) , e . getValue ( ) ) ; } checkStatusChange ( this . interfaceStatuses , newInterfaceStatuses ) ; this . interfaceStatuses = newInterfaceStatuses ; } catch ( Exception ex ) { logger . error ( ""Unexpected error during monitoring"" , ex ) ; } } }","logger . debug ( ""processing modem {}"" , e . getKey ( ) ) ;",Meaningful
"public class A { @ Override protected void process ( ClusterDistributionManager dm ) { if ( CacheClientNotifier . getInstance ( ) != null ) { Class < ? extends DataSerializer > c ; try { c = getCachedClass ( this . className ) ; } catch ( ClassNotFoundException ex ) { logger . warn ( ""Could not load data serializer class {} so both clients of this server and this server will not have this data serializer. Load failed because: {}"" , this . className , getFullMessage ( ex ) ) ; return ; } DataSerializer s ; try { s = newInstance ( c ) ; } catch ( IllegalArgumentException ex ) { logger . warn ( ""Could not create an instance of data serializer for class {} so both clients of this server and this server will not have this data serializer. Create failed because: {}"" , this . className , getFullMessage ( ex ) ) ; return ; } s . setEventId ( this . eventId ) ; try { InternalDataSerializer . _register ( s , false ) ; } catch ( IllegalArgumentException | IllegalStateException ex ) { logger . warn ( ""Could not register data serializer for class {} so both clients of this server and this server will not have this data serializer. Registration failed because: {}"" , this . className , getFullMessage ( ex ) ) ; } } else { try { InternalDataSerializer . register ( this . className , false , this . eventId , null , this . id ) ; } catch ( IllegalArgumentException | IllegalStateException ex ) { logger . warn ( ""Could not register data serializer for class {} so both clients of this server and this server will not have this data serializer. Registration failed because: {}"" , this . className , getFullMessage ( ex ) ) ; } } } }","public class A { @ Override protected void process ( ClusterDistributionManager dm ) { if ( CacheClientNotifier . getInstance ( ) != null ) { Class < ? extends DataSerializer > c ; try { c = getCachedClass ( this . className ) ; } catch ( ClassNotFoundException ex ) { logger . warn ( ""Could not load data serializer class {} so both clients of this server and this server will not have this data serializer. Load failed because: {}"" , this . className , getFullMessage ( ex ) ) ; return ; } DataSerializer s ; try { s = newInstance ( c ) ; } catch ( IllegalArgumentException ex ) { logger . warn ( ""Could not create an instance of data serializer for class {} so both clients of this server and this server will not have this data serializer. Create failed because: {}"" , this . className , getFullMessage ( ex ) ) ; return ; } s . setEventId ( this . eventId ) ; try { InternalDataSerializer . _register ( s , false ) ; } catch ( IllegalArgumentException | IllegalStateException ex ) { logger . warn ( ""Could not register data serializer for class {} so both clients of this server and this server will not have this data serializer. Registration failed because: {}"" , this . className , getFullMessage ( ex ) ) ; } } else { try { InternalDataSerializer . register ( this . className , false , this . eventId , null , this . id ) ; } catch ( IllegalArgumentException | IllegalStateException ex ) { logger . warn ( ""Could not register data serializer for class {} so it will not be available in this JVM. Registration failed because: {}"" , this . className , getFullMessage ( ex ) ) ; } } } }","logger . warn ( ""Could not register data serializer for class {} so it will not be available in this JVM. Registration failed because: {}"" , this . className , getFullMessage ( ex ) ) ;",Meaningful
"public class A { @ Override public void save ( @ Nonnull final Patient patient , @ Nonnull final PatientWritePolicy policy ) { try { final XWikiContext context = this . xcontextProvider . get ( ) ; final BaseObject xobject = patient . getXDocument ( ) . getXObject ( Patient . CLASS_REFERENCE , true , context ) ; final PatientData < List < VocabularyTerm > > data = patient . getData ( getName ( ) ) ; if ( data == null ) { if ( PatientWritePolicy . REPLACE . equals ( policy ) ) { getProperties ( ) . forEach ( p -> xobject . set ( p , null , context ) ) ; } } else { if ( ! data . isNamed ( ) ) { this . logger . error ( ERROR_MESSAGE_DATA_IN_MEMORY_IN_WRONG_FORMAT ) ; return ; } saveQualifiersData ( patient , xobject , data , policy , context ) ; } } catch ( final Exception ex ) { this . logger . error ( ""Failed to save phenotypes: {}"" , ex . getMessage ( ) , ex ) ; } } }","public class A { @ Override public void save ( @ Nonnull final Patient patient , @ Nonnull final PatientWritePolicy policy ) { try { final XWikiContext context = this . xcontextProvider . get ( ) ; final BaseObject xobject = patient . getXDocument ( ) . getXObject ( Patient . CLASS_REFERENCE , true , context ) ; final PatientData < List < VocabularyTerm > > data = patient . getData ( getName ( ) ) ; if ( data == null ) { if ( PatientWritePolicy . REPLACE . equals ( policy ) ) { getProperties ( ) . forEach ( p -> xobject . set ( p , null , context ) ) ; } } else { if ( ! data . isNamed ( ) ) { this . logger . error ( ERROR_MESSAGE_DATA_IN_MEMORY_IN_WRONG_FORMAT ) ; return ; } saveQualifiersData ( patient , xobject , data , policy , context ) ; } } catch ( final Exception ex ) { this . logger . error ( ""Failed to save global qualifiers data: {}"" , ex . getMessage ( ) , ex ) ; } } }","this . logger . error ( ""Failed to save global qualifiers data: {}"" , ex . getMessage ( ) , ex ) ;",Meaningful
"public class A { @ Override public void enqueueMutateBatch ( List < RowMutation > mutations ) throws BlurException , TException { try { resetSearchers ( ) ; for ( RowMutation mutation : mutations ) { checkTable ( _cluster , mutation . table ) ; checkForUpdates ( _cluster , mutation . table ) ; MutationHelper . validateMutation ( mutation ) ; } _indexManager . enqueue ( mutations ) ; } catch ( Exception e ) { LOG . error ( ""Unknown error while trying to enqueue updates [table={0}]"" , e , mutations ) ; if ( e instanceof BlurException ) { throw ( BlurException ) e ; } throw new BException ( e . getMessage ( ) , e ) ; } } }","public class A { @ Override public void enqueueMutateBatch ( List < RowMutation > mutations ) throws BlurException , TException { try { resetSearchers ( ) ; for ( RowMutation mutation : mutations ) { checkTable ( _cluster , mutation . table ) ; checkForUpdates ( _cluster , mutation . table ) ; MutationHelper . validateMutation ( mutation ) ; } _indexManager . enqueue ( mutations ) ; } catch ( Exception e ) { LOG . error ( ""Unknown error during processing of [mutations={0}]"" , e , mutations ) ; if ( e instanceof BlurException ) { throw ( BlurException ) e ; } throw new BException ( e . getMessage ( ) , e ) ; } } }","LOG . error ( ""Unknown error during processing of [mutations={0}]"" , e , mutations ) ;",Meaningful
"public class A { private void createClearingRequest ( ResourceRequest request , ResourceResponse response ) throws PortletException { User user = UserCacheHolder . getUserFromRequest ( request ) ; ClearingRequest clearingRequest = null ; AddDocumentRequestSummary requestSummary = null ; try { JsonNode crNode = OBJECT_MAPPER . readValue ( request . getParameter ( CLEARING_REQUEST ) , JsonNode . class ) ; clearingRequest = OBJECT_MAPPER . convertValue ( crNode , ClearingRequest . class ) ; clearingRequest . setRequestingUser ( user . getEmail ( ) ) ; clearingRequest . setClearingState ( ClearingRequestState . NEW ) ; LiferayPortletURL projectUrl = createDetailLinkTemplate ( request ) ; projectUrl . setParameter ( PROJECT_ID , clearingRequest . getProjectId ( ) ) ; projectUrl . setParameter ( PAGENAME , PAGENAME_DETAIL ) ; ProjectService . Iface client = thriftClients . makeProjectClient ( ) ; requestSummary = client . createClearingRequest ( clearingRequest , user , projectUrl . toString ( ) ) ; } catch ( IOException | TException e ) { log . error ( ""Failed to create clearing request"" , e ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , ""500"" ) ; } try { JsonGenerator jsonGenerator = JSON_FACTORY . createGenerator ( response . getWriter ( ) ) ; jsonGenerator . writeStartObject ( ) ; jsonGenerator . writeStringField ( RESULT , requestSummary . getRequestStatus ( ) . toString ( ) ) ; if ( AddDocumentRequestStatus . FAILURE . equals ( requestSummary . getRequestStatus ( ) ) ) { ResourceBundle resourceBundle = ResourceBundleUtil . getBundle ( ""content.Language"" , request . getLocale ( ) , getClass ( ) ) ; jsonGenerator . writeStringField ( ""message"" , LanguageUtil . get ( resourceBundle , requestSummary . getMessage ( ) . replace ( ' ' , '.' ) . toLowerCase ( ) ) ) ; } else { jsonGenerator . writeStringField ( CLEARING_REQUEST_ID , requestSummary . getId ( ) ) ; } jsonGenerator . writeEndObject ( ) ; jsonGenerator . close ( ) ; } catch ( IOException e ) { log . error ( ""Cannot write JSON response for clearing request id "" + requestSummary . getId ( ) + "" in project "" + clearingRequest . getProjectId ( ) + ""."" , e ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , ""500"" ) ; } } }","public class A { private void createClearingRequest ( ResourceRequest request , ResourceResponse response ) throws PortletException { User user = UserCacheHolder . getUserFromRequest ( request ) ; ClearingRequest clearingRequest = null ; AddDocumentRequestSummary requestSummary = null ; try { JsonNode crNode = OBJECT_MAPPER . readValue ( request . getParameter ( CLEARING_REQUEST ) , JsonNode . class ) ; clearingRequest = OBJECT_MAPPER . convertValue ( crNode , ClearingRequest . class ) ; clearingRequest . setRequestingUser ( user . getEmail ( ) ) ; clearingRequest . setClearingState ( ClearingRequestState . NEW ) ; LiferayPortletURL projectUrl = createDetailLinkTemplate ( request ) ; projectUrl . setParameter ( PROJECT_ID , clearingRequest . getProjectId ( ) ) ; projectUrl . setParameter ( PAGENAME , PAGENAME_DETAIL ) ; ProjectService . Iface client = thriftClients . makeProjectClient ( ) ; requestSummary = client . createClearingRequest ( clearingRequest , user , projectUrl . toString ( ) ) ; } catch ( IOException | TException e ) { log . error ( ""Error creating clearing request for project: "" + clearingRequest . getProjectId ( ) , e ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , ""500"" ) ; } try { JsonGenerator jsonGenerator = JSON_FACTORY . createGenerator ( response . getWriter ( ) ) ; jsonGenerator . writeStartObject ( ) ; jsonGenerator . writeStringField ( RESULT , requestSummary . getRequestStatus ( ) . toString ( ) ) ; if ( AddDocumentRequestStatus . FAILURE . equals ( requestSummary . getRequestStatus ( ) ) ) { ResourceBundle resourceBundle = ResourceBundleUtil . getBundle ( ""content.Language"" , request . getLocale ( ) , getClass ( ) ) ; jsonGenerator . writeStringField ( ""message"" , LanguageUtil . get ( resourceBundle , requestSummary . getMessage ( ) . replace ( ' ' , '.' ) . toLowerCase ( ) ) ) ; } else { jsonGenerator . writeStringField ( CLEARING_REQUEST_ID , requestSummary . getId ( ) ) ; } jsonGenerator . writeEndObject ( ) ; jsonGenerator . close ( ) ; } catch ( IOException e ) { log . error ( ""Cannot write JSON response for clearing request id "" + requestSummary . getId ( ) + "" in project "" + clearingRequest . getProjectId ( ) + ""."" , e ) ; response . setProperty ( ResourceResponse . HTTP_STATUS_CODE , ""500"" ) ; } } }","log . error ( ""Error creating clearing request for project: "" + clearingRequest . getProjectId ( ) , e ) ;",Meaningful
"public class A { private void initialize ( ChannelHandlerContext ctx ) { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""Initialize {}"" , ctx . channel ( ) . remoteAddress ( ) ) ; } switch ( state ) { case 1 : case 2 : return ; } state = 1 ; EventExecutor loop = ctx . executor ( ) ; lastExecutionTime = System . nanoTime ( ) ; resenderTimeout = loop . schedule ( new WriterIdleTimeoutTask ( ctx ) , resenderTimeNanos , TimeUnit . NANOSECONDS ) ; } }","public class A { private void initialize ( ChannelHandlerContext ctx ) { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""Initializing autoflush handler on channel {}"" , ctx . channel ( ) ) ; } switch ( state ) { case 1 : case 2 : return ; } state = 1 ; EventExecutor loop = ctx . executor ( ) ; lastExecutionTime = System . nanoTime ( ) ; resenderTimeout = loop . schedule ( new WriterIdleTimeoutTask ( ctx ) , resenderTimeNanos , TimeUnit . NANOSECONDS ) ; } }","LOG . debug ( ""Initializing autoflush handler on channel {}"" , ctx . channel ( ) ) ;",Meaningful
"public class A { private void checkConnectionOpen ( ) { try { if ( ! connection . isValid ( 10 ) ) { LOG . info ( ""db connection reconnect.."" ) ; establishConnection ( ) ; jdbcWriter . prepareStatement ( connection ) ; } } catch ( SQLException e ) { LOG . error ( ""check connection open failed.."" , e ) ; } catch ( ClassNotFoundException e ) { LOG . error ( ""check connection open failed.. ClassNotFoundException was not found."" , e ) ; } catch ( IOException e ) { LOG . error ( ""jdbc io exception.."" , e ) ; } } }","public class A { private void checkConnectionOpen ( ) { try { if ( ! connection . isValid ( 10 ) ) { LOG . info ( ""db connection reconnect.."" ) ; establishConnection ( ) ; jdbcWriter . prepareStatement ( connection ) ; } } catch ( SQLException e ) { LOG . error ( ""check connection open failed.."" , e ) ; } catch ( ClassNotFoundException e ) { LOG . error ( ""load jdbc class error when reconnect db.."" , e ) ; } catch ( IOException e ) { LOG . error ( ""jdbc io exception.."" , e ) ; } } }","LOG . error ( ""load jdbc class error when reconnect db.."" , e ) ;",Meaningful
"public class A { @ Override public void onEntry ( FilterContext filterContext , FilterChain next ) throws SoaException { try { InvocationContextImpl invocationContext = ( InvocationContextImpl ) filterContext . getAttribute ( ""context"" ) ; filterContext . setAttribute ( ""startTime"" , System . currentTimeMillis ( ) ) ; InvocationInfoImpl invocationInfo = new InvocationInfoImpl ( ) ; invocationContext . lastInvocationInfo ( invocationInfo ) ; if ( ! invocationContext . sessionTid ( ) . isPresent ( ) ) { if ( TransactionContext . hasCurrentInstance ( ) && TransactionContext . Factory . currentInstance ( ) . sessionTid ( ) . isPresent ( ) ) { invocationContext . sessionTid ( TransactionContext . Factory . currentInstance ( ) . sessionTid ( ) . get ( ) ) ; } else { invocationContext . sessionTid ( DapengUtil . generateTid ( ) ) ; } } String logLevel = invocationContext . cookie ( SoaSystemEnvProperties . THREAD_LEVEL_KEY ) ; if ( logLevel != null ) { MDC . put ( SoaSystemEnvProperties . THREAD_LEVEL_KEY , logLevel ) ; } MDC . put ( SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID , invocationContext . sessionTid ( ) . map ( DapengUtil :: longToHexStr ) . orElse ( ""0"" ) ) ; String infoLog = ""request[seqId:"" + invocationContext . seqId ( ) + "", server:"" + filterContext . getAttribute ( ""serverInfo"" ) + ""]:"" + ""service["" + invocationContext . serviceName ( ) + ""]:version["" + invocationContext . versionName ( ) + ""]:method["" + invocationContext . methodName ( ) + ""]"" ; LOGGER . info ( infoLog ) ; } finally { next . onEntry ( filterContext ) ; } } }","public class A { @ Override public void onEntry ( FilterContext filterContext , FilterChain next ) throws SoaException { try { InvocationContextImpl invocationContext = ( InvocationContextImpl ) filterContext . getAttribute ( ""context"" ) ; filterContext . setAttribute ( ""startTime"" , System . currentTimeMillis ( ) ) ; InvocationInfoImpl invocationInfo = new InvocationInfoImpl ( ) ; invocationContext . lastInvocationInfo ( invocationInfo ) ; if ( ! invocationContext . sessionTid ( ) . isPresent ( ) ) { if ( TransactionContext . hasCurrentInstance ( ) && TransactionContext . Factory . currentInstance ( ) . sessionTid ( ) . isPresent ( ) ) { invocationContext . sessionTid ( TransactionContext . Factory . currentInstance ( ) . sessionTid ( ) . get ( ) ) ; } else { invocationContext . sessionTid ( DapengUtil . generateTid ( ) ) ; } } String logLevel = invocationContext . cookie ( SoaSystemEnvProperties . THREAD_LEVEL_KEY ) ; if ( logLevel != null ) { MDC . put ( SoaSystemEnvProperties . THREAD_LEVEL_KEY , logLevel ) ; } MDC . put ( SoaSystemEnvProperties . KEY_LOGGER_SESSION_TID , invocationContext . sessionTid ( ) . map ( DapengUtil :: longToHexStr ) . orElse ( ""0"" ) ) ; String infoLog = ""request[seqId:"" + invocationContext . seqId ( ) + "", server:"" + filterContext . getAttribute ( ""serverInfo"" ) + ""]:"" + ""service["" + invocationContext . serviceName ( ) + ""]:version["" + invocationContext . versionName ( ) + ""]:method["" + invocationContext . methodName ( ) + ""]"" ; LOGGER . info ( getClass ( ) . getSimpleName ( ) + ""::onEntry,"" + infoLog ) ; } finally { next . onEntry ( filterContext ) ; } } }","LOGGER . info ( getClass ( ) . getSimpleName ( ) + ""::onEntry,"" + infoLog ) ;",Meaningful
"public class A { public void editDomain ( ActionEvent event ) { UpdateDomain updateDomain = new UpdateDomain ( ) ; updateDomain . setDomain ( selectedDomain ) ; try { directService . updateDomain ( updateDomain ) ; selectedDomain = null ; refreshDomains ( ) ; } catch ( DomainException domainException ) { FacesContext . getCurrentInstance ( ) . validationFailed ( ) ; FacesContext . getCurrentInstance ( ) . addMessage ( ""domainEditErrors"" , new FacesMessage ( FacesMessage . SEVERITY_ERROR , ""Cannot update domain: "" + domainException . getLocalizedMessage ( ) , """" ) ) ; LOG . error ( ""Cannot update domain"" , domainException ) ; } } }","public class A { public void editDomain ( ActionEvent event ) { UpdateDomain updateDomain = new UpdateDomain ( ) ; updateDomain . setDomain ( selectedDomain ) ; try { directService . updateDomain ( updateDomain ) ; selectedDomain = null ; refreshDomains ( ) ; } catch ( DomainException domainException ) { FacesContext . getCurrentInstance ( ) . validationFailed ( ) ; FacesContext . getCurrentInstance ( ) . addMessage ( ""domainEditErrors"" , new FacesMessage ( FacesMessage . SEVERITY_ERROR , ""Cannot update domain: "" + domainException . getLocalizedMessage ( ) , """" ) ) ; LOG . error ( ""Error updating domain: {}"" , domainException . getLocalizedMessage ( ) , domainException ) ; } } }","LOG . error ( ""Error updating domain: {}"" , domainException . getLocalizedMessage ( ) , domainException ) ;",Meaningful
"public class A { public void setup ( String [ ] names , String [ ] tablepaths , Schema [ ] schemas ) throws Exception { LOG . info ( ""==================================================="" ) ; LOG . info ( ""Creating a runtime table path.."" ) ; LOG . info ( ""==================================================="" ) ; util = new TajoTestingCluster ( ) ; util . startMiniCluster ( 1 ) ; conf = util . getConfiguration ( ) ; client = util . newTajoClient ( ) ; FileSystem fs = util . getDefaultFileSystem ( ) ; Path rootDir = TajoConf . getWarehouseDir ( conf ) ; fs . mkdirs ( rootDir ) ; for ( int i = 0 ; i < tablepaths . length ; i ++ ) { Path localPath = new Path ( tablepaths [ i ] ) ; Path tablePath = new Path ( rootDir , names [ i ] ) ; fs . mkdirs ( tablePath ) ; Path dfsPath = new Path ( tablePath , localPath . getName ( ) ) ; fs . copyFromLocalFile ( localPath , dfsPath ) ; TableMeta meta = CatalogUtil . newTableMeta ( BuiltinStorages . TEXT , conf ) ; TableStats stats = new TableStats ( ) ; stats . setNumBytes ( TPCH . tableVolumes . get ( names [ i ] ) ) ; TableDesc tableDesc = new TableDesc ( IdentifierUtil . buildFQName ( TajoConstants . DEFAULT_DATABASE_NAME , names [ i ] ) , schemas [ i ] , meta , tablePath . toUri ( ) ) ; tableDesc . setStats ( stats ) ; util . getMaster ( ) . getCatalog ( ) . createTable ( tableDesc ) ; } LOG . info ( ""==================================================="" ) ; LOG . info ( ""Test Cluster ready and test table created."" ) ; LOG . info ( ""==================================================="" ) ; } }","public class A { public void setup ( String [ ] names , String [ ] tablepaths , Schema [ ] schemas ) throws Exception { LOG . info ( ""==================================================="" ) ; LOG . info ( ""Starting Test Cluster."" ) ; LOG . info ( ""==================================================="" ) ; util = new TajoTestingCluster ( ) ; util . startMiniCluster ( 1 ) ; conf = util . getConfiguration ( ) ; client = util . newTajoClient ( ) ; FileSystem fs = util . getDefaultFileSystem ( ) ; Path rootDir = TajoConf . getWarehouseDir ( conf ) ; fs . mkdirs ( rootDir ) ; for ( int i = 0 ; i < tablepaths . length ; i ++ ) { Path localPath = new Path ( tablepaths [ i ] ) ; Path tablePath = new Path ( rootDir , names [ i ] ) ; fs . mkdirs ( tablePath ) ; Path dfsPath = new Path ( tablePath , localPath . getName ( ) ) ; fs . copyFromLocalFile ( localPath , dfsPath ) ; TableMeta meta = CatalogUtil . newTableMeta ( BuiltinStorages . TEXT , conf ) ; TableStats stats = new TableStats ( ) ; stats . setNumBytes ( TPCH . tableVolumes . get ( names [ i ] ) ) ; TableDesc tableDesc = new TableDesc ( IdentifierUtil . buildFQName ( TajoConstants . DEFAULT_DATABASE_NAME , names [ i ] ) , schemas [ i ] , meta , tablePath . toUri ( ) ) ; tableDesc . setStats ( stats ) ; util . getMaster ( ) . getCatalog ( ) . createTable ( tableDesc ) ; } LOG . info ( ""==================================================="" ) ; LOG . info ( ""Test Cluster ready and test table created."" ) ; LOG . info ( ""==================================================="" ) ; } }","LOG . info ( ""Starting Test Cluster."" ) ;",Meaningful
"public class A { private boolean testForDuplicateSize ( SearchResultItem result1 , SearchResultItem result2 , float duplicateSizeDifference ) { if ( result1 . getSize ( ) == null || result2 . getSize ( ) == null ) { return false ; } long sizeDifference = Math . abs ( result1 . getSize ( ) - result2 . getSize ( ) ) ; float sizeAverage = ( result1 . getSize ( ) + result2 . getSize ( ) ) / 2F ; float sizeDiffPercent = Math . abs ( sizeDifference / sizeAverage ) * 100 ; boolean sameSize = sizeDiffPercent <= duplicateSizeDifference ; log . debug ( ""Comparing {} from {} to {}"" , result1 , sizeDifference , sameSize ) ; return sameSize ; } }","public class A { private boolean testForDuplicateSize ( SearchResultItem result1 , SearchResultItem result2 , float duplicateSizeDifference ) { if ( result1 . getSize ( ) == null || result2 . getSize ( ) == null ) { return false ; } long sizeDifference = Math . abs ( result1 . getSize ( ) - result2 . getSize ( ) ) ; float sizeAverage = ( result1 . getSize ( ) + result2 . getSize ( ) ) / 2F ; float sizeDiffPercent = Math . abs ( sizeDifference / sizeAverage ) * 100 ; boolean sameSize = sizeDiffPercent <= duplicateSizeDifference ; logger . debug ( LoggingMarkers . DUPLICATES , ""Same size: {}"" , sameSize ) ; return sameSize ; } }","logger . debug ( LoggingMarkers . DUPLICATES , ""Same size: {}"" , sameSize ) ;",Meaningful
"public class A { private boolean parseStandardNoSeparator ( boolean isLatitude , String locStr ) { boolean isLikelyValue = false ; String toParse = locStr ; if ( toParse . length ( ) > 5 ) { if ( toParse . contains ( ""."" ) ) { toParse = toParse . split ( ""\\."" ) [ 0 ] ; } try { if ( isLatitude ) { int locValue = Integer . parseInt ( toParse . substring ( 0 , 2 ) ) ; if ( locValue >= 0 && locValue <= 90 && validateMinSecValue ( toParse . substring ( 2 , 4 ) ) && validateMinSecValue ( toParse . substring ( 4 , 6 ) ) ) { isLikelyValue = true ; myConfidence = 1.0f ; } } else { int locValue = - 1 ; String minStr = null ; String secStr = null ; if ( toParse . length ( ) == 6 ) { locValue = Integer . parseInt ( toParse . substring ( 0 , 2 ) ) ; minStr = toParse . substring ( 2 , 3 ) ; secStr = toParse . substring ( 4 ) ; } else if ( toParse . length ( ) == 7 ) { locValue = Integer . parseInt ( toParse . substring ( 0 , 3 ) ) ; minStr = toParse . substring ( 3 , 5 ) ; secStr = toParse . substring ( 5 ) ; } if ( locValue >= 0 && locValue <= 180 && validateMinSecValue ( minStr ) && validateMinSecValue ( secStr ) ) { isLikelyValue = true ; myConfidence = 1.0f ; } } } catch ( NumberFormatException e ) { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( ""Error parsing version. ["" + e . getMessage ( ) + ""]."" , e ) ; } } catch ( PatternSyntaxException e ) { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( ""Error validating standard format DMS pattern."" , e ) ; } } } return isLikelyValue ; } }","public class A { private boolean parseStandardNoSeparator ( boolean isLatitude , String locStr ) { boolean isLikelyValue = false ; String toParse = locStr ; if ( toParse . length ( ) > 5 ) { if ( toParse . contains ( ""."" ) ) { toParse = toParse . split ( ""\\."" ) [ 0 ] ; } try { if ( isLatitude ) { int locValue = Integer . parseInt ( toParse . substring ( 0 , 2 ) ) ; if ( locValue >= 0 && locValue <= 90 && validateMinSecValue ( toParse . substring ( 2 , 4 ) ) && validateMinSecValue ( toParse . substring ( 4 , 6 ) ) ) { isLikelyValue = true ; myConfidence = 1.0f ; } } else { int locValue = - 1 ; String minStr = null ; String secStr = null ; if ( toParse . length ( ) == 6 ) { locValue = Integer . parseInt ( toParse . substring ( 0 , 2 ) ) ; minStr = toParse . substring ( 2 , 3 ) ; secStr = toParse . substring ( 4 ) ; } else if ( toParse . length ( ) == 7 ) { locValue = Integer . parseInt ( toParse . substring ( 0 , 3 ) ) ; minStr = toParse . substring ( 3 , 5 ) ; secStr = toParse . substring ( 5 ) ; } if ( locValue >= 0 && locValue <= 180 && validateMinSecValue ( minStr ) && validateMinSecValue ( secStr ) ) { isLikelyValue = true ; myConfidence = 1.0f ; } } } catch ( NumberFormatException e ) { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( ""Error validating standard format DMS string."" , e ) ; } } catch ( PatternSyntaxException e ) { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( ""Error validating standard format DMS pattern."" , e ) ; } } } return isLikelyValue ; } }","LOGGER . debug ( ""Error validating standard format DMS string."" , e ) ;",Meaningful
"public class A { @ Nullable Pack getPack ( AnyObjectId objectId ) { PackList pList ; do { pList = packList . get ( ) ; for ( Pack p : pList . packs ) { try { if ( p . hasObject ( objectId ) ) { return p ; } } catch ( IOException e ) { log . warn ( ""Pack {} was not found"" , objectId ) ; remove ( p ) ; } } } while ( searchPacksAgain ( pList ) ) ; return null ; } }","public class A { @ Nullable Pack getPack ( AnyObjectId objectId ) { PackList pList ; do { pList = packList . get ( ) ; for ( Pack p : pList . packs ) { try { if ( p . hasObject ( objectId ) ) { return p ; } } catch ( IOException e ) { LOG . warn ( MessageFormat . format ( JGitText . get ( ) . unableToReadPackfile , p . getPackFile ( ) . getAbsolutePath ( ) ) , e ) ; remove ( p ) ; } } } while ( searchPacksAgain ( pList ) ) ; return null ; } }","LOG . warn ( MessageFormat . format ( JGitText . get ( ) . unableToReadPackfile , p . getPackFile ( ) . getAbsolutePath ( ) ) , e ) ;",Meaningful
"public class A { protected boolean isValid ( AopMethodInvocation aopMethodInvocation , Object [ ] arguments , int phase ) { Method method = aopMethodInvocation . getMethod ( ) ; Class < ? > [ ] parameterTypes = method . getParameterTypes ( ) ; if ( parameterTypes . length == 0 ) { if ( _log . isDebugEnabled ( ) && ( phase == _PHASE_BEFORE ) ) { _log . debug ( ""The first parameter of "" + aopMethodInvocation + "" must be a first parameter"" ) ; } return false ; } Class < ? > parameterType = parameterTypes [ 0 ] ; if ( ! ClassedModel . class . isAssignableFrom ( parameterType ) ) { if ( _log . isDebugEnabled ( ) && ( phase == _PHASE_BEFORE ) ) { _log . debug ( ""The first parameter of "" + aopMethodInvocation + "" must implement ClassedModel"" ) ; } return false ; } ClassedModel classedModel = ( ClassedModel ) arguments [ 0 ] ; if ( ( classedModel == null ) || ! ( classedModel . getPrimaryKeyObj ( ) instanceof Long ) ) { if ( _log . isDebugEnabled ( ) && ( phase == _PHASE_BEFORE ) ) { _log . debug ( ""The first parameter of "" + aopMethodInvocation + "" must be a long"" ) ; } return false ; } if ( phase != _PHASE_AFTER_RETURNING ) { return true ; } if ( ! AuditedModel . class . isAssignableFrom ( parameterType ) && ! GroupedModel . class . isAssignableFrom ( parameterType ) && ! StagedModel . class . isAssignableFrom ( parameterType ) ) { if ( _log . isDebugEnabled ( ) ) { StringBundler sb = new StringBundler ( 4 ) ; sb . append ( ""If send is true, the first parameter of "" ) ; sb . append ( aopMethodInvocation ) ; sb . append ( "" must implement AuditedModel, GroupedModel, or "" ) ; sb . append ( ""StagedModel"" ) ; _log . debug ( sb . toString ( ) ) ; } return false ; } return true ; } }","public class A { protected boolean isValid ( AopMethodInvocation aopMethodInvocation , Object [ ] arguments , int phase ) { Method method = aopMethodInvocation . getMethod ( ) ; Class < ? > [ ] parameterTypes = method . getParameterTypes ( ) ; if ( parameterTypes . length == 0 ) { if ( _log . isDebugEnabled ( ) && ( phase == _PHASE_BEFORE ) ) { _log . debug ( ""The method "" + aopMethodInvocation + "" must have at least one parameter"" ) ; } return false ; } Class < ? > parameterType = parameterTypes [ 0 ] ; if ( ! ClassedModel . class . isAssignableFrom ( parameterType ) ) { if ( _log . isDebugEnabled ( ) && ( phase == _PHASE_BEFORE ) ) { _log . debug ( ""The first parameter of "" + aopMethodInvocation + "" must implement ClassedModel"" ) ; } return false ; } ClassedModel classedModel = ( ClassedModel ) arguments [ 0 ] ; if ( ( classedModel == null ) || ! ( classedModel . getPrimaryKeyObj ( ) instanceof Long ) ) { if ( _log . isDebugEnabled ( ) && ( phase == _PHASE_BEFORE ) ) { _log . debug ( ""The first parameter of "" + aopMethodInvocation + "" must be a long"" ) ; } return false ; } if ( phase != _PHASE_AFTER_RETURNING ) { return true ; } if ( ! AuditedModel . class . isAssignableFrom ( parameterType ) && ! GroupedModel . class . isAssignableFrom ( parameterType ) && ! StagedModel . class . isAssignableFrom ( parameterType ) ) { if ( _log . isDebugEnabled ( ) ) { StringBundler sb = new StringBundler ( 4 ) ; sb . append ( ""If send is true, the first parameter of "" ) ; sb . append ( aopMethodInvocation ) ; sb . append ( "" must implement AuditedModel, GroupedModel, or "" ) ; sb . append ( ""StagedModel"" ) ; _log . debug ( sb . toString ( ) ) ; } return false ; } return true ; } }","_log . debug ( ""The method "" + aopMethodInvocation + "" must have at least one parameter"" ) ;",Meaningful
"public class A { @ Override protected void decode ( ChannelHandlerContext chc , DatagramPacket msg , List < Object > list ) { ByteBuf bb = msg . content ( ) ; if ( bb . readableBytes ( ) < LENGTH_OF_HEADER ) { LOG . debug ( ""skipping header of msg: {} < {}"" , msg . readableBytes ( ) , LENGTH_OF_HEADER ) ; return ; } int length = bb . getUnsignedShort ( bb . readerIndex ( ) + LENGTH_INDEX_IN_HEADER ) ; if ( bb . readableBytes ( ) < length ) { LOG . debug ( ""skipping bb - too few data for msg: {} < {}"" , bb . readableBytes ( ) , length ) ; return ; } LOG . debug ( ""OF Protocol message received, type:{}"" , bb . getByte ( bb . readerIndex ( ) + 1 ) ) ; ByteBuf messageBuffer = bb . slice ( bb . readerIndex ( ) , length ) ; list . add ( messageBuffer ) ; messageBuffer . retain ( ) ; bb . skipBytes ( length ) ; } }","public class A { @ Override protected void decode ( ChannelHandlerContext chc , DatagramPacket msg , List < Object > list ) { ByteBuf bb = msg . content ( ) ; if ( bb . readableBytes ( ) < LENGTH_OF_HEADER ) { LOG . debug ( ""skipping bb - too few data for header: {}"" , bb . readableBytes ( ) ) ; return ; } int length = bb . getUnsignedShort ( bb . readerIndex ( ) + LENGTH_INDEX_IN_HEADER ) ; if ( bb . readableBytes ( ) < length ) { LOG . debug ( ""skipping bb - too few data for msg: {} < {}"" , bb . readableBytes ( ) , length ) ; return ; } LOG . debug ( ""OF Protocol message received, type:{}"" , bb . getByte ( bb . readerIndex ( ) + 1 ) ) ; ByteBuf messageBuffer = bb . slice ( bb . readerIndex ( ) , length ) ; list . add ( messageBuffer ) ; messageBuffer . retain ( ) ; bb . skipBytes ( length ) ; } }","LOG . debug ( ""skipping bb - too few data for header: {}"" , bb . readableBytes ( ) ) ;",Meaningful
"public class A { @ Override public void stop ( ) { try { for ( Transport transport : transports ) { if ( ! ( transport instanceof TcpTransport ) ) { continue ; } List < WriteFuture > writeFutures = new ArrayList < > ( ) ; List < IoSession > sessions ; try { sessions = new ArrayList < > ( getSocketAcceptor ( transport ) . getManagedSessions ( ) . values ( ) ) ; } catch ( IllegalArgumentException e ) { LOG . warn ( ""Invalid transport ID: {}. Discarding the LDAP service."" , transport ) ; return ; } getSocketAcceptor ( transport ) . dispose ( ) ; if ( LOG . isInfoEnabled ( ) ) { LOG . info ( ""Unbind of an LDAP service ({}) is complete."" , getPort ( ) ) ; LOG . info ( ""Sending notice of disconnect to existing clients sessions."" ) ; } if ( sessions != null ) { for ( IoSession session : sessions ) { writeFutures . add ( session . write ( NoticeOfDisconnect . UNAVAILABLE ) ) ; } } Iterator < IoSession > sessionIt = sessions . iterator ( ) ; for ( WriteFuture future : writeFutures ) { future . await ( 1000L ) ; sessionIt . next ( ) . closeNow ( ) ; } if ( replicationReqHandler != null ) { replicationReqHandler . stop ( ) ; } } stopConsumers ( ) ; } catch ( Exception e ) { LOG . warn ( ""Failed to sent NoD."" , e ) ; } started = false ; LOG . info ( ""Ldap service stopped."" ) ; } }","public class A { @ Override public void stop ( ) { try { for ( Transport transport : transports ) { if ( ! ( transport instanceof TcpTransport ) ) { continue ; } List < WriteFuture > writeFutures = new ArrayList < > ( ) ; List < IoSession > sessions ; try { sessions = new ArrayList < > ( getSocketAcceptor ( transport ) . getManagedSessions ( ) . values ( ) ) ; } catch ( IllegalArgumentException e ) { LOG . warn ( ""Seems like the LDAP service ({}) has already been unbound."" , getPort ( ) ) ; return ; } getSocketAcceptor ( transport ) . dispose ( ) ; if ( LOG . isInfoEnabled ( ) ) { LOG . info ( ""Unbind of an LDAP service ({}) is complete."" , getPort ( ) ) ; LOG . info ( ""Sending notice of disconnect to existing clients sessions."" ) ; } if ( sessions != null ) { for ( IoSession session : sessions ) { writeFutures . add ( session . write ( NoticeOfDisconnect . UNAVAILABLE ) ) ; } } Iterator < IoSession > sessionIt = sessions . iterator ( ) ; for ( WriteFuture future : writeFutures ) { future . await ( 1000L ) ; sessionIt . next ( ) . closeNow ( ) ; } if ( replicationReqHandler != null ) { replicationReqHandler . stop ( ) ; } } stopConsumers ( ) ; } catch ( Exception e ) { LOG . warn ( ""Failed to sent NoD."" , e ) ; } started = false ; LOG . info ( ""Ldap service stopped."" ) ; } }","LOG . warn ( ""Seems like the LDAP service ({}) has already been unbound."" , getPort ( ) ) ;",Meaningful
"public class A { private void showGenericGitErrorNotificationWithBranch ( BranchHelper branch ) { Platform . runLater ( ( ) -> { logger . warn ( ""Generic error notification"" ) ; notificationPaneController . addNotification ( String . format ( ""Sorry, there was a git error on branch %s."" , branch . getRefName ( ) ) ) ; } ) ; } }","public class A { private void showGenericGitErrorNotificationWithBranch ( BranchHelper branch ) { Platform . runLater ( ( ) -> { logger . warn ( ""Git error on branch notification"" ) ; notificationPaneController . addNotification ( String . format ( ""Sorry, there was a git error on branch %s."" , branch . getRefName ( ) ) ) ; } ) ; } }","logger . warn ( ""Git error on branch notification"" ) ;",Meaningful
"public class A { private static String transform ( Bundle bundle ) throws ModuleManagementException { try { PersistentBundle persistentBundle = ModuleUtils . persist ( bundle ) ; String newLocation = persistentBundle . getLocation ( ) ; logger . info ( ""RealDatasetLocation: "" + newLocation ) ; return newLocation ; } catch ( ModuleManagementException e ) { throw e ; } catch ( Exception e ) { String msg = ""Unable to transform bundle "" + bundle + "". Cause: "" + e . getMessage ( ) ; logger . error ( msg , e ) ; throw new ModuleManagementException ( msg , e ) ; } } }","public class A { private static String transform ( Bundle bundle ) throws ModuleManagementException { try { PersistentBundle persistentBundle = ModuleUtils . persist ( bundle ) ; String newLocation = persistentBundle . getLocation ( ) ; logger . info ( ""Transformed bundle {} with location {} to be handled by the DX protocol handler under new location {}"" , getDisplayName ( bundle ) , bundle . getLocation ( ) , newLocation ) ; return newLocation ; } catch ( ModuleManagementException e ) { throw e ; } catch ( Exception e ) { String msg = ""Unable to transform bundle "" + bundle + "". Cause: "" + e . getMessage ( ) ; logger . error ( msg , e ) ; throw new ModuleManagementException ( msg , e ) ; } } }","logger . info ( ""Transformed bundle {} with location {} to be handled by the DX protocol handler under new location {}"" , getDisplayName ( bundle ) , bundle . getLocation ( ) , newLocation ) ;",Meaningful
"public class A { @ Override public void ping ( TCredentials credentials ) { logger . info ( ""Not ping: {} for {}"" , credentials . getUserInfo ( ) , credentials ) ; } }","public class A { @ Override public void ping ( TCredentials credentials ) { log . info ( ""Manager reports: I just got pinged!"" ) ; } }","log . info ( ""Manager reports: I just got pinged!"" ) ;",Meaningful
"public class A { public static WeightingModel newInstance ( String name , Index index ) { final Logger logger = LoggerFactory . getLogger ( WeightingModelFactory . class ) ; WeightingModel model = null ; if ( name == null || name . length ( ) == 0 ) throw new IllegalArgumentException ( ""Weighting model must be defined and non-empty"" ) ; name = name . replaceFirst ( ""^([^\\.]+(\\(|$))"" , NAMESPACE + ""$1"" ) ; if ( ! cache . containsKey ( index ) ) { cache . put ( index , new HashMap < String , WeightingModel > ( ) ) ; } model = cache . get ( index ) . get ( name ) ; if ( model == null ) { try { if ( name . indexOf ( ""("" ) > 0 ) { String params = name . substring ( name . indexOf ( ""("" ) + 1 , name . indexOf ( "")"" ) ) ; String [ ] parameters = params . split ( ""\\s*,\\s*"" ) ; model = ApplicationSetup . getClass ( name . substring ( 0 , name . indexOf ( ""("" ) ) ) . asSubclass ( WeightingModel . class ) . getConstructor ( new Class [ ] { String [ ] . class } ) . newInstance ( new Object [ ] { parameters } ) ; } else { model = ApplicationSetup . getClass ( name ) . asSubclass ( WeightingModel . class ) . newInstance ( ) ; } } catch ( InvocationTargetException e ) { logger . error ( ""Recursive problem with weighting model named: "" + name , e ) ; } catch ( Exception e ) { logger . error ( ""Exception while creating weighting model named: "" + name , e ) ; } cache . get ( index ) . put ( name , model ) ; } return model ; } }","public class A { public static WeightingModel newInstance ( String name , Index index ) { final Logger logger = LoggerFactory . getLogger ( WeightingModelFactory . class ) ; WeightingModel model = null ; if ( name == null || name . length ( ) == 0 ) throw new IllegalArgumentException ( ""Weighting model must be defined and non-empty"" ) ; name = name . replaceFirst ( ""^([^\\.]+(\\(|$))"" , NAMESPACE + ""$1"" ) ; if ( ! cache . containsKey ( index ) ) { cache . put ( index , new HashMap < String , WeightingModel > ( ) ) ; } model = cache . get ( index ) . get ( name ) ; if ( model == null ) { try { if ( name . indexOf ( ""("" ) > 0 ) { String params = name . substring ( name . indexOf ( ""("" ) + 1 , name . indexOf ( "")"" ) ) ; String [ ] parameters = params . split ( ""\\s*,\\s*"" ) ; model = ApplicationSetup . getClass ( name . substring ( 0 , name . indexOf ( ""("" ) ) ) . asSubclass ( WeightingModel . class ) . getConstructor ( new Class [ ] { String [ ] . class } ) . newInstance ( new Object [ ] { parameters } ) ; } else { model = ApplicationSetup . getClass ( name ) . asSubclass ( WeightingModel . class ) . newInstance ( ) ; } } catch ( InvocationTargetException e ) { logger . error ( ""Recursive problem with weighting model named: "" + name , e ) ; } catch ( Exception e ) { logger . error ( ""Problem with weighting model named: "" + name , e ) ; } cache . get ( index ) . put ( name , model ) ; } return model ; } }","logger . error ( ""Problem with weighting model named: "" + name , e ) ;",Meaningful
"public class A { private Exchange addSpecificContent ( final Exchange exchange , final TImplementationArtifact implementationArtifact ) { final Object any = implementationArtifact . getAny ( ) ; final Document specificContent = any instanceof Element ? XMLHelper . fromRootNode ( ( Element ) any ) : null ; if ( specificContent != null ) { LOG . debug ( ""Adding CDI hex artifact {} to the parent of file (no Kieker log): {}"" , implementationArtifact . getName ( ) , implementationArtifact . getId ( ) ) ; exchange . getIn ( ) . setHeader ( MBHeader . SPECIFICCONTENT_DOCUMENT . toString ( ) , specificContent ) ; } return exchange ; } }","public class A { private Exchange addSpecificContent ( final Exchange exchange , final TImplementationArtifact implementationArtifact ) { final Object any = implementationArtifact . getAny ( ) ; final Document specificContent = any instanceof Element ? XMLHelper . fromRootNode ( ( Element ) any ) : null ; if ( specificContent != null ) { LOG . debug ( ""ArtifactSpecificContent specified!"" ) ; exchange . getIn ( ) . setHeader ( MBHeader . SPECIFICCONTENT_DOCUMENT . toString ( ) , specificContent ) ; } return exchange ; } }","LOG . debug ( ""ArtifactSpecificContent specified!"" ) ;",Meaningful
"public class A { public boolean removeAcls ( scala . collection . immutable . Set < Acl > acls , final Resource resource ) { verifyAcls ( acls ) ; LOG . info ( ""Removing ACL: {}"" , acls ) ; final Iterator < Acl > iterator = acls . iterator ( ) ; while ( iterator . hasNext ( ) ) { final Acl acl = iterator . next ( ) ; final String role = getRole ( acl ) ; try { execute ( new Command < Void > ( ) { @ Override public Void run ( SentryGenericServiceClient client ) throws Exception { client . dropPrivilege ( requestorName , role , toTSentryPrivilege ( acl , resource ) ) ; return null ; } } ) ; } catch ( KafkaException kex ) { LOG . error ( ""Failed to remove acls."" , kex ) ; return false ; } } return true ; } }","public class A { public boolean removeAcls ( scala . collection . immutable . Set < Acl > acls , final Resource resource ) { verifyAcls ( acls ) ; LOG . info ( ""Removing Acl: acl->"" + acls + "" resource->"" + resource ) ; final Iterator < Acl > iterator = acls . iterator ( ) ; while ( iterator . hasNext ( ) ) { final Acl acl = iterator . next ( ) ; final String role = getRole ( acl ) ; try { execute ( new Command < Void > ( ) { @ Override public Void run ( SentryGenericServiceClient client ) throws Exception { client . dropPrivilege ( requestorName , role , toTSentryPrivilege ( acl , resource ) ) ; return null ; } } ) ; } catch ( KafkaException kex ) { LOG . error ( ""Failed to remove acls."" , kex ) ; return false ; } } return true ; } }","LOG . info ( ""Removing Acl: acl->"" + acls + "" resource->"" + resource ) ;",Meaningful
"public class A { @ DB @ Override public void removeDhcpServiceInSubnet ( final Nic nic ) { final Network network = _networksDao . findById ( nic . getNetworkId ( ) ) ; final DhcpServiceProvider dhcpServiceProvider = getDhcpServiceProvider ( network ) ; try { final NicIpAliasVO ipAlias = _nicIpAliasDao . findByGatewayAndNetworkIdAndState ( nic . getIPv4Gateway ( ) , network . getId ( ) , NicIpAlias . State . active ) ; if ( ipAlias != null ) { ipAlias . setState ( NicIpAlias . State . revoked ) ; Transaction . execute ( new TransactionCallbackNoReturn ( ) { @ Override public void doInTransactionWithoutResult ( final TransactionStatus status ) { _nicIpAliasDao . update ( ipAlias . getId ( ) , ipAlias ) ; final IPAddressVO aliasIpaddressVo = _publicIpAddressDao . findByIpAndSourceNetworkId ( ipAlias . getNetworkId ( ) , ipAlias . getIp4Address ( ) ) ; _publicIpAddressDao . unassignIpAddress ( aliasIpaddressVo . getId ( ) ) ; } } ) ; if ( ! dhcpServiceProvider . removeDhcpSupportForSubnet ( network ) ) { s_logger . warn ( ""Unable to remove the ip alias: "" + ipAlias . toString ( ) ) ; } } } catch ( final ResourceUnavailableException e ) { s_logger . info ( ""Unable to delete the ip alias due to unable to contact the virtualrouter."" ) ; } } }","public class A { @ DB @ Override public void removeDhcpServiceInSubnet ( final Nic nic ) { final Network network = _networksDao . findById ( nic . getNetworkId ( ) ) ; final DhcpServiceProvider dhcpServiceProvider = getDhcpServiceProvider ( network ) ; try { final NicIpAliasVO ipAlias = _nicIpAliasDao . findByGatewayAndNetworkIdAndState ( nic . getIPv4Gateway ( ) , network . getId ( ) , NicIpAlias . State . active ) ; if ( ipAlias != null ) { ipAlias . setState ( NicIpAlias . State . revoked ) ; Transaction . execute ( new TransactionCallbackNoReturn ( ) { @ Override public void doInTransactionWithoutResult ( final TransactionStatus status ) { _nicIpAliasDao . update ( ipAlias . getId ( ) , ipAlias ) ; final IPAddressVO aliasIpaddressVo = _publicIpAddressDao . findByIpAndSourceNetworkId ( ipAlias . getNetworkId ( ) , ipAlias . getIp4Address ( ) ) ; _publicIpAddressDao . unassignIpAddress ( aliasIpaddressVo . getId ( ) ) ; } } ) ; if ( ! dhcpServiceProvider . removeDhcpSupportForSubnet ( network ) ) { s_logger . warn ( ""Failed to remove the ip alias on the router, marking it as removed in db and freed the allocated ip "" + ipAlias . getIp4Address ( ) ) ; } } } catch ( final ResourceUnavailableException e ) { s_logger . info ( ""Unable to delete the ip alias due to unable to contact the virtualrouter."" ) ; } } }","s_logger . warn ( ""Failed to remove the ip alias on the router, marking it as removed in db and freed the allocated ip "" + ipAlias . getIp4Address ( ) ) ;",Meaningful
"public class A { public void testDatasourceUpdate ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( ""entity -submit -type cluster -file "" + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( ""entity -submit -type cluster -file "" + filePath ) , 0 ) ; String dsName = ""datasource-test-1"" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE1 , overlay ) ; LOG . info ( ""Submit datatsource entity {} via entity -submit -type datasource -file {}"" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( ""entity -submit -type datasource -file "" + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( ""Submit feed with datasource {} via entity -submitAndSchedule -type feed -file {}"" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( ""entity -submitAndSchedule -type feed -file "" + filePath ) ) ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE5 , overlay ) ; LOG . info ( ""entity -update -type datasource -file {}"" , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( ""entity -update -type datasource -file "" + filePath ) ) ; } }","public class A { public void testDatasourceUpdate ( ) throws Exception { TestContext context = new TestContext ( ) ; Map < String , String > overlay = context . getUniqueOverlay ( ) ; String filePath = TestContext . overlayParametersOverTemplate ( TestContext . CLUSTER_TEMPLATE , overlay ) ; context . setCluster ( filePath ) ; LOG . info ( ""entity -submit -type cluster -file "" + filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( ""entity -submit -type cluster -file "" + filePath ) , 0 ) ; String dsName = ""datasource-test-1"" ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE1 , overlay ) ; LOG . info ( ""Submit datatsource entity {} via entity -submit -type datasource -file {}"" , dsName , filePath ) ; Assert . assertEquals ( TestContext . executeWithURL ( ""entity -submit -type datasource -file "" + filePath ) , 0 ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . FEED_TEMPLATE3 , overlay ) ; LOG . info ( ""Submit feed with datasource {} via entity -submitAndSchedule -type feed -file {}"" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( ""entity -submitAndSchedule -type feed -file "" + filePath ) ) ; overlay . put ( DATASOURCE_NAME_KEY , dsName ) ; filePath = TestContext . overlayParametersOverTemplate ( TestContext . DATASOURCE_TEMPLATE5 , overlay ) ; LOG . info ( ""update datasource {} via -update -type datasource -file {}"" , dsName , filePath ) ; Assert . assertEquals ( 0 , TestContext . executeWithURL ( ""entity -update -type datasource -file "" + filePath ) ) ; } }","LOG . info ( ""update datasource {} via -update -type datasource -file {}"" , dsName , filePath ) ;",Meaningful
"public class A { @ Override @ SuppressWarnings ( ""unchecked"" ) public ResidueSolvablePolynomial < C > parse ( Reader r ) { GenPolynomialTokenizer pt = new GenPolynomialTokenizer ( this , r ) ; ResidueSolvablePolynomial < C > p = null ; try { GenSolvablePolynomial < SolvableResidue < C > > s = pt . nextSolvablePolynomial ( ) ; p = new ResidueSolvablePolynomial < C > ( this , s ) ; } catch ( IOException e ) { logger . error ( ""result not found, exiting"" , e ) ; p = ZERO ; } return p ; } }","public class A { @ Override @ SuppressWarnings ( ""unchecked"" ) public ResidueSolvablePolynomial < C > parse ( Reader r ) { GenPolynomialTokenizer pt = new GenPolynomialTokenizer ( this , r ) ; ResidueSolvablePolynomial < C > p = null ; try { GenSolvablePolynomial < SolvableResidue < C > > s = pt . nextSolvablePolynomial ( ) ; p = new ResidueSolvablePolynomial < C > ( this , s ) ; } catch ( IOException e ) { logger . error ( e . toString ( ) + "" parse "" + this ) ; p = ZERO ; } return p ; } }","logger . error ( e . toString ( ) + "" parse "" + this ) ;",Meaningful
"public class A { private void processImportedMappings ( ) { for ( final Entry < String , String > entry : ImmutableSet . copyOf ( importMapping . entrySet ( ) ) ) { final String model = entry . getKey ( ) ; final String [ ] namespaceInfo = entry . getValue ( ) . split ( ""\\s"" ) ; final String [ ] namespace = ( namespaceInfo . length > 0 ? namespaceInfo [ 0 ] . trim ( ) : """" ) . split ( "":"" ) ; final String namespaceName = namespace . length > 0 ? namespace [ 0 ] . trim ( ) : null ; final String modelClass = namespace . length > 1 ? namespace [ 1 ] . trim ( ) : null ; final String assembly = namespaceInfo . length > 1 ? namespaceInfo [ 1 ] . trim ( ) : null ; final String assemblyVersion = namespaceInfo . length > 2 ? namespaceInfo [ 2 ] . trim ( ) : null ; final String assemblyFramework = namespaceInfo . length > 3 ? namespaceInfo [ 3 ] . trim ( ) : ""net45"" ; if ( isNullOrEmpty ( model ) || isNullOrEmpty ( namespaceName ) ) { LOGGER . warn ( String . format ( Locale . ROOT , ""Could not import: '%s' - invalid namespace: '%s'"" , model , entry . getValue ( ) ) ) ; importMapping . remove ( model ) ; } else { LOGGER . info ( String . format ( Locale . ROOT , ""Could not import: '%s' - valid namespace: '%s'"" , model , namespaceName ) ) ; importMapping . put ( model , namespaceName ) ; } if ( ! isNullOrEmpty ( modelClass ) ) { LOGGER . info ( String . format ( Locale . ROOT , ""Mapping: '%s' class to '%s'"" , model , modelClass ) ) ; modelNameMapping . put ( model , modelClass ) ; } if ( assembly != null && assemblyVersion != null ) { LOGGER . info ( String . format ( Locale . ROOT , ""Adding dependency: '%s', version: '%s', framework: '%s'"" , assembly , assemblyVersion , assemblyVersion ) ) ; dependencies . put ( assembly , new DependencyInfo ( assemblyVersion , assemblyFramework ) ) ; } } } }","public class A { private void processImportedMappings ( ) { for ( final Entry < String , String > entry : ImmutableSet . copyOf ( importMapping . entrySet ( ) ) ) { final String model = entry . getKey ( ) ; final String [ ] namespaceInfo = entry . getValue ( ) . split ( ""\\s"" ) ; final String [ ] namespace = ( namespaceInfo . length > 0 ? namespaceInfo [ 0 ] . trim ( ) : """" ) . split ( "":"" ) ; final String namespaceName = namespace . length > 0 ? namespace [ 0 ] . trim ( ) : null ; final String modelClass = namespace . length > 1 ? namespace [ 1 ] . trim ( ) : null ; final String assembly = namespaceInfo . length > 1 ? namespaceInfo [ 1 ] . trim ( ) : null ; final String assemblyVersion = namespaceInfo . length > 2 ? namespaceInfo [ 2 ] . trim ( ) : null ; final String assemblyFramework = namespaceInfo . length > 3 ? namespaceInfo [ 3 ] . trim ( ) : ""net45"" ; if ( isNullOrEmpty ( model ) || isNullOrEmpty ( namespaceName ) ) { LOGGER . warn ( String . format ( Locale . ROOT , ""Could not import: '%s' - invalid namespace: '%s'"" , model , entry . getValue ( ) ) ) ; importMapping . remove ( model ) ; } else { LOGGER . info ( String . format ( Locale . ROOT , ""Importing: '%s' from '%s' namespace."" , model , namespaceName ) ) ; importMapping . put ( model , namespaceName ) ; } if ( ! isNullOrEmpty ( modelClass ) ) { LOGGER . info ( String . format ( Locale . ROOT , ""Mapping: '%s' class to '%s'"" , model , modelClass ) ) ; modelNameMapping . put ( model , modelClass ) ; } if ( assembly != null && assemblyVersion != null ) { LOGGER . info ( String . format ( Locale . ROOT , ""Adding dependency: '%s', version: '%s', framework: '%s'"" , assembly , assemblyVersion , assemblyVersion ) ) ; dependencies . put ( assembly , new DependencyInfo ( assemblyVersion , assemblyFramework ) ) ; } } } }","LOGGER . info ( String . format ( Locale . ROOT , ""Importing: '%s' from '%s' namespace."" , model , namespaceName ) ) ;",Meaningful
"public class A { protected final Set < VEvent > convertCalendarToEvents ( net . fortuna . ical4j . model . Calendar calendar , Interval interval ) throws CalendarException { Period period = new Period ( new net . fortuna . ical4j . model . DateTime ( interval . getStartMillis ( ) ) , new net . fortuna . ical4j . model . DateTime ( interval . getEndMillis ( ) ) ) ; Set < VEvent > events = new HashSet < VEvent > ( ) ; if ( calendar == null ) { log . info ( ""calendar empty, returning empty set"" ) ; return Collections . emptySet ( ) ; } for ( Iterator < Component > i = calendar . getComponents ( ) . iterator ( ) ; i . hasNext ( ) ; ) { Component component = i . next ( ) ; if ( component . getName ( ) . equals ( ""VEVENT"" ) ) { VEvent event = ( VEvent ) component ; if ( log . isTraceEnabled ( ) ) { log . trace ( ""processing event "" + event . getSummary ( ) . getValue ( ) ) ; } PeriodList periods = event . calculateRecurrenceSet ( period ) ; for ( Iterator < Period > iter = periods . iterator ( ) ; iter . hasNext ( ) ; ) { Period eventper = iter . next ( ) ; PropertyList props = event . getProperties ( ) ; PropertyList newprops = new PropertyList ( ) ; newprops . add ( new DtStart ( eventper . getStart ( ) ) ) ; newprops . add ( new DtEnd ( eventper . getEnd ( ) ) ) ; for ( Iterator < Property > iter2 = props . iterator ( ) ; iter2 . hasNext ( ) ; ) { Property prop = iter2 . next ( ) ; if ( ! ( prop instanceof DtStart ) && ! ( prop instanceof DtEnd ) && ! ( prop instanceof Duration ) && ! ( prop instanceof RRule ) ) newprops . add ( prop ) ; } VEvent newevent = new VEvent ( newprops ) ; events . add ( newevent ) ; if ( log . isTraceEnabled ( ) ) { log . trace ( ""end "" + event . getSummary ( ) . getValue ( ) + "" added event "" + newevent . toString ( ) ) ; } } } } return events ; } }","public class A { protected final Set < VEvent > convertCalendarToEvents ( net . fortuna . ical4j . model . Calendar calendar , Interval interval ) throws CalendarException { Period period = new Period ( new net . fortuna . ical4j . model . DateTime ( interval . getStartMillis ( ) ) , new net . fortuna . ical4j . model . DateTime ( interval . getEndMillis ( ) ) ) ; Set < VEvent > events = new HashSet < VEvent > ( ) ; if ( calendar == null ) { log . info ( ""calendar empty, returning empty set"" ) ; return Collections . emptySet ( ) ; } for ( Iterator < Component > i = calendar . getComponents ( ) . iterator ( ) ; i . hasNext ( ) ; ) { Component component = i . next ( ) ; if ( component . getName ( ) . equals ( ""VEVENT"" ) ) { VEvent event = ( VEvent ) component ; if ( log . isTraceEnabled ( ) ) { log . trace ( ""processing event "" + event . getSummary ( ) . getValue ( ) ) ; } PeriodList periods = event . calculateRecurrenceSet ( period ) ; for ( Iterator < Period > iter = periods . iterator ( ) ; iter . hasNext ( ) ; ) { Period eventper = iter . next ( ) ; PropertyList props = event . getProperties ( ) ; PropertyList newprops = new PropertyList ( ) ; newprops . add ( new DtStart ( eventper . getStart ( ) ) ) ; newprops . add ( new DtEnd ( eventper . getEnd ( ) ) ) ; for ( Iterator < Property > iter2 = props . iterator ( ) ; iter2 . hasNext ( ) ; ) { Property prop = iter2 . next ( ) ; if ( ! ( prop instanceof DtStart ) && ! ( prop instanceof DtEnd ) && ! ( prop instanceof Duration ) && ! ( prop instanceof RRule ) ) newprops . add ( prop ) ; } VEvent newevent = new VEvent ( newprops ) ; events . add ( newevent ) ; if ( log . isTraceEnabled ( ) ) { log . trace ( ""added event "" + newevent ) ; } } } } return events ; } }","log . trace ( ""added event "" + newevent ) ;",Meaningful
"public class A { public static void load ( ) { String fileName = System . getProperty ( ""test.properties"" ) ; if ( fileName == null ) { LOG . info ( ""Test properties was not provided, therefore not loading any test properties"" ) ; return ; } try ( InputStream stream = new FileInputStream ( fileName ) ) { properties . load ( stream ) ; System . getProperties ( ) . putAll ( properties ) ; } catch ( FileNotFoundException e ) { LOG . error ( ""The given test properties file does not exist"" , e ) ; fail ( ""The given test properties file does not exist"" ) ; } catch ( IOException e ) { LOG . error ( ""I/O error reading the test properties at {}: {}"" , fileName , e . getMessage ( ) , e ) ; fail ( ""Unable to read the test properties file"" ) ; } } }","public class A { public static void load ( ) { String fileName = System . getProperty ( ""test.properties"" ) ; if ( fileName == null ) { LOG . info ( ""Test properties was not provided, therefore not loading any test properties"" ) ; return ; } try ( InputStream stream = new FileInputStream ( fileName ) ) { properties . load ( stream ) ; System . getProperties ( ) . putAll ( properties ) ; } catch ( FileNotFoundException e ) { LOG . error ( ""Test properties provided at {} does not exist, therefore aborting the test execution"" , fileName ) ; fail ( ""The given test properties file does not exist"" ) ; } catch ( IOException e ) { LOG . error ( ""I/O error reading the test properties at {}: {}"" , fileName , e . getMessage ( ) , e ) ; fail ( ""Unable to read the test properties file"" ) ; } } }","LOG . error ( ""Test properties provided at {} does not exist, therefore aborting the test execution"" , fileName ) ;",Meaningful
"public class A { private void schedule ( final IndexSet indexSet ) { final String indexSetId = indexSet . getConfig ( ) . id ( ) ; final String indexSetTitle = indexSet . getConfig ( ) . title ( ) ; final Duration refreshInterval = indexSet . getConfig ( ) . fieldTypeRefreshInterval ( ) ; if ( Duration . ZERO . equals ( refreshInterval ) ) { LOG . debug ( ""Skipping index set with ZERO refresh interval <{}/{}>"" , indexSetTitle , indexSetId ) ; return ; } if ( ! indexSet . getConfig ( ) . isWritable ( ) ) { LOG . debug ( ""Skipping non-writable index set <{}/{}>"" , indexSetTitle , indexSetId ) ; return ; } cancel ( futures . get ( indexSetId ) ) ; LOG . debug ( ""Schedule index field type updating for index set <{}/{}> every {} ms"" , indexSetId , indexSetTitle , refreshInterval . getMillis ( ) ) ; final ScheduledFuture < ? > future = scheduler . scheduleAtFixedRate ( ( ) -> { if ( serverIsNotRunning ( ) ) { return ; } try { final String activeWriteIndex = indexSet . getActiveWriteIndex ( ) ; if ( activeWriteIndex != null ) { LOG . debug ( ""Updating index field types for active write index <{}> in index set <{}/{}>"" , activeWriteIndex , indexSetTitle , indexSetId ) ; poller . pollIndex ( activeWriteIndex , indexSetId ) . ifPresent ( dbService :: upsert ) ; } else { LOG . warn ( ""Active write index for index set \""{}\"" ({}) doesn't exist yet"" , indexSetTitle , indexSetId ) ; } } catch ( TooManyAliasesException e ) { LOG . error ( ""Couldn't get active write index"" , e ) ; } catch ( Exception e ) { LOG . error ( ""Unable to update index field types"" , e ) ; } } , 0 , refreshInterval . getMillis ( ) , TimeUnit . MILLISECONDS ) ; futures . put ( indexSetId , future ) ; } }","public class A { private void schedule ( final IndexSet indexSet ) { final String indexSetId = indexSet . getConfig ( ) . id ( ) ; final String indexSetTitle = indexSet . getConfig ( ) . title ( ) ; final Duration refreshInterval = indexSet . getConfig ( ) . fieldTypeRefreshInterval ( ) ; if ( Duration . ZERO . equals ( refreshInterval ) ) { LOG . debug ( ""Skipping index set with ZERO refresh interval <{}/{}>"" , indexSetTitle , indexSetId ) ; return ; } if ( ! indexSet . getConfig ( ) . isWritable ( ) ) { LOG . debug ( ""Skipping non-writable index set <{}/{}>"" , indexSetTitle , indexSetId ) ; return ; } cancel ( futures . get ( indexSetId ) ) ; LOG . debug ( ""Schedule index field type updating for index set <{}/{}> every {} ms"" , indexSetId , indexSetTitle , refreshInterval . getMillis ( ) ) ; final ScheduledFuture < ? > future = scheduler . scheduleAtFixedRate ( ( ) -> { if ( serverIsNotRunning ( ) ) { return ; } try { final String activeWriteIndex = indexSet . getActiveWriteIndex ( ) ; if ( activeWriteIndex != null ) { LOG . debug ( ""Updating index field types for active write index <{}> in index set <{}/{}>"" , activeWriteIndex , indexSetTitle , indexSetId ) ; poller . pollIndex ( activeWriteIndex , indexSetId ) . ifPresent ( dbService :: upsert ) ; } else { LOG . warn ( ""Active write index for index set \""{}\"" ({}) doesn't exist yet"" , indexSetTitle , indexSetId ) ; } } catch ( TooManyAliasesException e ) { LOG . error ( ""Couldn't get active write index"" , e ) ; } catch ( Exception e ) { LOG . error ( ""Couldn't update field types for index set <{}/{}>"" , indexSetTitle , indexSetId , e ) ; } } , 0 , refreshInterval . getMillis ( ) , TimeUnit . MILLISECONDS ) ; futures . put ( indexSetId , future ) ; } }","LOG . error ( ""Couldn't update field types for index set <{}/{}>"" , indexSetTitle , indexSetId , e ) ;",Meaningful
"public class A { public void service ( HttpServletRequest req , HttpServletResponse res ) throws Exception { boolean debug = log . isDebugEnabled ( ) ; try { req . setCharacterEncoding ( ""UTF-8"" ) ; } catch ( UnsupportedEncodingException e ) { log . error ( ""Encoding not supported"" , e ) ; } String portalPath = req . getRequestURI ( ) . substring ( req . getContextPath ( ) . length ( ) ) ; Router router = routerRef . get ( ) ; if ( router != null ) { Iterator < Map < QualifiedName , String > > matcher = router . matcher ( portalPath , req . getParameterMap ( ) ) ; boolean started = false ; boolean processed = false ; try { while ( matcher . hasNext ( ) && ! processed ) { Map < QualifiedName , String > parameters = matcher . next ( ) ; String handlerKey = parameters . get ( HANDLER_PARAM ) ; if ( handlerKey != null ) { WebRequestHandler handler = handlers . get ( handlerKey ) ; if ( handler != null ) { if ( debug ) { log . debug ( ""Serving request path="" + portalPath + "", parameters="" + parameters + "" with handler "" + handler ) ; } if ( ! started && handler . getRequiresLifeCycle ( ) ) { if ( debug ) { log . debug ( ""Starting RequestLifeCycle for handler "" + handler ) ; } RequestLifeCycle . begin ( ExoContainerContext . getCurrentContainer ( ) ) ; started = true ; } processed = handler . execute ( new ControllerContext ( this , router , req , res , parameters ) ) ; } else { if ( debug ) { log . debug ( ""No handler "" + handlerKey + "" for request path="" + portalPath + "", parameters="" + parameters ) ; } } } } } finally { if ( started ) { if ( debug ) { log . debug ( ""Ending request"" ) ; } RequestLifeCycle . end ( ) ; } } if ( ! processed ) { log . error ( ""Could not associate the request path="" + portalPath + "" with an handler"" ) ; res . sendError ( HttpServletResponse . SC_NOT_FOUND ) ; } } else { log . error ( ""Missing valid router configuration "" + configurationPathRef . get ( ) ) ; res . sendError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR ) ; } } }","public class A { public void service ( HttpServletRequest req , HttpServletResponse res ) throws Exception { boolean debug = log . isDebugEnabled ( ) ; try { req . setCharacterEncoding ( ""UTF-8"" ) ; } catch ( UnsupportedEncodingException e ) { log . error ( ""Encoding not supported"" , e ) ; } String portalPath = req . getRequestURI ( ) . substring ( req . getContextPath ( ) . length ( ) ) ; Router router = routerRef . get ( ) ; if ( router != null ) { Iterator < Map < QualifiedName , String > > matcher = router . matcher ( portalPath , req . getParameterMap ( ) ) ; boolean started = false ; boolean processed = false ; try { while ( matcher . hasNext ( ) && ! processed ) { Map < QualifiedName , String > parameters = matcher . next ( ) ; String handlerKey = parameters . get ( HANDLER_PARAM ) ; if ( handlerKey != null ) { WebRequestHandler handler = handlers . get ( handlerKey ) ; if ( handler != null ) { if ( debug ) { log . debug ( ""Serving request path="" + portalPath + "", parameters="" + parameters + "" with handler "" + handler ) ; } if ( ! started && handler . getRequiresLifeCycle ( ) ) { if ( debug ) { log . debug ( ""Starting RequestLifeCycle for handler "" + handler ) ; } RequestLifeCycle . begin ( ExoContainerContext . getCurrentContainer ( ) ) ; started = true ; } processed = handler . execute ( new ControllerContext ( this , router , req , res , parameters ) ) ; } else { if ( debug ) { log . debug ( ""No handler "" + handlerKey + "" for request path="" + portalPath + "", parameters="" + parameters ) ; } } } } } finally { if ( started ) { if ( debug ) { log . debug ( ""Finishing RequestLifeCycle for current request"" ) ; } RequestLifeCycle . end ( ) ; } } if ( ! processed ) { log . error ( ""Could not associate the request path="" + portalPath + "" with an handler"" ) ; res . sendError ( HttpServletResponse . SC_NOT_FOUND ) ; } } else { log . error ( ""Missing valid router configuration "" + configurationPathRef . get ( ) ) ; res . sendError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR ) ; } } }","log . debug ( ""Finishing RequestLifeCycle for current request"" ) ;",Meaningful
"public class A { protected void rollbackTransaction ( ) { closePreviousStatement ( ) ; if ( ! _txn ) { if ( s_logger . isTraceEnabled ( ) ) { s_logger . trace ( ""Rollback called for "" + _name + "" when there's no transaction: "" + buildName ( ) ) ; } return ; } assert ( ! hasTxnInStack ( ) ) : ""Who's rolling back transaction when there's still txn in stack?"" ; _txn = false ; try { if ( _conn != null ) { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( ""Purging connection "" + _name ) ; } _conn . rollback ( ) ; } clearLockTimes ( ) ; closeConnection ( ) ; } catch ( final SQLException e ) { s_logger . warn ( ""Unable to rollback"" , e ) ; } } }","public class A { protected void rollbackTransaction ( ) { closePreviousStatement ( ) ; if ( ! _txn ) { if ( s_logger . isTraceEnabled ( ) ) { s_logger . trace ( ""Rollback called for "" + _name + "" when there's no transaction: "" + buildName ( ) ) ; } return ; } assert ( ! hasTxnInStack ( ) ) : ""Who's rolling back transaction when there's still txn in stack?"" ; _txn = false ; try { if ( _conn != null ) { if ( s_logger . isDebugEnabled ( ) ) { s_logger . debug ( ""Rolling back the transaction: Time = "" + ( System . currentTimeMillis ( ) - _txnTime ) + "" Name = "" + _name + ""; called by "" + buildName ( ) ) ; } _conn . rollback ( ) ; } clearLockTimes ( ) ; closeConnection ( ) ; } catch ( final SQLException e ) { s_logger . warn ( ""Unable to rollback"" , e ) ; } } }","s_logger . debug ( ""Rolling back the transaction: Time = "" + ( System . currentTimeMillis ( ) - _txnTime ) + "" Name = "" + _name + ""; called by "" + buildName ( ) ) ;",Meaningful
"public class A { @ Override public void run ( String ... args ) { securityUtil . logInAs ( ""system"" ) ; Page < ProcessDefinition > processDefinitionPage = processRuntime . processDefinitions ( Pageable . of ( 0 , 10 ) ) ; logger . info ( ""\t> Process definitionPage: "" + processDefinitionPage . getContent ( ) . size ( ) ) ; for ( ProcessDefinition pd : processDefinitionPage . getContent ( ) ) { logger . info ( ""\t > Process definition: "" + pd ) ; } } }","public class A { @ Override public void run ( String ... args ) { securityUtil . logInAs ( ""system"" ) ; Page < ProcessDefinition > processDefinitionPage = processRuntime . processDefinitions ( Pageable . of ( 0 , 10 ) ) ; logger . info ( ""> Available Process definitions: "" + processDefinitionPage . getTotalItems ( ) ) ; for ( ProcessDefinition pd : processDefinitionPage . getContent ( ) ) { logger . info ( ""\t > Process definition: "" + pd ) ; } } }","logger . info ( ""> Available Process definitions: "" + processDefinitionPage . getTotalItems ( ) ) ;",Meaningful
"public class A { public boolean copy ( String src , String dest ) { try { URI srcUri = new URI ( src ) , destUri = new URI ( dest ) ; Configuration config = new Configuration ( ) ; config . set ( ""fs.default.name"" , srcUri . resolve ( ""/"" ) . toString ( ) ) ; FileSystem dfs = FileSystem . get ( config ) ; Path destPath = new Path ( destUri . toString ( ) ) ; FileStatus [ ] files = dfs . listStatus ( new Path ( srcUri . toString ( ) ) ) ; if ( files == null || files . length == 0 ) return false ; for ( FileStatus f : files ) { log . info ( ""Copying "" + f . getPath ( ) . toString ( ) ) ; dfs . copyToLocalFile ( f . getPath ( ) , destPath ) ; } return true ; } catch ( Exception e ) { log . error ( ""Failed to copy data file"" , e ) ; return false ; } } }","public class A { public boolean copy ( String src , String dest ) { try { URI srcUri = new URI ( src ) , destUri = new URI ( dest ) ; Configuration config = new Configuration ( ) ; config . set ( ""fs.default.name"" , srcUri . resolve ( ""/"" ) . toString ( ) ) ; FileSystem dfs = FileSystem . get ( config ) ; Path destPath = new Path ( destUri . toString ( ) ) ; FileStatus [ ] files = dfs . listStatus ( new Path ( srcUri . toString ( ) ) ) ; if ( files == null || files . length == 0 ) return false ; for ( FileStatus f : files ) { log . info ( ""Copying "" + f . getPath ( ) . toString ( ) ) ; dfs . copyToLocalFile ( f . getPath ( ) , destPath ) ; } return true ; } catch ( Exception e ) { log . error ( e . getMessage ( ) , e ) ; return false ; } } }","log . error ( e . getMessage ( ) , e ) ;",Meaningful
"public class A { public void shutdown ( final long txnId , final boolean checkpoint ) { if ( ! initialised ) { return ; } if ( currentBuffer == null ) { return ; } if ( ! BrokerPool . FORCE_CORRUPTION ) { if ( checkpoint ) { LOG . info ( ""Shutting down Journal with checkpoint..."" ) ; try { writeToLog ( new Checkpoint ( txnId ) ) ; } catch ( final JournalException e ) { LOG . error ( ""Unable to write checkpoint: {}"" , e . getMessage ( ) , e ) ; } } flushBuffer ( ) ; } try { channel . close ( ) ; } catch ( final IOException e ) { LOG . error ( ""Unable to close Journal file: {}"" , e . getMessage ( ) , e ) ; } channel = null ; fileLock . release ( ) ; currentBuffer = null ; } }","public class A { public void shutdown ( final long txnId , final boolean checkpoint ) { if ( ! initialised ) { return ; } if ( currentBuffer == null ) { return ; } if ( ! BrokerPool . FORCE_CORRUPTION ) { if ( checkpoint ) { LOG . info ( ""Shutting down Journal with checkpoint..."" ) ; try { writeToLog ( new Checkpoint ( txnId ) ) ; } catch ( final JournalException e ) { LOG . error ( ""An error occurred whilst writing a checkpoint to the Journal: {}"" , e . getMessage ( ) , e ) ; } } flushBuffer ( ) ; } try { channel . close ( ) ; } catch ( final IOException e ) { LOG . error ( ""Unable to close Journal file: {}"" , e . getMessage ( ) , e ) ; } channel = null ; fileLock . release ( ) ; currentBuffer = null ; } }","LOG . error ( ""An error occurred whilst writing a checkpoint to the Journal: {}"" , e . getMessage ( ) , e ) ;",Meaningful
"public class A { void securityMessageReceived ( NextFilter nextFilter , IoSession session , Object message ) throws Exception { final boolean loggerIsEnabled = logger != null && logger . isTraceEnabled ( ) ; if ( ! httpRequestMessageReceived ( nextFilter , session , message ) ) return ; HttpRequestMessage httpRequest = ( HttpRequestMessage ) message ; ResourceAddress httpAddress = httpRequest . getLocalAddress ( ) ; if ( alreadyLoggedIn ( httpRequest ) ) { if ( httpRequest . getLoginContext ( ) == null ) { setUnprotectedLoginContext ( httpRequest ) ; } if ( loggerIsEnabled ) { logger . trace ( ""HttpSecurityStandardFilter skipped because the realm is configured."" ) ; } super . doMessageReceived ( nextFilter , session , message ) ; return ; } HttpRealmInfo [ ] realms = httpAddress . getOption ( HttpResourceAddress . REALMS ) ; if ( realms . length == 0 ) { setUnprotectedLoginContext ( httpRequest ) ; if ( loggerIsEnabled ) { logger . trace ( ""HttpSecurityStandardFilter skipped because no realm is configured."" ) ; } super . doMessageReceived ( nextFilter , session , message ) ; return ; } String challengeIdentity = httpRequest . getHeader ( HEADER_SEC_CHALLENGE_IDENTITY ) ; LoginContext [ ] loginContexts = getLoginContexts ( challengeIdentity ) ; int realmIndex = findCurrentRealm ( loginContexts ) ; HttpRealmInfo realm = realms [ realmIndex ] ; AuthenticationTokenExtractor tokenExtractor = DefaultAuthenticationTokenExtractor . INSTANCE ; DefaultAuthenticationToken authToken = ( DefaultAuthenticationToken ) tokenExtractor . extract ( httpRequest , realm ) ; String expectedChallengeScheme = getBaseAuthScheme ( realm . getChallengeScheme ( ) ) ; if ( authToken . getScheme ( ) == null ) { authToken . setScheme ( expectedChallengeScheme ) ; } suspendIncoming ( session ) ; TypedCallbackHandlerMap additionalCallbacks = null ; if ( realmIndex > 0 ) { additionalCallbacks = new TypedCallbackHandlerMap ( ) ; Function < String , Subject > subjects = name -> findNamedSubject ( name , realms , realmIndex , loginContexts ) ; NamedSubjectCallbackHandler callbackHandler = new NamedSubjectCallbackHandler ( subjects ) ; additionalCallbacks . put ( NamedSubjectCallback . class , callbackHandler ) ; } LoginContextTask loginContextTask = new LoginContextTask ( nextFilter , session , httpRequest , authToken , additionalCallbacks , realms , realmIndex , loginContexts ) ; scheduler . execute ( loginContextTask ) ; } }","public class A { void securityMessageReceived ( NextFilter nextFilter , IoSession session , Object message ) throws Exception { final boolean loggerIsEnabled = logger != null && logger . isTraceEnabled ( ) ; if ( ! httpRequestMessageReceived ( nextFilter , session , message ) ) return ; HttpRequestMessage httpRequest = ( HttpRequestMessage ) message ; ResourceAddress httpAddress = httpRequest . getLocalAddress ( ) ; if ( alreadyLoggedIn ( httpRequest ) ) { if ( httpRequest . getLoginContext ( ) == null ) { setUnprotectedLoginContext ( httpRequest ) ; } if ( loggerIsEnabled ) { logger . trace ( ""HttpSubjectSecurityFilter skipped because we are already allowed or logged in."" ) ; } super . doMessageReceived ( nextFilter , session , message ) ; return ; } HttpRealmInfo [ ] realms = httpAddress . getOption ( HttpResourceAddress . REALMS ) ; if ( realms . length == 0 ) { setUnprotectedLoginContext ( httpRequest ) ; if ( loggerIsEnabled ) { logger . trace ( ""HttpSecurityStandardFilter skipped because no realm is configured."" ) ; } super . doMessageReceived ( nextFilter , session , message ) ; return ; } String challengeIdentity = httpRequest . getHeader ( HEADER_SEC_CHALLENGE_IDENTITY ) ; LoginContext [ ] loginContexts = getLoginContexts ( challengeIdentity ) ; int realmIndex = findCurrentRealm ( loginContexts ) ; HttpRealmInfo realm = realms [ realmIndex ] ; AuthenticationTokenExtractor tokenExtractor = DefaultAuthenticationTokenExtractor . INSTANCE ; DefaultAuthenticationToken authToken = ( DefaultAuthenticationToken ) tokenExtractor . extract ( httpRequest , realm ) ; String expectedChallengeScheme = getBaseAuthScheme ( realm . getChallengeScheme ( ) ) ; if ( authToken . getScheme ( ) == null ) { authToken . setScheme ( expectedChallengeScheme ) ; } suspendIncoming ( session ) ; TypedCallbackHandlerMap additionalCallbacks = null ; if ( realmIndex > 0 ) { additionalCallbacks = new TypedCallbackHandlerMap ( ) ; Function < String , Subject > subjects = name -> findNamedSubject ( name , realms , realmIndex , loginContexts ) ; NamedSubjectCallbackHandler callbackHandler = new NamedSubjectCallbackHandler ( subjects ) ; additionalCallbacks . put ( NamedSubjectCallback . class , callbackHandler ) ; } LoginContextTask loginContextTask = new LoginContextTask ( nextFilter , session , httpRequest , authToken , additionalCallbacks , realms , realmIndex , loginContexts ) ; scheduler . execute ( loginContextTask ) ; } }","logger . trace ( ""HttpSubjectSecurityFilter skipped because we are already allowed or logged in."" ) ;",Meaningful
"public class A { public static List < PrincipalName > cryptoRetrieveX509Sans ( List < Certificate > certificates ) throws KrbException { List < PrincipalName > principalNames = new ArrayList < > ( ) ; for ( Certificate cert : certificates ) { LOG . info ( ""X509Sans: "" + cert . getSerialNumber ( ) ) ; } return principalNames ; } }","public class A { public static List < PrincipalName > cryptoRetrieveX509Sans ( List < Certificate > certificates ) throws KrbException { List < PrincipalName > principalNames = new ArrayList < > ( ) ; for ( Certificate cert : certificates ) { LOG . info ( ""Looking for SANs in cert: "" + cert . getTBSCertificate ( ) . getSubject ( ) ) ; } return principalNames ; } }","LOG . info ( ""Looking for SANs in cert: "" + cert . getTBSCertificate ( ) . getSubject ( ) ) ;",Meaningful
"public class A { private void schedule ( final IndexSet indexSet ) { final String indexSetId = indexSet . getConfig ( ) . id ( ) ; final String indexSetTitle = indexSet . getConfig ( ) . title ( ) ; final Duration refreshInterval = indexSet . getConfig ( ) . fieldTypeRefreshInterval ( ) ; if ( Duration . ZERO . equals ( refreshInterval ) ) { LOG . debug ( ""Skipping header refresh for index set <{}/{}>"" , indexSetTitle , indexSetId ) ; return ; } if ( ! indexSet . getConfig ( ) . isWritable ( ) ) { LOG . debug ( ""Skipping non-writable index set <{}/{}>"" , indexSetTitle , indexSetId ) ; return ; } cancel ( futures . get ( indexSetId ) ) ; LOG . debug ( ""Schedule index field type updating for index set <{}/{}> every {} ms"" , indexSetId , indexSetTitle , refreshInterval . getMillis ( ) ) ; final ScheduledFuture < ? > future = scheduler . scheduleAtFixedRate ( ( ) -> { if ( serverIsNotRunning ( ) ) { return ; } try { final String activeWriteIndex = indexSet . getActiveWriteIndex ( ) ; if ( activeWriteIndex != null ) { LOG . debug ( ""Updating index field types for active write index <{}> in index set <{}/{}>"" , activeWriteIndex , indexSetTitle , indexSetId ) ; poller . pollIndex ( activeWriteIndex , indexSetId ) . ifPresent ( dbService :: upsert ) ; } else { LOG . warn ( ""Active write index for index set \""{}\"" ({}) doesn't exist yet"" , indexSetTitle , indexSetId ) ; } } catch ( TooManyAliasesException e ) { LOG . error ( ""Couldn't get active write index"" , e ) ; } catch ( Exception e ) { LOG . error ( ""Couldn't update field types for index set <{}/{}>"" , indexSetTitle , indexSetId , e ) ; } } , 0 , refreshInterval . getMillis ( ) , TimeUnit . MILLISECONDS ) ; futures . put ( indexSetId , future ) ; } }","public class A { private void schedule ( final IndexSet indexSet ) { final String indexSetId = indexSet . getConfig ( ) . id ( ) ; final String indexSetTitle = indexSet . getConfig ( ) . title ( ) ; final Duration refreshInterval = indexSet . getConfig ( ) . fieldTypeRefreshInterval ( ) ; if ( Duration . ZERO . equals ( refreshInterval ) ) { LOG . debug ( ""Skipping index set with ZERO refresh interval <{}/{}>"" , indexSetTitle , indexSetId ) ; return ; } if ( ! indexSet . getConfig ( ) . isWritable ( ) ) { LOG . debug ( ""Skipping non-writable index set <{}/{}>"" , indexSetTitle , indexSetId ) ; return ; } cancel ( futures . get ( indexSetId ) ) ; LOG . debug ( ""Schedule index field type updating for index set <{}/{}> every {} ms"" , indexSetId , indexSetTitle , refreshInterval . getMillis ( ) ) ; final ScheduledFuture < ? > future = scheduler . scheduleAtFixedRate ( ( ) -> { if ( serverIsNotRunning ( ) ) { return ; } try { final String activeWriteIndex = indexSet . getActiveWriteIndex ( ) ; if ( activeWriteIndex != null ) { LOG . debug ( ""Updating index field types for active write index <{}> in index set <{}/{}>"" , activeWriteIndex , indexSetTitle , indexSetId ) ; poller . pollIndex ( activeWriteIndex , indexSetId ) . ifPresent ( dbService :: upsert ) ; } else { LOG . warn ( ""Active write index for index set \""{}\"" ({}) doesn't exist yet"" , indexSetTitle , indexSetId ) ; } } catch ( TooManyAliasesException e ) { LOG . error ( ""Couldn't get active write index"" , e ) ; } catch ( Exception e ) { LOG . error ( ""Couldn't update field types for index set <{}/{}>"" , indexSetTitle , indexSetId , e ) ; } } , 0 , refreshInterval . getMillis ( ) , TimeUnit . MILLISECONDS ) ; futures . put ( indexSetId , future ) ; } }","LOG . debug ( ""Skipping index set with ZERO refresh interval <{}/{}>"" , indexSetTitle , indexSetId ) ;",Meaningful
"public class A { @ Override public void enableResource ( final String clusterName , final String resourceName , final boolean enabled ) { logger . info ( ""Enabling resource {} on cluster {}"" , resourceName , clusterName ) ; String path = PropertyPathBuilder . idealState ( clusterName , resourceName ) ; BaseDataAccessor < ZNRecord > baseAccessor = new ZkBaseDataAccessor < ZNRecord > ( _zkClient ) ; if ( ! baseAccessor . exists ( path , 0 ) ) { throw new HelixException ( ""Cluster "" + clusterName + "", resource: "" + resourceName + "", ideal-state does not exist"" ) ; } baseAccessor . update ( path , new DataUpdater < ZNRecord > ( ) { @ Override public ZNRecord update ( ZNRecord currentData ) { if ( currentData == null ) { throw new HelixException ( ""Cluster: "" + clusterName + "", resource: "" + resourceName + "", ideal-state is null"" ) ; } IdealState idealState = new IdealState ( currentData ) ; idealState . enable ( enabled ) ; return idealState . getRecord ( ) ; } } , AccessOption . PERSISTENT ) ; } }","public class A { @ Override public void enableResource ( final String clusterName , final String resourceName , final boolean enabled ) { logger . info ( ""{} resource {} in cluster {}."" , enabled ? ""Enable"" : ""Disable"" , resourceName , clusterName ) ; String path = PropertyPathBuilder . idealState ( clusterName , resourceName ) ; BaseDataAccessor < ZNRecord > baseAccessor = new ZkBaseDataAccessor < ZNRecord > ( _zkClient ) ; if ( ! baseAccessor . exists ( path , 0 ) ) { throw new HelixException ( ""Cluster "" + clusterName + "", resource: "" + resourceName + "", ideal-state does not exist"" ) ; } baseAccessor . update ( path , new DataUpdater < ZNRecord > ( ) { @ Override public ZNRecord update ( ZNRecord currentData ) { if ( currentData == null ) { throw new HelixException ( ""Cluster: "" + clusterName + "", resource: "" + resourceName + "", ideal-state is null"" ) ; } IdealState idealState = new IdealState ( currentData ) ; idealState . enable ( enabled ) ; return idealState . getRecord ( ) ; } } , AccessOption . PERSISTENT ) ; } }","logger . info ( ""{} resource {} in cluster {}."" , enabled ? ""Enable"" : ""Disable"" , resourceName , clusterName ) ;",Meaningful
"public class A { @ BeforeEach void logTest ( TestInfo testInfo ) { logger . info ( ""{}"" , testInfo ) ; } }","public class A { @ BeforeEach void logTest ( TestInfo testInfo ) { log . info ( ""Executing: {}"" , testInfo . getDisplayName ( ) ) ; } }","log . info ( ""Executing: {}"" , testInfo . getDisplayName ( ) ) ;",Meaningful
"public class A { @ Override protected void onInitialize ( ) { super . onInitialize ( ) ; final Form < ? > form = new Form < Void > ( ""form"" ) ; this . add ( form ) ; AjaxButton button = new AjaxButton ( ""button"" ) { private static final long serialVersionUID = 1L ; @ Override protected void onConfigure ( ) { super . onConfigure ( ) ; LOG . info ( ""Component#onConfigure()"" ) ; this . add ( new JQueryBehavior ( JQueryWidget . getSelector ( this ) ) { private static final long serialVersionUID = 1L ; @ Override public void onConfigure ( Component component ) { component . setEnabled ( false ) ; super . onConfigure ( component ) ; } } ) ; } @ Override public void onConfigure ( JQueryBehavior behavior ) { super . onConfigure ( behavior ) ; LOG . info ( ""Component#onConfigure(JQueryBehavior)"" ) ; behavior . setOption ( ""active"" , this . isEnabledInHierarchy ( ) ) ; } @ Override protected void onBeforeRender ( ) { super . onBeforeRender ( ) ; LOG . info ( ""Component#onBeforeRender(JQueryBehavior)"" ) ; } @ Override public void onBeforeRender ( JQueryBehavior behavior ) { super . onBeforeRender ( behavior ) ; LOG . info ( ""Component#onBeforeRender(JQueryBehavior)"" ) ; } } ; form . add ( button ) ; } }","public class A { @ Override protected void onInitialize ( ) { super . onInitialize ( ) ; final Form < ? > form = new Form < Void > ( ""form"" ) ; this . add ( form ) ; AjaxButton button = new AjaxButton ( ""button"" ) { private static final long serialVersionUID = 1L ; @ Override protected void onConfigure ( ) { super . onConfigure ( ) ; LOG . info ( ""Component#onConfigure()"" ) ; this . add ( new JQueryBehavior ( JQueryWidget . getSelector ( this ) ) { private static final long serialVersionUID = 1L ; @ Override public void onConfigure ( Component component ) { component . setEnabled ( false ) ; super . onConfigure ( component ) ; } } ) ; } @ Override public void onConfigure ( JQueryBehavior behavior ) { super . onConfigure ( behavior ) ; LOG . info ( ""Component#onConfigure(JQueryBehavior)"" ) ; behavior . setOption ( ""active"" , this . isEnabledInHierarchy ( ) ) ; } @ Override protected void onBeforeRender ( ) { super . onBeforeRender ( ) ; LOG . info ( ""Component#onBeforeRender()"" ) ; } @ Override public void onBeforeRender ( JQueryBehavior behavior ) { super . onBeforeRender ( behavior ) ; LOG . info ( ""Component#onBeforeRender(JQueryBehavior)"" ) ; } } ; form . add ( button ) ; } }","LOG . info ( ""Component#onBeforeRender()"" ) ;",Meaningful
"public class A { private void clearOldCheckpoint ( FileSystem fs , Path checkpointPath ) throws IOException { int maxCheckpoint = context . getConf ( ) . getInt ( AngelConf . ANGEL_PS_CHECKPOINTS_MAX_NUM , AngelConf . DEFAULT_ANGEL_PS_CHECKPOINTS_MAX_NUM ) ; FileStatus [ ] status = fs . listStatus ( checkpointPath ) ; if ( status . length <= maxCheckpoint ) { return ; } int [ ] checkpointIds = new int [ status . length ] ; for ( int i = 0 ; i < status . length ; i ++ ) { try { checkpointIds [ i ] = Integer . valueOf ( status [ i ] . getPath ( ) . getName ( ) ) ; } catch ( Throwable x ) { LOG . warn ( ""Path "" + status [ i ] . getPath ( ) . toString ( ) + "" is not a valid checkpoint path, just remove it"" ) ; } } Sort . quickSort ( checkpointIds , status , 0 , checkpointIds . length - 1 ) ; for ( int i = 0 ; i < checkpointIds . length - maxCheckpoint ; i ++ ) { try { if ( ! fs . delete ( status [ i ] . getPath ( ) , true ) ) { LOG . warn ( ""Delete path "" + status [ i ] . getPath ( ) + "" failed "" ) ; } else { LOG . info ( ""Deleted path "" + status [ i ] . getPath ( ) ) ; } } catch ( Throwable x ) { LOG . warn ( ""Delete path "" + status [ i ] . getPath ( ) + "" failed "" , x ) ; } } } }","public class A { private void clearOldCheckpoint ( FileSystem fs , Path checkpointPath ) throws IOException { int maxCheckpoint = context . getConf ( ) . getInt ( AngelConf . ANGEL_PS_CHECKPOINTS_MAX_NUM , AngelConf . DEFAULT_ANGEL_PS_CHECKPOINTS_MAX_NUM ) ; FileStatus [ ] status = fs . listStatus ( checkpointPath ) ; if ( status . length <= maxCheckpoint ) { return ; } int [ ] checkpointIds = new int [ status . length ] ; for ( int i = 0 ; i < status . length ; i ++ ) { try { checkpointIds [ i ] = Integer . valueOf ( status [ i ] . getPath ( ) . getName ( ) ) ; } catch ( Throwable x ) { LOG . warn ( ""Path "" + status [ i ] . getPath ( ) . toString ( ) + "" is not a valid checkpoint path, just remove it"" ) ; } } Sort . quickSort ( checkpointIds , status , 0 , checkpointIds . length - 1 ) ; for ( int i = 0 ; i < checkpointIds . length - maxCheckpoint ; i ++ ) { try { if ( ! fs . delete ( status [ i ] . getPath ( ) , true ) ) { LOG . warn ( ""Delete path "" + status [ i ] . getPath ( ) + "" failed "" ) ; } else { LOG . info ( ""Delete old checkpoint "" + status [ i ] . getPath ( ) + "" failed "" ) ; } } catch ( Throwable x ) { LOG . warn ( ""Delete path "" + status [ i ] . getPath ( ) + "" failed "" , x ) ; } } } }","LOG . info ( ""Delete old checkpoint "" + status [ i ] . getPath ( ) + "" failed "" ) ;",Meaningful
"public class A { public static void updateRefNamesInRelations ( ServiceContext < PoxPayloadIn , PoxPayloadOut > ctx , RepositoryClient < PoxPayloadIn , PoxPayloadOut > repoClient , CoreSessionInterface repoSession , String targetField , String oldRefName , String newRefName ) throws Exception { int docsUpdated = 0 ; int currentPage = 0 ; int docsInCurrentPage = 0 ; final String ORDER_BY_VALUE = CollectionSpaceClient . CORE_CREATED_AT + "", "" + IQueryManager . NUXEO_UUID ; try { boolean morePages = true ; while ( morePages ) { DocumentModelList docModelList = findRelationsWithRefName ( ctx , repoClient , repoSession , oldRefName , targetField , ORDER_BY_VALUE , currentPage , DEFAULT_PAGE_SIZE , true ) ; if ( docModelList == null ) { logger . trace ( ""updateRefNamesInRelations: no documents could be found that referenced the old refName"" ) ; break ; } docsInCurrentPage = docModelList . size ( ) ; logger . trace ( ""updateRefNamesInRelations: current page="" + currentPage + "" documents included in page="" + docsInCurrentPage ) ; if ( docsInCurrentPage == 0 ) { logger . trace ( ""updateRefNamesInRelations: no more documents requiring refName updates could be found"" ) ; break ; } if ( docsInCurrentPage < DEFAULT_PAGE_SIZE ) { logger . trace ( ""updateRefNamesInRelations: assuming no more documents requiring refName updates will be found, as docsInCurrentPage < pageSize"" ) ; morePages = false ; } for ( DocumentModel docModel : docModelList ) { try { docModel . setProperty ( IRelationsManager . SERVICE_COMMONPART_NAME , targetField , newRefName ) ; repoSession . saveDocument ( docModel ) ; } catch ( ClientException e ) { logger . error ( String . format ( ""Could not update field '%s' with updated refName '%s' for relations record CSID=%s"" , targetField , newRefName , docModel . getName ( ) ) ) ; } } docsUpdated += docsInCurrentPage ; if ( morePages ) { currentPage ++ ; } } } catch ( Exception e ) { logger . error ( ""Internal error updating the ref names in relations: "" + e . getLocalizedMessage ( ) ) ; logger . debug ( Tools . errorToString ( e , true ) ) ; throw e ; } try { repoSession . save ( ) ; } catch ( ClientException e ) { logger . error ( String . format ( ""Could not update field '%s' with new refName '%s' where "" + targetField + "" contained old refName "" + oldRefName , e ) ) ; } logger . debug ( ""updateRefNamesInRelations updated "" + docsUpdated + "" relations document(s)"" + "" with new refName "" + newRefName + "" where "" + targetField + "" contained old refName "" + oldRefName ) ; } }","public class A { public static void updateRefNamesInRelations ( ServiceContext < PoxPayloadIn , PoxPayloadOut > ctx , RepositoryClient < PoxPayloadIn , PoxPayloadOut > repoClient , CoreSessionInterface repoSession , String targetField , String oldRefName , String newRefName ) throws Exception { int docsUpdated = 0 ; int currentPage = 0 ; int docsInCurrentPage = 0 ; final String ORDER_BY_VALUE = CollectionSpaceClient . CORE_CREATED_AT + "", "" + IQueryManager . NUXEO_UUID ; try { boolean morePages = true ; while ( morePages ) { DocumentModelList docModelList = findRelationsWithRefName ( ctx , repoClient , repoSession , oldRefName , targetField , ORDER_BY_VALUE , currentPage , DEFAULT_PAGE_SIZE , true ) ; if ( docModelList == null ) { logger . trace ( ""updateRefNamesInRelations: no documents could be found that referenced the old refName"" ) ; break ; } docsInCurrentPage = docModelList . size ( ) ; logger . trace ( ""updateRefNamesInRelations: current page="" + currentPage + "" documents included in page="" + docsInCurrentPage ) ; if ( docsInCurrentPage == 0 ) { logger . trace ( ""updateRefNamesInRelations: no more documents requiring refName updates could be found"" ) ; break ; } if ( docsInCurrentPage < DEFAULT_PAGE_SIZE ) { logger . trace ( ""updateRefNamesInRelations: assuming no more documents requiring refName updates will be found, as docsInCurrentPage < pageSize"" ) ; morePages = false ; } for ( DocumentModel docModel : docModelList ) { try { docModel . setProperty ( IRelationsManager . SERVICE_COMMONPART_NAME , targetField , newRefName ) ; repoSession . saveDocument ( docModel ) ; } catch ( ClientException e ) { logger . error ( String . format ( ""Could not update field '%s' with updated refName '%s' for relations record CSID=%s"" , targetField , newRefName , docModel . getName ( ) ) ) ; } } docsUpdated += docsInCurrentPage ; if ( morePages ) { currentPage ++ ; } } } catch ( Exception e ) { logger . error ( ""Internal error updating the ref names in relations: "" + e . getLocalizedMessage ( ) ) ; logger . debug ( Tools . errorToString ( e , true ) ) ; throw e ; } try { repoSession . save ( ) ; } catch ( ClientException e ) { logger . error ( ""Could not flush results of relation-refName payload updates to Nuxeo repository"" ) ; } logger . debug ( ""updateRefNamesInRelations updated "" + docsUpdated + "" relations document(s)"" + "" with new refName "" + newRefName + "" where "" + targetField + "" contained old refName "" + oldRefName ) ; } }","logger . error ( ""Could not flush results of relation-refName payload updates to Nuxeo repository"" ) ;",Meaningful
"public class A { public void publish ( Release release ) { ServiceRegistration < Release > oldServiceRegistration = _serviceConfiguratorRegistrations . get ( release . getServletContextName ( ) ) ; if ( oldServiceRegistration != null ) { oldServiceRegistration . unregister ( ) ; } Dictionary < String , Object > properties = new Hashtable < > ( ) ; properties . put ( ""release.bundle.symbolic.name"" , release . getBundleSymbolicName ( ) ) ; properties . put ( ""release.state"" , release . getState ( ) ) ; try { if ( Validator . isNotNull ( release . getSchemaVersion ( ) ) ) { Version version = new Version ( release . getSchemaVersion ( ) ) ; properties . put ( ""release.schema.version"" , version ) ; } } catch ( IllegalArgumentException illegalArgumentException ) { if ( _log . isWarnEnabled ( ) ) { _log . warn ( StringBundler . concat ( ""Unable to generate version for "" , release , "" in "" , properties ) , illegalArgumentException ) ; } } ServiceRegistration < Release > newServiceRegistration = _bundleContext . registerService ( Release . class , release , properties ) ; _serviceConfiguratorRegistrations . put ( release . getServletContextName ( ) , newServiceRegistration ) ; } }","public class A { public void publish ( Release release ) { ServiceRegistration < Release > oldServiceRegistration = _serviceConfiguratorRegistrations . get ( release . getServletContextName ( ) ) ; if ( oldServiceRegistration != null ) { oldServiceRegistration . unregister ( ) ; } Dictionary < String , Object > properties = new Hashtable < > ( ) ; properties . put ( ""release.bundle.symbolic.name"" , release . getBundleSymbolicName ( ) ) ; properties . put ( ""release.state"" , release . getState ( ) ) ; try { if ( Validator . isNotNull ( release . getSchemaVersion ( ) ) ) { Version version = new Version ( release . getSchemaVersion ( ) ) ; properties . put ( ""release.schema.version"" , version ) ; } } catch ( IllegalArgumentException illegalArgumentException ) { if ( _log . isWarnEnabled ( ) ) { _log . warn ( ""Invalid schema version for release: "" + release , illegalArgumentException ) ; } } ServiceRegistration < Release > newServiceRegistration = _bundleContext . registerService ( Release . class , release , properties ) ; _serviceConfiguratorRegistrations . put ( release . getServletContextName ( ) , newServiceRegistration ) ; } }","_log . warn ( ""Invalid schema version for release: "" + release , illegalArgumentException ) ;",Meaningful
"public class A { @ SuppressWarnings ( ""unchecked"" ) private void register ( final G gateway , final int attempt , final long timeoutMillis ) { if ( canceled ) { return ; } try { log . debug ( ""Registration at {} attempt {} (timeout={}ms)"" , targetName , attempt , timeoutMillis ) ; CompletableFuture < RegistrationResponse > registrationFuture = invokeRegistration ( gateway , fencingToken , timeoutMillis ) ; CompletableFuture < Void > registrationAcceptFuture = registrationFuture . thenAcceptAsync ( ( RegistrationResponse result ) -> { if ( ! isCanceled ( ) ) { if ( result instanceof RegistrationResponse . Success ) { log . debug ( ""Registration with {} at {} was successful."" , targetName , targetAddress ) ; S success = ( S ) result ; completionFuture . complete ( RetryingRegistrationResult . success ( gateway , success ) ) ; } else if ( result instanceof RegistrationResponse . Rejection ) { log . debug ( ""Registration with {} at {} was rejected."" , targetName , targetAddress ) ; R rejection = ( R ) result ; completionFuture . complete ( RetryingRegistrationResult . rejection ( rejection ) ) ; } else { if ( result instanceof RegistrationResponse . Failure ) { RegistrationResponse . Failure failure = ( RegistrationResponse . Failure ) result ; log . info ( ""Registration failure at {} occurred."" , targetName , failure . getReason ( ) ) ; } else { log . error ( ""Registration at {} failed due to an error: {}"" , targetName , result . getReason ( ) ) ; } log . info ( ""Pausing and re-attempting registration in {} ms"" , retryingRegistrationConfiguration . getRefusedDelayMillis ( ) ) ; registerLater ( gateway , 1 , retryingRegistrationConfiguration . getInitialRegistrationTimeoutMillis ( ) , retryingRegistrationConfiguration . getRefusedDelayMillis ( ) ) ; } } } , rpcService . getExecutor ( ) ) ; registrationAcceptFuture . whenCompleteAsync ( ( Void v , Throwable failure ) -> { if ( failure != null && ! isCanceled ( ) ) { if ( ExceptionUtils . stripCompletionException ( failure ) instanceof TimeoutException ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Registration at {} ({}) attempt {} timed out after {} ms"" , targetName , targetAddress , attempt , timeoutMillis ) ; } long newTimeoutMillis = Math . min ( 2 * timeoutMillis , retryingRegistrationConfiguration . getMaxRegistrationTimeoutMillis ( ) ) ; register ( gateway , attempt + 1 , newTimeoutMillis ) ; } else { log . error ( ""Registration at {} failed due to an error"" , targetName , failure ) ; log . info ( ""Pausing and re-attempting registration in {} ms"" , retryingRegistrationConfiguration . getErrorDelayMillis ( ) ) ; registerLater ( gateway , 1 , retryingRegistrationConfiguration . getInitialRegistrationTimeoutMillis ( ) , retryingRegistrationConfiguration . getErrorDelayMillis ( ) ) ; } } } , rpcService . getExecutor ( ) ) ; } catch ( Throwable t ) { completionFuture . completeExceptionally ( t ) ; cancel ( ) ; } } }","public class A { @ SuppressWarnings ( ""unchecked"" ) private void register ( final G gateway , final int attempt , final long timeoutMillis ) { if ( canceled ) { return ; } try { log . debug ( ""Registration at {} attempt {} (timeout={}ms)"" , targetName , attempt , timeoutMillis ) ; CompletableFuture < RegistrationResponse > registrationFuture = invokeRegistration ( gateway , fencingToken , timeoutMillis ) ; CompletableFuture < Void > registrationAcceptFuture = registrationFuture . thenAcceptAsync ( ( RegistrationResponse result ) -> { if ( ! isCanceled ( ) ) { if ( result instanceof RegistrationResponse . Success ) { log . debug ( ""Registration with {} at {} was successful."" , targetName , targetAddress ) ; S success = ( S ) result ; completionFuture . complete ( RetryingRegistrationResult . success ( gateway , success ) ) ; } else if ( result instanceof RegistrationResponse . Rejection ) { log . debug ( ""Registration with {} at {} was rejected."" , targetName , targetAddress ) ; R rejection = ( R ) result ; completionFuture . complete ( RetryingRegistrationResult . rejection ( rejection ) ) ; } else { if ( result instanceof RegistrationResponse . Failure ) { RegistrationResponse . Failure failure = ( RegistrationResponse . Failure ) result ; log . info ( ""Registration failure at {} occurred."" , targetName , failure . getReason ( ) ) ; } else { log . error ( ""Received unknown response to registration attempt: {}"" , result ) ; } log . info ( ""Pausing and re-attempting registration in {} ms"" , retryingRegistrationConfiguration . getRefusedDelayMillis ( ) ) ; registerLater ( gateway , 1 , retryingRegistrationConfiguration . getInitialRegistrationTimeoutMillis ( ) , retryingRegistrationConfiguration . getRefusedDelayMillis ( ) ) ; } } } , rpcService . getExecutor ( ) ) ; registrationAcceptFuture . whenCompleteAsync ( ( Void v , Throwable failure ) -> { if ( failure != null && ! isCanceled ( ) ) { if ( ExceptionUtils . stripCompletionException ( failure ) instanceof TimeoutException ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Registration at {} ({}) attempt {} timed out after {} ms"" , targetName , targetAddress , attempt , timeoutMillis ) ; } long newTimeoutMillis = Math . min ( 2 * timeoutMillis , retryingRegistrationConfiguration . getMaxRegistrationTimeoutMillis ( ) ) ; register ( gateway , attempt + 1 , newTimeoutMillis ) ; } else { log . error ( ""Registration at {} failed due to an error"" , targetName , failure ) ; log . info ( ""Pausing and re-attempting registration in {} ms"" , retryingRegistrationConfiguration . getErrorDelayMillis ( ) ) ; registerLater ( gateway , 1 , retryingRegistrationConfiguration . getInitialRegistrationTimeoutMillis ( ) , retryingRegistrationConfiguration . getErrorDelayMillis ( ) ) ; } } } , rpcService . getExecutor ( ) ) ; } catch ( Throwable t ) { completionFuture . completeExceptionally ( t ) ; cancel ( ) ; } } }","log . error ( ""Received unknown response to registration attempt: {}"" , result ) ;",Meaningful
"public class A { public void updateAlertDefinition ( StatAlertDefinition [ ] defns , int actionCode ) { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Entered StatAlertsManager.updateAlertDefinition *****"" ) ; } synchronized ( alertDefinitionsMap ) { if ( actionCode == UpdateAlertDefinitionMessage . REMOVE_ALERT_DEFINITION ) { for ( int i = 0 ; i < defns . length ; i ++ ) { alertDefinitionsMap . remove ( Integer . valueOf ( defns [ i ] . getId ( ) ) ) ; if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Removed alert definition from stat alert definition #{}"" , defns [ i ] . getId ( ) ) ; } } } else { StatAlertDefinition [ ] alertDefns = this . createMemberStatAlertDefinition ( dm , defns ) ; StatAlertDefinition defn ; for ( int i = 0 ; i < alertDefns . length ; i ++ ) { defn = alertDefns [ i ] ; alertDefinitionsMap . put ( Integer . valueOf ( defns [ i ] . getId ( ) ) , defn ) ; } } } if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Exiting StatAlertsManager.updateAlertDefinition *****"" ) ; } } }","public class A { public void updateAlertDefinition ( StatAlertDefinition [ ] defns , int actionCode ) { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Entered StatAlertsManager.updateAlertDefinition *****"" ) ; } synchronized ( alertDefinitionsMap ) { if ( actionCode == UpdateAlertDefinitionMessage . REMOVE_ALERT_DEFINITION ) { for ( int i = 0 ; i < defns . length ; i ++ ) { alertDefinitionsMap . remove ( Integer . valueOf ( defns [ i ] . getId ( ) ) ) ; if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Removed StatAlertDefinition: {}"" , defns [ i ] . getName ( ) ) ; } } } else { StatAlertDefinition [ ] alertDefns = this . createMemberStatAlertDefinition ( dm , defns ) ; StatAlertDefinition defn ; for ( int i = 0 ; i < alertDefns . length ; i ++ ) { defn = alertDefns [ i ] ; alertDefinitionsMap . put ( Integer . valueOf ( defns [ i ] . getId ( ) ) , defn ) ; } } } if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Exiting StatAlertsManager.updateAlertDefinition *****"" ) ; } } }","logger . debug ( ""Removed StatAlertDefinition: {}"" , defns [ i ] . getName ( ) ) ;",Meaningful
"public class A { @ Override public SearchResult search ( final ObjectClass objectClass , final Filter filter , final SearchResultsHandler handler , final OperationOptions options ) { SearchResult result = new SearchResult ( ) ; try { for ( int record = 1 ; reader ( ) . hasNext ( ) ; record ++ ) { Map < String , String > row = reader ( ) . next ( ) ; String keyValue = row . get ( keyColumn ) ; if ( StringUtils . isBlank ( keyValue ) ) { keyValue = ""Record "" + record ; } ConnectorObjectBuilder builder = new ConnectorObjectBuilder ( ) . setObjectClass ( objectClass ) . setUid ( keyValue ) . setName ( keyValue ) ; row . forEach ( ( key , value ) -> builder . addAttribute ( arrayElementsSeparator == null ? AttributeBuilder . build ( key , value ) : AttributeBuilder . build ( key , ( Object [ ] ) StringUtils . splitByWholeSeparator ( value , arrayElementsSeparator ) ) ) ) ; ConnectorObject obj = builder . build ( ) ; if ( filter == null || filter . accept ( obj ) ) { handler . handle ( obj ) ; } else { LOG . debug ( ""Filter is null"" ) ; } } } catch ( IOException e ) { LOG . error ( ""Could not read CSV from provided stream"" , e ) ; throw new ConnectorException ( e ) ; } return result ; } }","public class A { @ Override public SearchResult search ( final ObjectClass objectClass , final Filter filter , final SearchResultsHandler handler , final OperationOptions options ) { SearchResult result = new SearchResult ( ) ; try { for ( int record = 1 ; reader ( ) . hasNext ( ) ; record ++ ) { Map < String , String > row = reader ( ) . next ( ) ; String keyValue = row . get ( keyColumn ) ; if ( StringUtils . isBlank ( keyValue ) ) { keyValue = ""Record "" + record ; } ConnectorObjectBuilder builder = new ConnectorObjectBuilder ( ) . setObjectClass ( objectClass ) . setUid ( keyValue ) . setName ( keyValue ) ; row . forEach ( ( key , value ) -> builder . addAttribute ( arrayElementsSeparator == null ? AttributeBuilder . build ( key , value ) : AttributeBuilder . build ( key , ( Object [ ] ) StringUtils . splitByWholeSeparator ( value , arrayElementsSeparator ) ) ) ) ; ConnectorObject obj = builder . build ( ) ; if ( filter == null || filter . accept ( obj ) ) { handler . handle ( obj ) ; } else { LOG . debug ( ""Found but not passing the provided filter {}: {}"" , filter , obj ) ; } } } catch ( IOException e ) { LOG . error ( ""Could not read CSV from provided stream"" , e ) ; throw new ConnectorException ( e ) ; } return result ; } }","LOG . debug ( ""Found but not passing the provided filter {}: {}"" , filter , obj ) ;",Meaningful
"public class A { private static Collection < SpecParameter < ? > > resolveParameters ( Collection < ? extends SpecParameter < ? > > newParams , AbstractBrooklynObjectSpec < ? , ? > spec ) { Collection < ? extends SpecParameter < ? > > existingReferenceParams = spec . getParameters ( ) ; Map < String , SpecParameter < ? > > existingToKeep = MutableMap . of ( ) ; if ( existingReferenceParams != null ) { for ( SpecParameter < ? > p : existingReferenceParams ) { if ( ConfigInheritances . isKeyReinheritable ( p . getConfigKey ( ) , InheritanceContext . TYPE_DEFINITION ) ) { existingToKeep . put ( p . getConfigKey ( ) . getName ( ) , p ) ; } } } List < SpecParameter < ? > > result = MutableList . < SpecParameter < ? > > of ( ) ; if ( newParams != null ) { for ( SpecParameter < ? > p : newParams ) { final SpecParameter < ? > existingP = existingToKeep . get ( p . getConfigKey ( ) . getName ( ) ) ; if ( p instanceof SpecParameterIncludingDefinitionForInheritance ) { p = ( ( SpecParameterIncludingDefinitionForInheritance < ? > ) p ) . resolveWithAncestor ( existingP ) ; } else { log . warn ( ""Cannot resolve "" + p + "" with "" + existingP + "" as it is not a spec of type "" + p . getConfigKey ( ) . getName ( ) + "", will skip some parameters."" ) ; } if ( existingP != null ) { existingToKeep . put ( p . getConfigKey ( ) . getName ( ) , p ) ; } else { result . add ( p ) ; } } } result . addAll ( existingToKeep . values ( ) ) ; return result ; } }","public class A { private static Collection < SpecParameter < ? > > resolveParameters ( Collection < ? extends SpecParameter < ? > > newParams , AbstractBrooklynObjectSpec < ? , ? > spec ) { Collection < ? extends SpecParameter < ? > > existingReferenceParams = spec . getParameters ( ) ; Map < String , SpecParameter < ? > > existingToKeep = MutableMap . of ( ) ; if ( existingReferenceParams != null ) { for ( SpecParameter < ? > p : existingReferenceParams ) { if ( ConfigInheritances . isKeyReinheritable ( p . getConfigKey ( ) , InheritanceContext . TYPE_DEFINITION ) ) { existingToKeep . put ( p . getConfigKey ( ) . getName ( ) , p ) ; } } } List < SpecParameter < ? > > result = MutableList . < SpecParameter < ? > > of ( ) ; if ( newParams != null ) { for ( SpecParameter < ? > p : newParams ) { final SpecParameter < ? > existingP = existingToKeep . get ( p . getConfigKey ( ) . getName ( ) ) ; if ( p instanceof SpecParameterIncludingDefinitionForInheritance ) { p = ( ( SpecParameterIncludingDefinitionForInheritance < ? > ) p ) . resolveWithAncestor ( existingP ) ; } else { log . warn ( ""Found non-definitional spec parameter: "" + p + "" adding to "" + spec ) ; } if ( existingP != null ) { existingToKeep . put ( p . getConfigKey ( ) . getName ( ) , p ) ; } else { result . add ( p ) ; } } } result . addAll ( existingToKeep . values ( ) ) ; return result ; } }","log . warn ( ""Found non-definitional spec parameter: "" + p + "" adding to "" + spec ) ;",Meaningful
"public class A { private void registerConfigurationChangeListeners ( ) { childAdded ( null , _broker ) ; if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Registering model listeners for broker "" + _broker ) ; } for ( VirtualHostNode < ? > vhostNode : _broker . getVirtualHostNodes ( ) ) { if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Loading model listener "" + vhostNode . getName ( ) ) ; } VirtualHost < ? , ? , ? > vhost = vhostNode . getVirtualHost ( ) ; if ( vhost != null ) { vhost . addChangeListener ( this ) ; addListenersForConnectionsAndChildren ( vhost ) ; addListenersForExchangesAndChildren ( vhost ) ; addListenersForQueuesAndChildren ( vhost ) ; } } if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Registered model listeners"" ) ; } } }","public class A { private void registerConfigurationChangeListeners ( ) { childAdded ( null , _broker ) ; if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Registering model listeners for broker "" + _broker ) ; } for ( VirtualHostNode < ? > vhostNode : _broker . getVirtualHostNodes ( ) ) { if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Considering virtualhostnode "" + vhostNode ) ; } VirtualHost < ? , ? , ? > vhost = vhostNode . getVirtualHost ( ) ; if ( vhost != null ) { vhost . addChangeListener ( this ) ; addListenersForConnectionsAndChildren ( vhost ) ; addListenersForExchangesAndChildren ( vhost ) ; addListenersForQueuesAndChildren ( vhost ) ; } } if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Registered model listeners"" ) ; } } }","_log . debug ( ""Considering virtualhostnode "" + vhostNode ) ;",Meaningful
"public class A { @ Override public Map < String , Object > getLogsByGroupId ( Integer groupId , Integer pageNo , Integer pageSize ) { try { Map < String , Object > returnMap = new HashMap < > ( ) ; Page < OperationLog > page = PageHelper . startPage ( pageNo , pageSize ) ; operationLogDao . selectLogsByOperationGroupId ( groupId ) ; returnMap . put ( ""logData"" , page ) ; returnMap . put ( ""totalCount"" , page . getTotal ( ) ) ; returnMap . put ( ""totalPage"" , page . getPages ( ) ) ; return returnMap ; } catch ( Exception e ) { logger . error ( ""Get logs group by group id failed, groupNo = "" + groupId , e ) ; return null ; } } }","public class A { @ Override public Map < String , Object > getLogsByGroupId ( Integer groupId , Integer pageNo , Integer pageSize ) { try { Map < String , Object > returnMap = new HashMap < > ( ) ; Page < OperationLog > page = PageHelper . startPage ( pageNo , pageSize ) ; operationLogDao . selectLogsByOperationGroupId ( groupId ) ; returnMap . put ( ""logData"" , page ) ; returnMap . put ( ""totalCount"" , page . getTotal ( ) ) ; returnMap . put ( ""totalPage"" , page . getPages ( ) ) ; return returnMap ; } catch ( Exception e ) { logger . error ( ""query logs from db error"" , e ) ; return null ; } } }","logger . error ( ""query logs from db error"" , e ) ;",Meaningful
"public class A { void processLogStream ( final InputStream is ) throws IOException , InterruptedException { ByteArrayOutputStream line = new ByteArrayOutputStream ( ) ; int c ; while ( ! logsController . stopped . get ( ) ) { c = is . read ( ) ; if ( c < 0 ) { break ; } line . write ( c ) ; if ( line . size ( ) > 1024 * 10 ) { line . write ( ""\""}"" . getBytes ( UTF_8 ) ) ; c = '\n' ; } if ( c == '\n' ) { processLine ( new String ( line . toByteArray ( ) , UTF_8 ) ) ; line . reset ( ) ; } } if ( ! logsController . stopped . get ( ) ) { if ( logsController . isPodRunning ( podName ) ) { LOG . info ( ""End of Log stream for running pod: {}"" , podName ) ; } else { LOG . info ( ""Start of Log stream for pod: {}"" , podName ) ; } } } }","public class A { void processLogStream ( final InputStream is ) throws IOException , InterruptedException { ByteArrayOutputStream line = new ByteArrayOutputStream ( ) ; int c ; while ( ! logsController . stopped . get ( ) ) { c = is . read ( ) ; if ( c < 0 ) { break ; } line . write ( c ) ; if ( line . size ( ) > 1024 * 10 ) { line . write ( ""\""}"" . getBytes ( UTF_8 ) ) ; c = '\n' ; } if ( c == '\n' ) { processLine ( new String ( line . toByteArray ( ) , UTF_8 ) ) ; line . reset ( ) ; } } if ( ! logsController . stopped . get ( ) ) { if ( logsController . isPodRunning ( podName ) ) { LOG . info ( ""End of Log stream for running pod: {}"" , podName ) ; } else { LOG . info ( ""End of Log stream for terminated pod: {}"" , podName ) ; } } } }","LOG . info ( ""End of Log stream for terminated pod: {}"" , podName ) ;",Meaningful
"public class A { public String getStatisticsType ( Integer statisticTypeId , List < StatisticImageListDto > statisticImageList ) throws DAOException { LOGGER . entry ( ""begin getStatisticsType()"" ) ; String statisticType = """" ; try { if ( ( statisticImageList != null ) && ! statisticImageList . isEmpty ( ) ) { for ( StatisticImageListDto statistic : statisticImageList ) { if ( statisticTypeId . intValue ( ) == statistic . getStatisticImageId ( ) . intValue ( ) ) { statisticType = statistic . getValue ( ) ; break ; } } } } catch ( Exception e ) { LOGGER . error ( ""getStatisticsType() :: ERROR"" , e ) ; } LOGGER . exit ( ""getStatisticsType() :: Ends"" ) ; return statisticType ; } }","public class A { public String getStatisticsType ( Integer statisticTypeId , List < StatisticImageListDto > statisticImageList ) throws DAOException { LOGGER . entry ( ""begin getStatisticsType()"" ) ; String statisticType = """" ; try { if ( ( statisticImageList != null ) && ! statisticImageList . isEmpty ( ) ) { for ( StatisticImageListDto statistic : statisticImageList ) { if ( statisticTypeId . intValue ( ) == statistic . getStatisticImageId ( ) . intValue ( ) ) { statisticType = statistic . getValue ( ) ; break ; } } } } catch ( Exception e ) { LOGGER . error ( ""DashboardMetaDataDao - getStatisticsType() :: ERROR"" , e ) ; } LOGGER . exit ( ""getStatisticsType() :: Ends"" ) ; return statisticType ; } }","LOGGER . error ( ""DashboardMetaDataDao - getStatisticsType() :: ERROR"" , e ) ;",Meaningful
"public class A { private Iterable < DiscoveryStrategy > loadDiscoveryStrategies ( DiscoveryServiceSettings settings ) { ClassLoader configClassLoader = settings . getConfigClassLoader ( ) ; try { Collection < DiscoveryStrategyConfig > discoveryStrategyConfigs = new ArrayList < DiscoveryStrategyConfig > ( settings . getAllDiscoveryConfigs ( ) ) ; List < DiscoveryStrategyFactory > factories = collectFactories ( discoveryStrategyConfigs , configClassLoader ) ; List < DiscoveryStrategy > discoveryStrategies = new ArrayList < DiscoveryStrategy > ( ) ; for ( DiscoveryStrategyConfig config : discoveryStrategyConfigs ) { DiscoveryStrategy discoveryStrategy = buildDiscoveryStrategy ( config , factories ) ; discoveryStrategies . add ( discoveryStrategy ) ; } if ( discoveryStrategies . isEmpty ( ) && settings . isAutoDetectionEnabled ( ) ) { logger . fine ( ""Discovery auto-detection enabled, looking for available discovery strategies"" ) ; DiscoveryStrategyFactory autoDetectedFactory = detectDiscoveryStrategyFactory ( factories ) ; if ( autoDetectedFactory != null ) { logger . info ( ""Instantiated discovery strategy"" ) ; discoveryStrategies . add ( autoDetectedFactory . newDiscoveryStrategy ( discoveryNode , logger , Collections . emptyMap ( ) ) ) ; } else { logger . info ( ""No discovery strategy is applicable for auto-detection"" ) ; } } return discoveryStrategies ; } catch ( Exception e ) { if ( e instanceof ValidationException ) { throw new InvalidConfigurationException ( ""Invalid configuration"" , e ) ; } else { throw new RuntimeException ( ""Failed to configure discovery strategies"" , e ) ; } } } }","public class A { private Iterable < DiscoveryStrategy > loadDiscoveryStrategies ( DiscoveryServiceSettings settings ) { ClassLoader configClassLoader = settings . getConfigClassLoader ( ) ; try { Collection < DiscoveryStrategyConfig > discoveryStrategyConfigs = new ArrayList < DiscoveryStrategyConfig > ( settings . getAllDiscoveryConfigs ( ) ) ; List < DiscoveryStrategyFactory > factories = collectFactories ( discoveryStrategyConfigs , configClassLoader ) ; List < DiscoveryStrategy > discoveryStrategies = new ArrayList < DiscoveryStrategy > ( ) ; for ( DiscoveryStrategyConfig config : discoveryStrategyConfigs ) { DiscoveryStrategy discoveryStrategy = buildDiscoveryStrategy ( config , factories ) ; discoveryStrategies . add ( discoveryStrategy ) ; } if ( discoveryStrategies . isEmpty ( ) && settings . isAutoDetectionEnabled ( ) ) { logger . fine ( ""Discovery auto-detection enabled, looking for available discovery strategies"" ) ; DiscoveryStrategyFactory autoDetectedFactory = detectDiscoveryStrategyFactory ( factories ) ; if ( autoDetectedFactory != null ) { logger . info ( String . format ( ""Auto-detection selected discovery strategy: %s"" , autoDetectedFactory . getClass ( ) ) ) ; discoveryStrategies . add ( autoDetectedFactory . newDiscoveryStrategy ( discoveryNode , logger , Collections . emptyMap ( ) ) ) ; } else { logger . info ( ""No discovery strategy is applicable for auto-detection"" ) ; } } return discoveryStrategies ; } catch ( Exception e ) { if ( e instanceof ValidationException ) { throw new InvalidConfigurationException ( ""Invalid configuration"" , e ) ; } else { throw new RuntimeException ( ""Failed to configure discovery strategies"" , e ) ; } } } }","logger . info ( String . format ( ""Auto-detection selected discovery strategy: %s"" , autoDetectedFactory . getClass ( ) ) ) ;",Meaningful
"public class A { public PortletRequestDispatcher getRequestDispatcher ( ServletContext servletContext , PortletApplicationDefinition app , String path ) { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""PortletRequestDispatcher requested for path: "" + path + "" at context: "" + app . getContextPath ( ) ) ; } if ( path == null || ! path . startsWith ( ""/"" ) ) { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( ""Failed to retrieve PortletRequestDispatcher: path name must begin with a slash '/'."" ) ; } return null ; } PortletRequestDispatcher portletRequestDispatcher = null ; try { RequestDispatcher servletRequestDispatcher = servletContext . getRequestDispatcher ( path ) ; if ( servletRequestDispatcher != null ) { portletRequestDispatcher = new PortletRequestDispatcherImpl ( servletRequestDispatcher , path , false ) ; } else { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( ""No matching request dispatcher found for: "" + path ) ; } } } catch ( Exception ex ) { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( ""Failed to retrieve request dispatcher: "" + path , ex ) ; } portletRequestDispatcher = null ; } return portletRequestDispatcher ; } }","public class A { public PortletRequestDispatcher getRequestDispatcher ( ServletContext servletContext , PortletApplicationDefinition app , String path ) { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""PortletRequestDispatcher requested for path: "" + path + "" at context: "" + app . getContextPath ( ) ) ; } if ( path == null || ! path . startsWith ( ""/"" ) ) { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( ""Failed to retrieve PortletRequestDispatcher: path name must begin with a slash '/'."" ) ; } return null ; } PortletRequestDispatcher portletRequestDispatcher = null ; try { RequestDispatcher servletRequestDispatcher = servletContext . getRequestDispatcher ( path ) ; if ( servletRequestDispatcher != null ) { portletRequestDispatcher = new PortletRequestDispatcherImpl ( servletRequestDispatcher , path , false ) ; } else { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( ""No matching request dispatcher found for: "" + path ) ; } } } catch ( Exception ex ) { if ( LOG . isInfoEnabled ( ) ) { LOG . info ( ""Failed to retrieve PortletRequestDispatcher: "" + ex . getMessage ( ) ) ; } portletRequestDispatcher = null ; } return portletRequestDispatcher ; } }","LOG . info ( ""Failed to retrieve PortletRequestDispatcher: "" + ex . getMessage ( ) ) ;",Meaningful
"public class A { public static int calculateSizeOfValue ( Object value ) { if ( value instanceof String ) { return ( ( String ) value ) . length ( ) ; } else if ( value instanceof Boolean ) { return 4 ; } else if ( value instanceof Number || value instanceof Date ) { return 8 ; } else if ( value instanceof Collection ) { return calculateSizeOfCollection ( ( Collection < ? > ) value ) ; } else if ( value instanceof Map ) { return calculateSizeOfMap ( ( Map < ? , ? > ) value ) ; } else { log . warn ( ""Should only return size of value type. "" + value . getClass ( ) . getName ( ) ) ; return 100 ; } } }","public class A { public static int calculateSizeOfValue ( Object value ) { if ( value instanceof String ) { return ( ( String ) value ) . length ( ) ; } else if ( value instanceof Boolean ) { return 4 ; } else if ( value instanceof Number || value instanceof Date ) { return 8 ; } else if ( value instanceof Collection ) { return calculateSizeOfCollection ( ( Collection < ? > ) value ) ; } else if ( value instanceof Map ) { return calculateSizeOfMap ( ( Map < ? , ? > ) value ) ; } else { LOGGER . warn ( ""unhandled object to calculate size for: "" + value . getClass ( ) . getName ( ) + "", defaulting to 100"" ) ; return 100 ; } } }","LOGGER . warn ( ""unhandled object to calculate size for: "" + value . getClass ( ) . getName ( ) + "", defaulting to 100"" ) ;",Meaningful
"public class A { private Object convertAvroUnionToOrientDBField ( final String docf , final Schema fieldSchema , final OrientDBMapping . DocumentFieldType storeType , final Object value ) { Object result ; Schema . Type type0 = fieldSchema . getTypes ( ) . get ( 0 ) . getType ( ) ; Schema . Type type1 = fieldSchema . getTypes ( ) . get ( 1 ) . getType ( ) ; if ( ! type0 . equals ( type1 ) && ( type0 . equals ( Schema . Type . NULL ) || type1 . equals ( Schema . Type . NULL ) ) ) { Schema innerSchema = null ; if ( type0 . equals ( Schema . Type . NULL ) ) { innerSchema = fieldSchema . getTypes ( ) . get ( 1 ) ; } else { innerSchema = fieldSchema . getTypes ( ) . get ( 0 ) ; } LOG . debug ( ""Connecting to OrientDBStore {} with type {} to {}"" , docf , fieldSchema . getType ( ) , innerSchema . getType ( ) ) ; result = convertAvroFieldToOrientField ( docf , innerSchema , innerSchema . getType ( ) , storeType , value ) ; } else { throw new IllegalStateException ( ""OrientDBStore only supports Union of two types field."" ) ; } return result ; } }","public class A { private Object convertAvroUnionToOrientDBField ( final String docf , final Schema fieldSchema , final OrientDBMapping . DocumentFieldType storeType , final Object value ) { Object result ; Schema . Type type0 = fieldSchema . getTypes ( ) . get ( 0 ) . getType ( ) ; Schema . Type type1 = fieldSchema . getTypes ( ) . get ( 1 ) . getType ( ) ; if ( ! type0 . equals ( type1 ) && ( type0 . equals ( Schema . Type . NULL ) || type1 . equals ( Schema . Type . NULL ) ) ) { Schema innerSchema = null ; if ( type0 . equals ( Schema . Type . NULL ) ) { innerSchema = fieldSchema . getTypes ( ) . get ( 1 ) ; } else { innerSchema = fieldSchema . getTypes ( ) . get ( 0 ) ; } LOG . debug ( ""Transform value to ODocument (UNION), type:{}, storeType:{}"" , new Object [ ] { innerSchema . getType ( ) , type1 , storeType } ) ; result = convertAvroFieldToOrientField ( docf , innerSchema , innerSchema . getType ( ) , storeType , value ) ; } else { throw new IllegalStateException ( ""OrientDBStore only supports Union of two types field."" ) ; } return result ; } }","LOG . debug ( ""Transform value to ODocument (UNION), type:{}, storeType:{}"" , new Object [ ] { innerSchema . getType ( ) , type1 , storeType } ) ;",Meaningful
"public class A { protected static void clearReferences ( ) { Enumeration < Driver > drivers = DriverManager . getDrivers ( ) ; while ( drivers . hasMoreElements ( ) ) { Driver driver = drivers . nextElement ( ) ; if ( driver . getClass ( ) . getClassLoader ( ) == getInstance ( ) ) { try { DriverManager . deregisterDriver ( driver ) ; } catch ( SQLException e ) { log . warn ( ""SQL driver deregistration failed"" , e ) ; } } } for ( WeakReference < Class < ? > > refClazz : getInstance ( ) . cachedClasses . values ( ) ) { if ( refClazz == null ) { continue ; } Class < ? > clazz = refClazz . get ( ) ; if ( clazz != null && clazz . getName ( ) . contains ( ""openmrs"" ) ) { try { Field [ ] fields = clazz . getDeclaredFields ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { Field field = fields [ i ] ; int mods = field . getModifiers ( ) ; if ( field . getType ( ) . isPrimitive ( ) || ( field . getName ( ) . indexOf ( ""$"" ) != - 1 ) ) { continue ; } if ( Modifier . isStatic ( mods ) ) { try { if ( clazz . equals ( OpenmrsClassLoader . class ) && ""log"" . equals ( field . getName ( ) ) ) { continue ; } field . setAccessible ( true ) ; if ( Modifier . isFinal ( mods ) ) { if ( ! ( field . getType ( ) . getName ( ) . startsWith ( ""javax."" ) ) ) { nullInstance ( field . get ( null ) ) ; } } else { field . set ( null , null ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( ""Set field "" + field . getName ( ) + "" to null in class "" + clazz . getName ( ) ) ; } } } catch ( Exception t ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Could not set field "" + field . getName ( ) + "" to null in class "" + clazz . getName ( ) , t ) ; } } } } } catch ( Exception t ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Exception while clearing field "" + refClazz + "" for "" + clazz . getName ( ) , t ) ; } } } } OpenmrsClassLoader . log = null ; getInstance ( ) . cachedClasses . clear ( ) ; } }","public class A { protected static void clearReferences ( ) { Enumeration < Driver > drivers = DriverManager . getDrivers ( ) ; while ( drivers . hasMoreElements ( ) ) { Driver driver = drivers . nextElement ( ) ; if ( driver . getClass ( ) . getClassLoader ( ) == getInstance ( ) ) { try { DriverManager . deregisterDriver ( driver ) ; } catch ( SQLException e ) { log . warn ( ""SQL driver deregistration failed"" , e ) ; } } } for ( WeakReference < Class < ? > > refClazz : getInstance ( ) . cachedClasses . values ( ) ) { if ( refClazz == null ) { continue ; } Class < ? > clazz = refClazz . get ( ) ; if ( clazz != null && clazz . getName ( ) . contains ( ""openmrs"" ) ) { try { Field [ ] fields = clazz . getDeclaredFields ( ) ; for ( int i = 0 ; i < fields . length ; i ++ ) { Field field = fields [ i ] ; int mods = field . getModifiers ( ) ; if ( field . getType ( ) . isPrimitive ( ) || ( field . getName ( ) . indexOf ( ""$"" ) != - 1 ) ) { continue ; } if ( Modifier . isStatic ( mods ) ) { try { if ( clazz . equals ( OpenmrsClassLoader . class ) && ""log"" . equals ( field . getName ( ) ) ) { continue ; } field . setAccessible ( true ) ; if ( Modifier . isFinal ( mods ) ) { if ( ! ( field . getType ( ) . getName ( ) . startsWith ( ""javax."" ) ) ) { nullInstance ( field . get ( null ) ) ; } } else { field . set ( null , null ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( ""Set field "" + field . getName ( ) + "" to null in class "" + clazz . getName ( ) ) ; } } } catch ( Exception t ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Could not set field "" + field . getName ( ) + "" to null in class "" + clazz . getName ( ) , t ) ; } } } } } catch ( Exception t ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Could not clean fields for class "" + clazz . getName ( ) , t ) ; } } } } OpenmrsClassLoader . log = null ; getInstance ( ) . cachedClasses . clear ( ) ; } }","log . debug ( ""Could not clean fields for class "" + clazz . getName ( ) , t ) ;",Meaningful
"public class A { public DrillRpcFuture < Ack > cancelQuery ( QueryId id ) { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""[{}] Cancel query {}"" , id , QueryId . getQueryId ( ) ) ; } return client . send ( RpcType . CANCEL_QUERY , id , Ack . class ) ; } }","public class A { public DrillRpcFuture < Ack > cancelQuery ( QueryId id ) { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Cancelling query {}"" , QueryIdHelper . getQueryId ( id ) ) ; } return client . send ( RpcType . CANCEL_QUERY , id , Ack . class ) ; } }","logger . debug ( ""Cancelling query {}"" , QueryIdHelper . getQueryId ( id ) ) ;",Meaningful
"public class A { @ Override public List < SecurityRuleInstance > getAllSecurityRules ( Order order , SystemUser systemUser ) throws FogbowException { LOGGER . debug ( String . format ( Messages . Log . MAPPING_USER_OP_S , GET_ALL_INSTANCE_OPERATION , systemUser ) ) ; CloudUser cloudUser = this . mapperPlugin . map ( systemUser ) ; LOGGER . debug ( String . format ( Messages . Log . MAPPED_USER_S , cloudUser ) ) ; List < SecurityRuleInstance > securityRuleInstances = null ; String auditableResponse = null ; try { securityRuleInstances = doGetAllSecurityRules ( order , cloudUser ) ; LOGGER . debug ( String . format ( Messages . Log . RESPONSE_RECEIVED_S , securityRuleInstances ) ) ; auditableResponse = securityRuleInstances . toString ( ) ; } catch ( Throwable e ) { LOGGER . debug ( String . format ( Messages . Exception . GENERIC_EXCEPTION_S , e + e . getMessage ( ) ) ) ; auditableResponse = e . getClass ( ) . getName ( ) ; throw e ; } finally { auditRequest ( Operation . GET_ALL , order . getType ( ) , systemUser , auditableResponse ) ; } return securityRuleInstances ; } }","public class A { @ Override public List < SecurityRuleInstance > getAllSecurityRules ( Order order , SystemUser systemUser ) throws FogbowException { LOGGER . debug ( String . format ( Messages . Log . MAPPING_USER_OP_S , GET_ALL_SECURITY_RULES_OPERATION , order ) ) ; CloudUser cloudUser = this . mapperPlugin . map ( systemUser ) ; LOGGER . debug ( String . format ( Messages . Log . MAPPED_USER_S , cloudUser ) ) ; List < SecurityRuleInstance > securityRuleInstances = null ; String auditableResponse = null ; try { securityRuleInstances = doGetAllSecurityRules ( order , cloudUser ) ; LOGGER . debug ( String . format ( Messages . Log . RESPONSE_RECEIVED_S , securityRuleInstances ) ) ; auditableResponse = securityRuleInstances . toString ( ) ; } catch ( Throwable e ) { LOGGER . debug ( String . format ( Messages . Exception . GENERIC_EXCEPTION_S , e + e . getMessage ( ) ) ) ; auditableResponse = e . getClass ( ) . getName ( ) ; throw e ; } finally { auditRequest ( Operation . GET_ALL , order . getType ( ) , systemUser , auditableResponse ) ; } return securityRuleInstances ; } }","LOGGER . debug ( String . format ( Messages . Log . MAPPING_USER_OP_S , GET_ALL_SECURITY_RULES_OPERATION , order ) ) ;",Meaningful
"public class A { @ Override public void process ( InputStream in ) throws IOException { TextLineDemarcator demarcator = new TextLineDemarcator ( in ) ; SplitInfo splitInfo = null ; long startOffset = 0 ; long start = System . nanoTime ( ) ; try { if ( SplitText . this . headerLineCount > 0 ) { splitInfo = SplitText . this . computeHeader ( demarcator , startOffset , SplitText . this . headerLineCount , null , null ) ; if ( ( splitInfo != null ) && ( splitInfo . lineCount < SplitText . this . headerLineCount ) ) { error . set ( true ) ; getLogger ( ) . error ( ""Unable to split "" + sourceFlowFile + "" due to insufficient amount of header lines. Required "" + SplitText . this . headerLineCount + "" but was "" + splitInfo . lineCount + "". Routing to failure."" ) ; } } else if ( SplitText . this . headerMarker != null ) { splitInfo = SplitText . this . computeHeader ( demarcator , startOffset , Long . MAX_VALUE , SplitText . this . headerMarker . getBytes ( StandardCharsets . UTF_8 ) , null ) ; } headerSplitInfoRef . set ( splitInfo ) ; } catch ( IllegalStateException e ) { error . set ( true ) ; getLogger ( ) . error ( ""Unable to split "" + sourceFlowFile + "" due to insufficient amount of header lines."" , e ) ; } if ( ! error . get ( ) ) { if ( headerSplitInfoRef . get ( ) != null ) { startOffset = headerSplitInfoRef . get ( ) . length ; } long preAccumulatedLength = startOffset ; while ( ( splitInfo = SplitText . this . nextSplit ( demarcator , startOffset , SplitText . this . lineCount , splitInfo , preAccumulatedLength ) ) != null ) { computedSplitsInfo . add ( splitInfo ) ; startOffset += splitInfo . length ; } long stop = System . nanoTime ( ) ; if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( ""Computed splits in "" + ( stop - start ) + "" milliseconds."" ) ; } } } }","public class A { @ Override public void process ( InputStream in ) throws IOException { TextLineDemarcator demarcator = new TextLineDemarcator ( in ) ; SplitInfo splitInfo = null ; long startOffset = 0 ; long start = System . nanoTime ( ) ; try { if ( SplitText . this . headerLineCount > 0 ) { splitInfo = SplitText . this . computeHeader ( demarcator , startOffset , SplitText . this . headerLineCount , null , null ) ; if ( ( splitInfo != null ) && ( splitInfo . lineCount < SplitText . this . headerLineCount ) ) { error . set ( true ) ; getLogger ( ) . error ( ""Unable to split "" + sourceFlowFile + "" due to insufficient amount of header lines. Required "" + SplitText . this . headerLineCount + "" but was "" + splitInfo . lineCount + "". Routing to failure."" ) ; } } else if ( SplitText . this . headerMarker != null ) { splitInfo = SplitText . this . computeHeader ( demarcator , startOffset , Long . MAX_VALUE , SplitText . this . headerMarker . getBytes ( StandardCharsets . UTF_8 ) , null ) ; } headerSplitInfoRef . set ( splitInfo ) ; } catch ( IllegalStateException e ) { error . set ( true ) ; getLogger ( ) . error ( e . getMessage ( ) + "" Routing to failure."" , e ) ; } if ( ! error . get ( ) ) { if ( headerSplitInfoRef . get ( ) != null ) { startOffset = headerSplitInfoRef . get ( ) . length ; } long preAccumulatedLength = startOffset ; while ( ( splitInfo = SplitText . this . nextSplit ( demarcator , startOffset , SplitText . this . lineCount , splitInfo , preAccumulatedLength ) ) != null ) { computedSplitsInfo . add ( splitInfo ) ; startOffset += splitInfo . length ; } long stop = System . nanoTime ( ) ; if ( getLogger ( ) . isDebugEnabled ( ) ) { getLogger ( ) . debug ( ""Computed splits in "" + ( stop - start ) + "" milliseconds."" ) ; } } } }","getLogger ( ) . error ( e . getMessage ( ) + "" Routing to failure."" , e ) ;",Meaningful
"public class A { @ Override public RemoteFileOperations < FTPFile > createRemoteFileOperations ( ) throws Exception { FTPClient client = ftpClient ; if ( client == null ) { client = createFtpClient ( ) ; } if ( getBufferSize ( ) > 0 ) { client . setBufferSize ( getBufferSize ( ) ) ; } if ( getConfiguration ( ) . getConnectTimeout ( ) > - 1 ) { client . setConnectTimeout ( getConfiguration ( ) . getConnectTimeout ( ) ) ; } if ( getConfiguration ( ) . getSoTimeout ( ) > - 1 ) { soTimeout = getConfiguration ( ) . getSoTimeout ( ) ; } dataTimeout = getConfiguration ( ) . getTimeout ( ) ; if ( getConfiguration ( ) . getActivePortRange ( ) != null ) { String [ ] parts = getConfiguration ( ) . getActivePortRange ( ) . split ( ""-"" ) ; if ( parts . length != 2 ) { throw new IllegalArgumentException ( ""The option activePortRange should have syntax: min-max"" ) ; } int min = getCamelContext ( ) . getTypeConverter ( ) . mandatoryConvertTo ( int . class , parts [ 0 ] ) ; int max = getCamelContext ( ) . getTypeConverter ( ) . mandatoryConvertTo ( int . class , parts [ 1 ] ) ; LOG . debug ( ""Using active port range: {}-{}"" , min , max ) ; client . setActivePortRange ( min , max ) ; } if ( ftpClientParameters != null ) { Map < String , Object > localParameters = new HashMap < > ( ftpClientParameters ) ; Object timeout = localParameters . remove ( ""soTimeout"" ) ; if ( timeout != null ) { soTimeout = getCamelContext ( ) . getTypeConverter ( ) . convertTo ( int . class , timeout ) ; } timeout = localParameters . remove ( ""dataTimeout"" ) ; if ( timeout != null ) { dataTimeout = getCamelContext ( ) . getTypeConverter ( ) . convertTo ( int . class , timeout ) ; } setProperties ( client , localParameters ) ; } if ( ftpClientConfigParameters != null ) { if ( ftpClientConfig == null ) { ftpClientConfig = new FTPClientConfig ( ) ; } Map < String , Object > localConfigParameters = new HashMap < > ( ftpClientConfigParameters ) ; setProperties ( ftpClientConfig , localConfigParameters ) ; } if ( dataTimeout > 0 ) { client . setDataTimeout ( dataTimeout ) ; } if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""Created FTP Client: {}"" , client ) ; } FtpOperations operations = new FtpOperations ( client , getFtpClientConfig ( ) ) ; operations . setEndpoint ( this ) ; return operations ; } }","public class A { @ Override public RemoteFileOperations < FTPFile > createRemoteFileOperations ( ) throws Exception { FTPClient client = ftpClient ; if ( client == null ) { client = createFtpClient ( ) ; } if ( getBufferSize ( ) > 0 ) { client . setBufferSize ( getBufferSize ( ) ) ; } if ( getConfiguration ( ) . getConnectTimeout ( ) > - 1 ) { client . setConnectTimeout ( getConfiguration ( ) . getConnectTimeout ( ) ) ; } if ( getConfiguration ( ) . getSoTimeout ( ) > - 1 ) { soTimeout = getConfiguration ( ) . getSoTimeout ( ) ; } dataTimeout = getConfiguration ( ) . getTimeout ( ) ; if ( getConfiguration ( ) . getActivePortRange ( ) != null ) { String [ ] parts = getConfiguration ( ) . getActivePortRange ( ) . split ( ""-"" ) ; if ( parts . length != 2 ) { throw new IllegalArgumentException ( ""The option activePortRange should have syntax: min-max"" ) ; } int min = getCamelContext ( ) . getTypeConverter ( ) . mandatoryConvertTo ( int . class , parts [ 0 ] ) ; int max = getCamelContext ( ) . getTypeConverter ( ) . mandatoryConvertTo ( int . class , parts [ 1 ] ) ; LOG . debug ( ""Using active port range: {}-{}"" , min , max ) ; client . setActivePortRange ( min , max ) ; } if ( ftpClientParameters != null ) { Map < String , Object > localParameters = new HashMap < > ( ftpClientParameters ) ; Object timeout = localParameters . remove ( ""soTimeout"" ) ; if ( timeout != null ) { soTimeout = getCamelContext ( ) . getTypeConverter ( ) . convertTo ( int . class , timeout ) ; } timeout = localParameters . remove ( ""dataTimeout"" ) ; if ( timeout != null ) { dataTimeout = getCamelContext ( ) . getTypeConverter ( ) . convertTo ( int . class , timeout ) ; } setProperties ( client , localParameters ) ; } if ( ftpClientConfigParameters != null ) { if ( ftpClientConfig == null ) { ftpClientConfig = new FTPClientConfig ( ) ; } Map < String , Object > localConfigParameters = new HashMap < > ( ftpClientConfigParameters ) ; setProperties ( ftpClientConfig , localConfigParameters ) ; } if ( dataTimeout > 0 ) { client . setDataTimeout ( dataTimeout ) ; } if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""Created FTPClient[connectTimeout: {}, soTimeout: {}, dataTimeout: {}, bufferSize: {}"" + "", receiveDataSocketBufferSize: {}, sendDataSocketBufferSize: {}]: {}"" , client . getConnectTimeout ( ) , getSoTimeout ( ) , dataTimeout , client . getBufferSize ( ) , client . getReceiveDataSocketBufferSize ( ) , client . getSendDataSocketBufferSize ( ) , client ) ; } FtpOperations operations = new FtpOperations ( client , getFtpClientConfig ( ) ) ; operations . setEndpoint ( this ) ; return operations ; } }","LOG . debug ( ""Created FTPClient[connectTimeout: {}, soTimeout: {}, dataTimeout: {}, bufferSize: {}"" + "", receiveDataSocketBufferSize: {}, sendDataSocketBufferSize: {}]: {}"" , client . getConnectTimeout ( ) , getSoTimeout ( ) , dataTimeout , client . getBufferSize ( ) , client . getReceiveDataSocketBufferSize ( ) , client . getSendDataSocketBufferSize ( ) , client ) ;",Meaningful
"public class A { @ Override public void configure ( Context context ) { super . configure ( context ) ; serializerType = context . getString ( ""serializer"" , ""TEXT"" ) ; useRawLocalFileSystem = context . getBoolean ( ""hdfs.useRawLocalFileSystem"" , false ) ; serializerContext = new Context ( context . getSubProperties ( EventSerializer . CTX_PREFIX ) ) ; LOGGER . info ( ""Initializing HDFS Event serializer {}."" , serializerType ) ; } }","public class A { @ Override public void configure ( Context context ) { super . configure ( context ) ; serializerType = context . getString ( ""serializer"" , ""TEXT"" ) ; useRawLocalFileSystem = context . getBoolean ( ""hdfs.useRawLocalFileSystem"" , false ) ; serializerContext = new Context ( context . getSubProperties ( EventSerializer . CTX_PREFIX ) ) ; logger . info ( ""Serializer = "" + serializerType + "", UseRawLocalFileSystem = "" + useRawLocalFileSystem ) ; } }","logger . info ( ""Serializer = "" + serializerType + "", UseRawLocalFileSystem = "" + useRawLocalFileSystem ) ;",Meaningful
"public class A { public CalendarEventSet getEvents ( CalendarConfiguration calendarConfiguration , Interval interval , PortletRequest request ) throws CalendarException { String url = this . urlCreator . constructUrl ( calendarConfiguration , interval , request ) ; log . debug ( ""generated url: "" + url ) ; String intermediateCacheKey = cacheKeyGenerator . getKey ( calendarConfiguration , interval , request , cacheKeyPrefix . concat ( ""."" ) . concat ( url ) ) ; T calendar ; Element cachedCalendar = this . cache . get ( intermediateCacheKey ) ; if ( cachedCalendar == null ) { Credentials credentials = credentialsExtractor . getCredentials ( request ) ; InputStream stream = retrieveCalendarHttp ( url , credentials ) ; try { calendar = ( T ) contentProcessor . getIntermediateCalendar ( interval , stream ) ; } catch ( CalendarException e ) { log . error ( ""Calendar parsing exception: "" + e . getCause ( ) . getMessage ( ) + "" from calendar at "" + url ) ; throw e ; } cachedCalendar = new Element ( intermediateCacheKey , calendar ) ; this . cache . put ( cachedCalendar ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( ""Cached calendar was "" + cachedCalendar ) ; } } else { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Retrieving calendar from cache, key:"" + intermediateCacheKey ) ; } calendar = ( T ) cachedCalendar . getObjectValue ( ) ; } String processorCacheKey = getIntervalSpecificCacheKey ( intermediateCacheKey , interval ) ; CalendarEventSet eventSet ; Element cachedElement = this . cache . get ( processorCacheKey ) ; if ( cachedElement == null ) { Set < VEvent > events = contentProcessor . getEvents ( interval , calendar ) ; log . debug ( ""contentProcessor found "" + events . size ( ) + "" events"" ) ; int timeToLiveInSeconds = - 1 ; long currentTime = System . currentTimeMillis ( ) ; if ( cachedCalendar . getExpirationTime ( ) > currentTime ) { long timeToLiveInMilliseconds = cachedCalendar . getExpirationTime ( ) - currentTime ; timeToLiveInSeconds = ( int ) timeToLiveInMilliseconds / 1000 ; } eventSet = insertCalendarEventSetIntoCache ( this . cache , processorCacheKey , events , timeToLiveInSeconds > 0 ? timeToLiveInSeconds : - 1 ) ; } else { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Retrieving calendar event set from cache, key:"" + processorCacheKey ) ; } eventSet = ( CalendarEventSet ) cachedElement . getObjectValue ( ) ; } return eventSet ; } }","public class A { public CalendarEventSet getEvents ( CalendarConfiguration calendarConfiguration , Interval interval , PortletRequest request ) throws CalendarException { String url = this . urlCreator . constructUrl ( calendarConfiguration , interval , request ) ; log . debug ( ""generated url: "" + url ) ; String intermediateCacheKey = cacheKeyGenerator . getKey ( calendarConfiguration , interval , request , cacheKeyPrefix . concat ( ""."" ) . concat ( url ) ) ; T calendar ; Element cachedCalendar = this . cache . get ( intermediateCacheKey ) ; if ( cachedCalendar == null ) { Credentials credentials = credentialsExtractor . getCredentials ( request ) ; InputStream stream = retrieveCalendarHttp ( url , credentials ) ; try { calendar = ( T ) contentProcessor . getIntermediateCalendar ( interval , stream ) ; } catch ( CalendarException e ) { log . error ( ""Calendar parsing exception: "" + e . getCause ( ) . getMessage ( ) + "" from calendar at "" + url ) ; throw e ; } cachedCalendar = new Element ( intermediateCacheKey , calendar ) ; this . cache . put ( cachedCalendar ) ; if ( log . isDebugEnabled ( ) ) { log . debug ( ""Storing calendar cache, key:"" + intermediateCacheKey ) ; } } else { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Retrieving calendar from cache, key:"" + intermediateCacheKey ) ; } calendar = ( T ) cachedCalendar . getObjectValue ( ) ; } String processorCacheKey = getIntervalSpecificCacheKey ( intermediateCacheKey , interval ) ; CalendarEventSet eventSet ; Element cachedElement = this . cache . get ( processorCacheKey ) ; if ( cachedElement == null ) { Set < VEvent > events = contentProcessor . getEvents ( interval , calendar ) ; log . debug ( ""contentProcessor found "" + events . size ( ) + "" events"" ) ; int timeToLiveInSeconds = - 1 ; long currentTime = System . currentTimeMillis ( ) ; if ( cachedCalendar . getExpirationTime ( ) > currentTime ) { long timeToLiveInMilliseconds = cachedCalendar . getExpirationTime ( ) - currentTime ; timeToLiveInSeconds = ( int ) timeToLiveInMilliseconds / 1000 ; } eventSet = insertCalendarEventSetIntoCache ( this . cache , processorCacheKey , events , timeToLiveInSeconds > 0 ? timeToLiveInSeconds : - 1 ) ; } else { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Retrieving calendar event set from cache, key:"" + processorCacheKey ) ; } eventSet = ( CalendarEventSet ) cachedElement . getObjectValue ( ) ; } return eventSet ; } }","log . debug ( ""Storing calendar cache, key:"" + intermediateCacheKey ) ;",Meaningful
"public class A { private boolean initMembershipChangeScheduleForLeavingMember ( long commitIndex , CPMemberInfo leavingMember ) { List < CPGroupId > leavingGroupIds = new ArrayList < > ( ) ; List < CPGroupMembershipChange > changes = new ArrayList < > ( ) ; for ( CPGroupInfo group : groups . values ( ) ) { CPGroupId groupId = group . id ( ) ; if ( ! group . containsMember ( leavingMember . toRaftEndpoint ( ) ) || group . status ( ) == DESTROYED ) { continue ; } CPMemberInfo substitute = findSubstitute ( group ) ; RaftEndpoint substituteEndpoint = substitute != null ? substitute . toRaftEndpoint ( ) : null ; leavingGroupIds . add ( groupId ) ; changes . add ( new CPGroupMembershipChange ( groupId , group . getMembersCommitIndex ( ) , group . memberImpls ( ) , substituteEndpoint , leavingMember . toRaftEndpoint ( ) ) ) ; } if ( changes . isEmpty ( ) ) { if ( logger . isFineEnabled ( ) ) { logger . fine ( ""Removing "" + leavingMember + "" directly since it is not present in any CP group."" ) ; } removeActiveMember ( commitIndex , leavingMember ) ; return true ; } membershipChangeSchedule = MembershipChangeSchedule . forLeavingMember ( singletonList ( commitIndex ) , leavingMember , changes ) ; if ( logger . isFineEnabled ( ) ) { logger . info ( ""Removed "" + membershipChangeSchedule ) ; } else { logger . info ( leavingMember + "" will be removed from "" + leavingGroupIds ) ; } return false ; } }","public class A { private boolean initMembershipChangeScheduleForLeavingMember ( long commitIndex , CPMemberInfo leavingMember ) { List < CPGroupId > leavingGroupIds = new ArrayList < > ( ) ; List < CPGroupMembershipChange > changes = new ArrayList < > ( ) ; for ( CPGroupInfo group : groups . values ( ) ) { CPGroupId groupId = group . id ( ) ; if ( ! group . containsMember ( leavingMember . toRaftEndpoint ( ) ) || group . status ( ) == DESTROYED ) { continue ; } CPMemberInfo substitute = findSubstitute ( group ) ; RaftEndpoint substituteEndpoint = substitute != null ? substitute . toRaftEndpoint ( ) : null ; leavingGroupIds . add ( groupId ) ; changes . add ( new CPGroupMembershipChange ( groupId , group . getMembersCommitIndex ( ) , group . memberImpls ( ) , substituteEndpoint , leavingMember . toRaftEndpoint ( ) ) ) ; } if ( changes . isEmpty ( ) ) { if ( logger . isFineEnabled ( ) ) { logger . fine ( ""Removing "" + leavingMember + "" directly since it is not present in any CP group."" ) ; } removeActiveMember ( commitIndex , leavingMember ) ; return true ; } membershipChangeSchedule = MembershipChangeSchedule . forLeavingMember ( singletonList ( commitIndex ) , leavingMember , changes ) ; if ( logger . isFineEnabled ( ) ) { logger . info ( leavingMember + "" will be removed from "" + changes ) ; } else { logger . info ( leavingMember + "" will be removed from "" + leavingGroupIds ) ; } return false ; } }","logger . info ( leavingMember + "" will be removed from "" + changes ) ;",Meaningful
"public class A { @ Override @ POST @ Path ( UNITS_RULES_URI ) @ Consumes ( MediaType . APPLICATION_JSON ) @ Produces ( MediaType . APPLICATION_JSON ) public Response massUpdateUnitsRules ( MassUpdateUnitRuleRequest massUpdateUnitRuleRequest ) { JsonNode queryDsl = massUpdateUnitRuleRequest . getDslRequest ( ) . deepCopy ( ) ; RuleActions ruleActions = massUpdateUnitRuleRequest . getRuleActions ( ) ; LOGGER . debug ( ""Start mass updating archive units with Dsl query {}"" , queryDsl ) ; Status status ; try ( ProcessingManagementClient processingClient = processingManagementClientFactory . getClient ( ) ; LogbookOperationsClient logbookOperationsClient = logbookOperationsClientFactory . getClient ( ) ; WorkspaceClient workspaceClient = workspaceClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( queryDsl ) ; if ( ! isAuthorized ( ) ) { status = Status . UNAUTHORIZED ; return Response . status ( status ) . entity ( getErrorEntity ( status , WRITE_PERMISSION_NOT_ALLOWED ) ) . build ( ) ; } String operationId = getVitamSession ( ) . getRequestId ( ) ; final LogbookOperationParameters initParameters = LogbookParameterHelper . newLogbookOperationParameters ( GUIDReader . getGUID ( operationId ) , Contexts . MASS_UPDATE_UNIT_RULE . getEventType ( ) , GUIDReader . getGUID ( operationId ) , LogbookTypeProcess . MASS_UPDATE , STARTED , VitamLogbookMessages . getCodeOp ( Contexts . MASS_UPDATE_UNIT_RULE . getEventType ( ) , STARTED ) , GUIDReader . getGUID ( operationId ) ) ; addRightsStatementIdentifier ( initParameters ) ; logbookOperationsClient . create ( initParameters ) ; workspaceClient . createContainer ( operationId ) ; workspaceClient . putObject ( operationId , OperationContextMonitor . OperationContextFileName , writeToInpustream ( OperationContextModel . get ( massUpdateUnitRuleRequest ) ) ) ; workspaceClient . putObject ( operationId , QUERY_FILE , writeToInpustream ( applyAccessContractRestrictionForUnitForUpdate ( queryDsl , getVitamSession ( ) . getContract ( ) ) ) ) ; workspaceClient . putObject ( operationId , ""actions.json"" , writeToInpustream ( ruleActions ) ) ; OperationContextMonitor . compressInWorkspace ( workspaceClientFactory , operationId , Contexts . MASS_UPDATE_UNIT_RULE . getLogbookTypeProcess ( ) , OperationContextMonitor . OperationContextFileName ) ; processingClient . initVitamProcess ( operationId , Contexts . MASS_UPDATE_UNIT_RULE . name ( ) ) ; RequestResponse < ItemStatus > requestResponse = processingClient . executeOperationProcess ( operationId , Contexts . MASS_UPDATE_UNIT_RULE . name ( ) , RESUME . getValue ( ) ) ; return requestResponse . toResponse ( ) ; } catch ( ContentAddressableStorageServerException | LogbookClientBadRequestException | LogbookClientAlreadyExistsException | InvalidGuidOperationException | LogbookClientServerException | VitamClientException | InternalServerException | OperationContextException e ) { LOGGER . error ( ""An error occured while mass updating archive units"" , e ) ; return Response . status ( INTERNAL_SERVER_ERROR ) . entity ( getErrorEntity ( INTERNAL_SERVER_ERROR , e . getMessage ( ) ) ) . build ( ) ; } catch ( InvalidParseOperationException | InvalidCreateOperationException | BadRequestException e ) { LOGGER . error ( ""The update unit rules failed"" , e ) ; status = Status . BAD_REQUEST ; return Response . status ( status ) . entity ( getErrorEntity ( status , e . getMessage ( ) ) ) . build ( ) ; } } }","public class A { @ Override @ POST @ Path ( UNITS_RULES_URI ) @ Consumes ( MediaType . APPLICATION_JSON ) @ Produces ( MediaType . APPLICATION_JSON ) public Response massUpdateUnitsRules ( MassUpdateUnitRuleRequest massUpdateUnitRuleRequest ) { JsonNode queryDsl = massUpdateUnitRuleRequest . getDslRequest ( ) . deepCopy ( ) ; RuleActions ruleActions = massUpdateUnitRuleRequest . getRuleActions ( ) ; LOGGER . debug ( ""Start mass updating archive units with Dsl query {}"" , queryDsl ) ; Status status ; try ( ProcessingManagementClient processingClient = processingManagementClientFactory . getClient ( ) ; LogbookOperationsClient logbookOperationsClient = logbookOperationsClientFactory . getClient ( ) ; WorkspaceClient workspaceClient = workspaceClientFactory . getClient ( ) ) { SanityChecker . checkJsonAll ( queryDsl ) ; if ( ! isAuthorized ( ) ) { status = Status . UNAUTHORIZED ; return Response . status ( status ) . entity ( getErrorEntity ( status , WRITE_PERMISSION_NOT_ALLOWED ) ) . build ( ) ; } String operationId = getVitamSession ( ) . getRequestId ( ) ; final LogbookOperationParameters initParameters = LogbookParameterHelper . newLogbookOperationParameters ( GUIDReader . getGUID ( operationId ) , Contexts . MASS_UPDATE_UNIT_RULE . getEventType ( ) , GUIDReader . getGUID ( operationId ) , LogbookTypeProcess . MASS_UPDATE , STARTED , VitamLogbookMessages . getCodeOp ( Contexts . MASS_UPDATE_UNIT_RULE . getEventType ( ) , STARTED ) , GUIDReader . getGUID ( operationId ) ) ; addRightsStatementIdentifier ( initParameters ) ; logbookOperationsClient . create ( initParameters ) ; workspaceClient . createContainer ( operationId ) ; workspaceClient . putObject ( operationId , OperationContextMonitor . OperationContextFileName , writeToInpustream ( OperationContextModel . get ( massUpdateUnitRuleRequest ) ) ) ; workspaceClient . putObject ( operationId , QUERY_FILE , writeToInpustream ( applyAccessContractRestrictionForUnitForUpdate ( queryDsl , getVitamSession ( ) . getContract ( ) ) ) ) ; workspaceClient . putObject ( operationId , ""actions.json"" , writeToInpustream ( ruleActions ) ) ; OperationContextMonitor . compressInWorkspace ( workspaceClientFactory , operationId , Contexts . MASS_UPDATE_UNIT_RULE . getLogbookTypeProcess ( ) , OperationContextMonitor . OperationContextFileName ) ; processingClient . initVitamProcess ( operationId , Contexts . MASS_UPDATE_UNIT_RULE . name ( ) ) ; RequestResponse < ItemStatus > requestResponse = processingClient . executeOperationProcess ( operationId , Contexts . MASS_UPDATE_UNIT_RULE . name ( ) , RESUME . getValue ( ) ) ; return requestResponse . toResponse ( ) ; } catch ( ContentAddressableStorageServerException | LogbookClientBadRequestException | LogbookClientAlreadyExistsException | InvalidGuidOperationException | LogbookClientServerException | VitamClientException | InternalServerException | OperationContextException e ) { LOGGER . error ( ""An error occured while mass updating archive units"" , e ) ; return Response . status ( INTERNAL_SERVER_ERROR ) . entity ( getErrorEntity ( INTERNAL_SERVER_ERROR , e . getMessage ( ) ) ) . build ( ) ; } catch ( InvalidParseOperationException | InvalidCreateOperationException | BadRequestException e ) { LOGGER . error ( BAD_REQUEST_EXCEPTION , e ) ; status = Status . BAD_REQUEST ; return Response . status ( status ) . entity ( getErrorEntity ( status , e . getMessage ( ) ) ) . build ( ) ; } } }","LOGGER . error ( BAD_REQUEST_EXCEPTION , e ) ;",Meaningful
"public class A { @ Override public Long getExperimentCount ( List < String > projectIds ) { try ( Session session = modelDBHibernateUtil . getSessionFactory ( ) . openSession ( ) ) { Query < ? > query = session . createQuery ( GET_PROJECT_EXPERIMENTS_COUNT_HQL ) ; query . setParameterList ( ModelDBConstants . PROJECT_IDS , projectIds ) ; Long count = ( Long ) query . uniqueResult ( ) ; LOGGER . debug ( ""Project count: {}"" , count ) ; return count ; } catch ( Exception ex ) { if ( ModelDBUtils . needToRetry ( ex ) ) { return getExperimentCount ( projectIds ) ; } else { throw ex ; } } } }","public class A { @ Override public Long getExperimentCount ( List < String > projectIds ) { try ( Session session = modelDBHibernateUtil . getSessionFactory ( ) . openSession ( ) ) { Query < ? > query = session . createQuery ( GET_PROJECT_EXPERIMENTS_COUNT_HQL ) ; query . setParameterList ( ModelDBConstants . PROJECT_IDS , projectIds ) ; Long count = ( Long ) query . uniqueResult ( ) ; LOGGER . debug ( ""Experiment Count : {}"" , count ) ; return count ; } catch ( Exception ex ) { if ( ModelDBUtils . needToRetry ( ex ) ) { return getExperimentCount ( projectIds ) ; } else { throw ex ; } } } }","LOGGER . debug ( ""Experiment Count : {}"" , count ) ;",Meaningful
"public class A { @ SuppressWarnings ( ""unchecked"" ) public ArtifactDefinition createHeatEnvPlaceHolder ( List < ArtifactDefinition > createdArtifacts , ArtifactDefinition heatArtifact , String envType , String parentId , NodeTypeEnum parentType , String parentName , User user , Component component , Map < String , String > existingEnvVersions ) { Map < String , Object > deploymentResourceArtifacts = ConfigurationManager . getConfigurationManager ( ) . getConfiguration ( ) . getDeploymentResourceInstanceArtifacts ( ) ; if ( deploymentResourceArtifacts == null ) { log . debug ( ""no deployment artifacts are configured for generated artifacts"" ) ; throw new ByActionStatusComponentException ( ActionStatus . GENERAL_ERROR ) ; } Map < String , Object > placeHolderData = ( Map < String , Object > ) deploymentResourceArtifacts . get ( envType ) ; if ( placeHolderData == null ) { log . debug ( ""no placeHolder data found for {}"" , envType ) ; throw new ByActionStatusComponentException ( ActionStatus . GENERAL_ERROR ) ; } String envLabel = ( heatArtifact . getArtifactLabel ( ) + HEAT_ENV_SUFFIX ) . toLowerCase ( ) ; ArtifactDefinition createArtifactPlaceHolder = createArtifactPlaceHolderInfo ( parentId , envLabel , placeHolderData , user . getUserId ( ) , ArtifactGroupTypeEnum . DEPLOYMENT , true ) ; ArtifactDefinition artifactHeatEnv = createArtifactPlaceHolder ; artifactHeatEnv . setGeneratedFromId ( heatArtifact . getUniqueId ( ) ) ; artifactHeatEnv . setHeatParamsUpdateDate ( System . currentTimeMillis ( ) ) ; artifactHeatEnv . setTimeout ( 0 ) ; artifactHeatEnv . setIsFromCsar ( heatArtifact . getIsFromCsar ( ) ) ; buildHeatEnvFileName ( heatArtifact , artifactHeatEnv , placeHolderData ) ; handleEnvArtifactVersion ( artifactHeatEnv , existingEnvVersions ) ; ArtifactDefinition heatEnvPlaceholder ; if ( parentType != NodeTypeEnum . ResourceInstance ) { String checkSum = artifactToscaOperation . sortAndCalculateChecksumForHeatParameters ( heatArtifact . getHeatParameters ( ) ) ; artifactHeatEnv . setArtifactChecksum ( checkSum ) ; Either < ArtifactDefinition , StorageOperationStatus > addHeatEnvArtifact = addHeatEnvArtifact ( artifactHeatEnv , heatArtifact , component , parentType , parentId ) ; if ( addHeatEnvArtifact . isRight ( ) ) { log . debug ( ""failed to create heat env artifact on resource instance"" ) ; throw new ByResponseFormatComponentException ( componentsUtils . getResponseFormatForResourceInstance ( componentsUtils . convertFromStorageResponseForResourceInstance ( addHeatEnvArtifact . right ( ) . value ( ) , false ) , """" , null ) ) ; } heatEnvPlaceholder = createArtifactPlaceHolder ; } else { heatEnvPlaceholder = artifactHeatEnv ; artifactToscaOperation . generateUUID ( heatEnvPlaceholder , heatEnvPlaceholder . getArtifactVersion ( ) ) ; setHeatCurrentValuesOnHeatEnvDefaultValues ( heatArtifact , heatEnvPlaceholder ) ; } ComponentTypeEnum componentType = component . getComponentType ( ) ; if ( parentType == NodeTypeEnum . ResourceInstance ) { componentType = ComponentTypeEnum . RESOURCE_INSTANCE ; } createdArtifacts . add ( heatEnvPlaceholder ) ; componentsUtils . auditComponent ( componentsUtils . getResponseFormat ( ActionStatus . OK ) , user , component , AuditingActionEnum . ARTIFACT_UPLOAD , new ResourceCommonInfo ( parentName , componentType . getValue ( ) ) , ResourceVersionInfo . newBuilder ( ) . build ( ) , ResourceVersionInfo . newBuilder ( ) . artifactUuid ( heatEnvPlaceholder . getUniqueId ( ) ) . build ( ) , null , heatEnvPlaceholder , null ) ; return heatEnvPlaceholder ; } }","public class A { @ SuppressWarnings ( ""unchecked"" ) public ArtifactDefinition createHeatEnvPlaceHolder ( List < ArtifactDefinition > createdArtifacts , ArtifactDefinition heatArtifact , String envType , String parentId , NodeTypeEnum parentType , String parentName , User user , Component component , Map < String , String > existingEnvVersions ) { Map < String , Object > deploymentResourceArtifacts = ConfigurationManager . getConfigurationManager ( ) . getConfiguration ( ) . getDeploymentResourceInstanceArtifacts ( ) ; if ( deploymentResourceArtifacts == null ) { log . debug ( ""no deployment artifacts are configured for generated artifacts"" ) ; throw new ByActionStatusComponentException ( ActionStatus . GENERAL_ERROR ) ; } Map < String , Object > placeHolderData = ( Map < String , Object > ) deploymentResourceArtifacts . get ( envType ) ; if ( placeHolderData == null ) { log . debug ( ""no env type {} are configured for generated artifacts"" , envType ) ; throw new ByActionStatusComponentException ( ActionStatus . GENERAL_ERROR ) ; } String envLabel = ( heatArtifact . getArtifactLabel ( ) + HEAT_ENV_SUFFIX ) . toLowerCase ( ) ; ArtifactDefinition createArtifactPlaceHolder = createArtifactPlaceHolderInfo ( parentId , envLabel , placeHolderData , user . getUserId ( ) , ArtifactGroupTypeEnum . DEPLOYMENT , true ) ; ArtifactDefinition artifactHeatEnv = createArtifactPlaceHolder ; artifactHeatEnv . setGeneratedFromId ( heatArtifact . getUniqueId ( ) ) ; artifactHeatEnv . setHeatParamsUpdateDate ( System . currentTimeMillis ( ) ) ; artifactHeatEnv . setTimeout ( 0 ) ; artifactHeatEnv . setIsFromCsar ( heatArtifact . getIsFromCsar ( ) ) ; buildHeatEnvFileName ( heatArtifact , artifactHeatEnv , placeHolderData ) ; handleEnvArtifactVersion ( artifactHeatEnv , existingEnvVersions ) ; ArtifactDefinition heatEnvPlaceholder ; if ( parentType != NodeTypeEnum . ResourceInstance ) { String checkSum = artifactToscaOperation . sortAndCalculateChecksumForHeatParameters ( heatArtifact . getHeatParameters ( ) ) ; artifactHeatEnv . setArtifactChecksum ( checkSum ) ; Either < ArtifactDefinition , StorageOperationStatus > addHeatEnvArtifact = addHeatEnvArtifact ( artifactHeatEnv , heatArtifact , component , parentType , parentId ) ; if ( addHeatEnvArtifact . isRight ( ) ) { log . debug ( ""failed to create heat env artifact on resource instance"" ) ; throw new ByResponseFormatComponentException ( componentsUtils . getResponseFormatForResourceInstance ( componentsUtils . convertFromStorageResponseForResourceInstance ( addHeatEnvArtifact . right ( ) . value ( ) , false ) , """" , null ) ) ; } heatEnvPlaceholder = createArtifactPlaceHolder ; } else { heatEnvPlaceholder = artifactHeatEnv ; artifactToscaOperation . generateUUID ( heatEnvPlaceholder , heatEnvPlaceholder . getArtifactVersion ( ) ) ; setHeatCurrentValuesOnHeatEnvDefaultValues ( heatArtifact , heatEnvPlaceholder ) ; } ComponentTypeEnum componentType = component . getComponentType ( ) ; if ( parentType == NodeTypeEnum . ResourceInstance ) { componentType = ComponentTypeEnum . RESOURCE_INSTANCE ; } createdArtifacts . add ( heatEnvPlaceholder ) ; componentsUtils . auditComponent ( componentsUtils . getResponseFormat ( ActionStatus . OK ) , user , component , AuditingActionEnum . ARTIFACT_UPLOAD , new ResourceCommonInfo ( parentName , componentType . getValue ( ) ) , ResourceVersionInfo . newBuilder ( ) . build ( ) , ResourceVersionInfo . newBuilder ( ) . artifactUuid ( heatEnvPlaceholder . getUniqueId ( ) ) . build ( ) , null , heatEnvPlaceholder , null ) ; return heatEnvPlaceholder ; } }","log . debug ( ""no env type {} are configured for generated artifacts"" , envType ) ;",Meaningful
"public class A { public static void log ( String s ) { logger . info ( ""ActiveJDBC Instrumentation - "" + s ) ; } }",public class A { public static void log ( String s ) { logger . info ( s ) ; } },logger . info ( s ) ;,Meaningful
"public class A { public void updateRegisteredDeployments ( @ Observes SystemRepositoryChangedEvent changedEvent ) { logger . debug ( ""Received deployment changed event, processing..."" ) ; Collection < ConfigGroup > deployments = configurationService . getConfiguration ( ConfigType . DEPLOYMENT ) ; if ( deployments != null ) { List < String > processedDeployments = new ArrayList < String > ( ) ; for ( ConfigGroup deploymentConfig : deployments ) { String name = deploymentConfig . getName ( ) ; if ( ! this . registeredDeployments . containsKey ( name ) ) { try { logger . debug ( ""New deployment {} has been discovered and will be deployed"" , name ) ; DeploymentConfig deployment = deploymentFactory . newDeployment ( deploymentConfig ) ; addedDeploymentEvent . fire ( new DeploymentConfigChangedEvent ( deployment . getDeploymentUnit ( ) ) ) ; registeredDeployments . put ( deployment . getIdentifier ( ) , deployment ) ; logger . debug ( ""Deployment {} deployed successfully"" , name ) ; } catch ( RuntimeException e ) { logger . warn ( ""Deployment {} failed to deploy due to {}"" , name , e . getMessage ( ) , e ) ; } } processedDeployments . add ( name ) ; } Set < String > registeredDeploymedIds = registeredDeployments . keySet ( ) ; for ( String identifier : registeredDeploymedIds ) { if ( ! processedDeployments . contains ( identifier ) ) { try { logger . debug ( ""Undeployment {} processed"" , identifier ) ; DeploymentConfig deployment = registeredDeployments . remove ( identifier ) ; removedDeploymentEvent . fire ( new DeploymentConfigChangedEvent ( deployment . getDeploymentUnit ( ) ) ) ; logger . debug ( ""Deployment {} undeployed successfully"" , identifier ) ; } catch ( RuntimeException e ) { logger . warn ( ""Undeployment {} failed to deploy due to {}"" , identifier , e . getMessage ( ) , e ) ; } } } } } }","public class A { public void updateRegisteredDeployments ( @ Observes SystemRepositoryChangedEvent changedEvent ) { logger . debug ( ""Received deployment changed event, processing..."" ) ; Collection < ConfigGroup > deployments = configurationService . getConfiguration ( ConfigType . DEPLOYMENT ) ; if ( deployments != null ) { List < String > processedDeployments = new ArrayList < String > ( ) ; for ( ConfigGroup deploymentConfig : deployments ) { String name = deploymentConfig . getName ( ) ; if ( ! this . registeredDeployments . containsKey ( name ) ) { try { logger . debug ( ""New deployment {} has been discovered and will be deployed"" , name ) ; DeploymentConfig deployment = deploymentFactory . newDeployment ( deploymentConfig ) ; addedDeploymentEvent . fire ( new DeploymentConfigChangedEvent ( deployment . getDeploymentUnit ( ) ) ) ; registeredDeployments . put ( deployment . getIdentifier ( ) , deployment ) ; logger . debug ( ""Deployment {} deployed successfully"" , name ) ; } catch ( RuntimeException e ) { logger . warn ( ""Deployment {} failed to deploy due to {}"" , name , e . getMessage ( ) , e ) ; } } processedDeployments . add ( name ) ; } Set < String > registeredDeploymedIds = registeredDeployments . keySet ( ) ; for ( String identifier : registeredDeploymedIds ) { if ( ! processedDeployments . contains ( identifier ) ) { try { logger . debug ( ""New deployment {} has been discovered and will be deployed"" , identifier ) ; DeploymentConfig deployment = registeredDeployments . remove ( identifier ) ; removedDeploymentEvent . fire ( new DeploymentConfigChangedEvent ( deployment . getDeploymentUnit ( ) ) ) ; logger . debug ( ""Deployment {} undeployed successfully"" , identifier ) ; } catch ( RuntimeException e ) { logger . warn ( ""Undeployment {} failed to deploy due to {}"" , identifier , e . getMessage ( ) , e ) ; } } } } } }","logger . debug ( ""New deployment {} has been discovered and will be deployed"" , identifier ) ;",Meaningful
"public class A { @ SuppressWarnings ( ""unchecked"" ) public < T > T invoke ( boolean sync , Object jsThis , Object function , Object ... arguments ) { FutureTask < T > task = new FutureTask < > ( ( ) -> { context . enter ( ) ; try { Value self = Value . asValue ( jsThis ) ; Value funktion = Value . asValue ( function ) ; if ( funktion . canExecute ( ) ) { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Invoking function {}"" , function ) ; } return ( T ) funktion . getMember ( ""call"" ) . execute ( coalesce ( self , arguments ) ) ; } String name = funktion . asString ( ) ; if ( self . hasMember ( name ) ) { Value member = self . getMember ( name ) ; if ( member . canExecute ( ) ) { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Invoking object+function {}"" , name ) ; } return ( T ) member . getMember ( ""call"" ) . execute ( coalesce ( self , arguments ) ) ; } else { logger . info ( ""Notvoking object+function {}"" , name ) ; return null ; } } else { logger . info ( ""{} not a member in {}"" , name , jsThis ) ; return null ; } } catch ( Throwable x ) { logger . info ( ""Exception while trying to invoke "" + function , x ) ; throw x ; } finally { context . leave ( ) ; } } ) ; submit ( task ) ; if ( ! sync ) { return null ; } return result ( task ) ; } }","public class A { @ SuppressWarnings ( ""unchecked"" ) public < T > T invoke ( boolean sync , Object jsThis , Object function , Object ... arguments ) { FutureTask < T > task = new FutureTask < > ( ( ) -> { context . enter ( ) ; try { Value self = Value . asValue ( jsThis ) ; Value funktion = Value . asValue ( function ) ; if ( funktion . canExecute ( ) ) { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Invoking function {}"" , function ) ; } return ( T ) funktion . getMember ( ""call"" ) . execute ( coalesce ( self , arguments ) ) ; } String name = funktion . asString ( ) ; if ( self . hasMember ( name ) ) { Value member = self . getMember ( name ) ; if ( member . canExecute ( ) ) { if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Invoking object+function {}"" , name ) ; } return ( T ) member . getMember ( ""call"" ) . execute ( coalesce ( self , arguments ) ) ; } else { logger . info ( ""Member {} not a function in {}"" , name , jsThis ) ; return null ; } } else { logger . info ( ""{} not a member in {}"" , name , jsThis ) ; return null ; } } catch ( Throwable x ) { logger . info ( ""Exception while trying to invoke "" + function , x ) ; throw x ; } finally { context . leave ( ) ; } } ) ; submit ( task ) ; if ( ! sync ) { return null ; } return result ( task ) ; } }","logger . info ( ""Member {} not a function in {}"" , name , jsThis ) ;",Meaningful
"public class A { protected boolean invokeOn ( long keyId ) throws InterruptedException { try { if ( transactionSize > 0 && remainingTxOps == transactionSize ) { txStartOperationId = operationId ; txStartKeyId = keyId ; txStartRandSeed = Utils . getRandomSeed ( keySelectorRandom ) ; startTransaction ( ) ; } boolean txBreakRequest = false ; try { if ( ! invokeLogic ( keyId ) ) return false ; } catch ( BreakTxRequest request ) { txBreakRequest = true ; } lastSuccessfulOpTimestamp = TimeService . currentTimeMillis ( ) ; if ( transactionSize <= 0 && operationId % manager . getLogLogicConfiguration ( ) . getCounterUpdatePeriod ( ) == 0 ) { writeStressorLastOperation ( ) ; lastConfirmedOperation = operationId ; } if ( transactionSize > 0 ) { remainingTxOps -- ; if ( remainingTxOps <= 0 || txBreakRequest ) { try { ongoingTx . commit ( ) ; lastSuccessfulTxTimestamp = TimeService . currentTimeMillis ( ) ; txFailedAttempts = 0 ; } catch ( Exception e ) { log . debugf ( ""Transaction %s was rolled back, restarting from operation %d"" , ongoingTx , txStartOperationId ) ; txRolledBack = true ; afterRollback ( ) ; return false ; } finally { remainingTxOps = transactionSize ; clearTransaction ( ) ; } if ( stressor . isTerminated ( ) ) { log . debugf ( ""Stressor %s is about to terminate, not executing delayed removes"" , stressor . getStatus ( ) ) ; return false ; } afterCommit ( ) ; if ( stressor . isTerminated ( ) ) { log . debugf ( ""Stressor %s is about to terminate, not writing the last operation %d"" , stressor . getStatus ( ) , operationId ) ; return false ; } if ( txBreakRequest ) { log . debugf ( ""Transaction was committed sooner, retrying operation %d"" , operationId ) ; return false ; } try { startTransaction ( ) ; writeStressorLastOperation ( ) ; ongoingTx . commit ( ) ; lastConfirmedOperation = operationId ; } catch ( Exception e ) { log . error ( ""Cannot write stressor last operation"" , e ) ; } finally { clearTransaction ( ) ; } } } return true ; } catch ( Exception e ) { InterruptedException ie = Utils . findThrowableCauseByClass ( e , InterruptedException . class ) ; if ( ie != null ) { throw ie ; } else if ( e . getClass ( ) . getName ( ) . contains ( ""SuspectException"" ) ) { log . error ( ""Cache operation error"" , e ) ; } else { log . error ( ""Cache operation error"" , e ) ; } if ( transactionSize > 0 && ongoingTx != null ) { try { ongoingTx . rollback ( ) ; log . errorf ( ""Transaction %s rolled back"" , ongoingTx ) ; } catch ( Exception e1 ) { log . errorf ( e1 , ""Error while rolling back transaction %s"" , ongoingTx ) ; } finally { log . debugf ( ""Restarting from operation %d, current operation %d"" , txStartOperationId , operationId ) ; clearTransaction ( ) ; remainingTxOps = transactionSize ; txRolledBack = true ; afterRollback ( ) ; } } return false ; } } }","public class A { protected boolean invokeOn ( long keyId ) throws InterruptedException { try { if ( transactionSize > 0 && remainingTxOps == transactionSize ) { txStartOperationId = operationId ; txStartKeyId = keyId ; txStartRandSeed = Utils . getRandomSeed ( keySelectorRandom ) ; startTransaction ( ) ; } boolean txBreakRequest = false ; try { if ( ! invokeLogic ( keyId ) ) return false ; } catch ( BreakTxRequest request ) { txBreakRequest = true ; } lastSuccessfulOpTimestamp = TimeService . currentTimeMillis ( ) ; if ( transactionSize <= 0 && operationId % manager . getLogLogicConfiguration ( ) . getCounterUpdatePeriod ( ) == 0 ) { writeStressorLastOperation ( ) ; lastConfirmedOperation = operationId ; } if ( transactionSize > 0 ) { remainingTxOps -- ; if ( remainingTxOps <= 0 || txBreakRequest ) { try { ongoingTx . commit ( ) ; lastSuccessfulTxTimestamp = TimeService . currentTimeMillis ( ) ; txFailedAttempts = 0 ; } catch ( Exception e ) { log . debugf ( ""Transaction %s was rolled back, restarting from operation %d"" , ongoingTx , txStartOperationId ) ; txRolledBack = true ; afterRollback ( ) ; return false ; } finally { remainingTxOps = transactionSize ; clearTransaction ( ) ; } if ( stressor . isTerminated ( ) ) { log . debugf ( ""Stressor %s is about to terminate, not executing delayed removes"" , stressor . getStatus ( ) ) ; return false ; } afterCommit ( ) ; if ( stressor . isTerminated ( ) ) { log . debugf ( ""Stressor %s is about to terminate, not writing the last operation %d"" , stressor . getStatus ( ) , operationId ) ; return false ; } if ( txBreakRequest ) { log . debugf ( ""Transaction was committed sooner, retrying operation %d"" , operationId ) ; return false ; } try { startTransaction ( ) ; writeStressorLastOperation ( ) ; ongoingTx . commit ( ) ; lastConfirmedOperation = operationId ; } catch ( Exception e ) { log . error ( ""Cannot write stressor last operation"" , e ) ; } finally { clearTransaction ( ) ; } } } return true ; } catch ( Exception e ) { InterruptedException ie = Utils . findThrowableCauseByClass ( e , InterruptedException . class ) ; if ( ie != null ) { throw ie ; } else if ( e . getClass ( ) . getName ( ) . contains ( ""SuspectException"" ) ) { log . error ( ""Request failed due to SuspectException: "" + e . getMessage ( ) ) ; } else { log . error ( ""Cache operation error"" , e ) ; } if ( transactionSize > 0 && ongoingTx != null ) { try { ongoingTx . rollback ( ) ; log . errorf ( ""Transaction %s rolled back"" , ongoingTx ) ; } catch ( Exception e1 ) { log . errorf ( e1 , ""Error while rolling back transaction %s"" , ongoingTx ) ; } finally { log . debugf ( ""Restarting from operation %d, current operation %d"" , txStartOperationId , operationId ) ; clearTransaction ( ) ; remainingTxOps = transactionSize ; txRolledBack = true ; afterRollback ( ) ; } } return false ; } } }","log . error ( ""Request failed due to SuspectException: "" + e . getMessage ( ) ) ;",Meaningful
"public class A { public void waitElement ( Integer index ) { final String locator = getLocatorWithParameters ( getElementMap ( ) . locator ( ) [ index ] . toString ( ) ) ; final By by = ByConverter . convert ( getElementMap ( ) . locatorType ( ) , locator ) ; final long startedTime = GregorianCalendar . getInstance ( ) . getTimeInMillis ( ) ; Date startWaitTotal = GregorianCalendar . getInstance ( ) . getTime ( ) ; while ( true ) { try { waitThreadSleep ( BehaveConfig . getRunner_ScreenMinWait ( ) ) ; getDriver ( ) . manage ( ) . timeouts ( ) . implicitlyWait ( getImplicitlyWaitTimeoutInMilliseconds ( ) , TimeUnit . MILLISECONDS ) ; waitLoading ( ) ; Date endLoading = GregorianCalendar . getInstance ( ) . getTime ( ) ; Long diffLoading = endLoading . getTime ( ) - startWaitTotal . getTime ( ) ; logStatistics ( ""O tempo para esperar o LOADING foi de ["" + diffLoading + ""ms]"" ) ; getDriver ( ) . manage ( ) . timeouts ( ) . implicitlyWait ( getImplicitlyWaitTimeoutInMilliseconds ( ) , TimeUnit . MILLISECONDS ) ; waitClickable ( by ) ; Date endClickable = GregorianCalendar . getInstance ( ) . getTime ( ) ; Long diffClickable = endClickable . getTime ( ) - endLoading . getTime ( ) ; Long diffTotal = endClickable . getTime ( ) - startWaitTotal . getTime ( ) ; logStatistics ( ""O tempo para esperar o CLICKABLE foi de ["" + diffClickable + ""ms] e total foi de ["" + diffTotal + ""ms]"" ) ; getDriver ( ) . manage ( ) . timeouts ( ) . implicitlyWait ( getImplicitlyWaitTimeoutInMilliseconds ( ) , TimeUnit . MILLISECONDS ) ; waitVisibility ( by ) ; Date endVisibility = GregorianCalendar . getInstance ( ) . getTime ( ) ; Long diffVisibility = endVisibility . getTime ( ) - endClickable . getTime ( ) ; diffTotal = endVisibility . getTime ( ) - startWaitTotal . getTime ( ) ; logStatistics ( ""O tempo para esperar o VISIBILITY foi de ["" + diffVisibility + ""ms] e total foi de ["" + diffTotal + ""ms]"" ) ; break ; } catch ( Exception e ) { log . warn ( ""Exception while trying to make new 'sup_ancestors()'."" ) ; log . warn ( e ) ; } finally { getDriver ( ) . manage ( ) . timeouts ( ) . implicitlyWait ( BehaveConfig . getRunner_ScreenMaxWait ( ) , TimeUnit . MILLISECONDS ) ; if ( GregorianCalendar . getInstance ( ) . getTimeInMillis ( ) - startedTime > BehaveConfig . getRunner_ScreenMaxWait ( ) ) { throw new BehaveException ( message . getString ( ""exception-element-not-found"" , getElementMap ( ) . name ( ) ) ) ; } } } } }","public class A { public void waitElement ( Integer index ) { final String locator = getLocatorWithParameters ( getElementMap ( ) . locator ( ) [ index ] . toString ( ) ) ; final By by = ByConverter . convert ( getElementMap ( ) . locatorType ( ) , locator ) ; final long startedTime = GregorianCalendar . getInstance ( ) . getTimeInMillis ( ) ; Date startWaitTotal = GregorianCalendar . getInstance ( ) . getTime ( ) ; while ( true ) { try { waitThreadSleep ( BehaveConfig . getRunner_ScreenMinWait ( ) ) ; getDriver ( ) . manage ( ) . timeouts ( ) . implicitlyWait ( getImplicitlyWaitTimeoutInMilliseconds ( ) , TimeUnit . MILLISECONDS ) ; waitLoading ( ) ; Date endLoading = GregorianCalendar . getInstance ( ) . getTime ( ) ; Long diffLoading = endLoading . getTime ( ) - startWaitTotal . getTime ( ) ; logStatistics ( ""O tempo para esperar o LOADING foi de ["" + diffLoading + ""ms]"" ) ; getDriver ( ) . manage ( ) . timeouts ( ) . implicitlyWait ( getImplicitlyWaitTimeoutInMilliseconds ( ) , TimeUnit . MILLISECONDS ) ; waitClickable ( by ) ; Date endClickable = GregorianCalendar . getInstance ( ) . getTime ( ) ; Long diffClickable = endClickable . getTime ( ) - endLoading . getTime ( ) ; Long diffTotal = endClickable . getTime ( ) - startWaitTotal . getTime ( ) ; logStatistics ( ""O tempo para esperar o CLICKABLE foi de ["" + diffClickable + ""ms] e total foi de ["" + diffTotal + ""ms]"" ) ; getDriver ( ) . manage ( ) . timeouts ( ) . implicitlyWait ( getImplicitlyWaitTimeoutInMilliseconds ( ) , TimeUnit . MILLISECONDS ) ; waitVisibility ( by ) ; Date endVisibility = GregorianCalendar . getInstance ( ) . getTime ( ) ; Long diffVisibility = endVisibility . getTime ( ) - endClickable . getTime ( ) ; diffTotal = endVisibility . getTime ( ) - startWaitTotal . getTime ( ) ; logStatistics ( ""O tempo para esperar o VISIBILITY foi de ["" + diffVisibility + ""ms] e total foi de ["" + diffTotal + ""ms]"" ) ; break ; } catch ( Exception e ) { log . warn ( ""Erro no Wait Element"" ) ; log . warn ( e ) ; } finally { getDriver ( ) . manage ( ) . timeouts ( ) . implicitlyWait ( BehaveConfig . getRunner_ScreenMaxWait ( ) , TimeUnit . MILLISECONDS ) ; if ( GregorianCalendar . getInstance ( ) . getTimeInMillis ( ) - startedTime > BehaveConfig . getRunner_ScreenMaxWait ( ) ) { throw new BehaveException ( message . getString ( ""exception-element-not-found"" , getElementMap ( ) . name ( ) ) ) ; } } } } }","log . warn ( ""Erro no Wait Element"" ) ;",Meaningful
"public class A { public static void getEndpointProperties ( CamelContext camelContext , FacebookEndpointConfiguration configuration , Map < String , Object > properties ) { BeanIntrospection beanIntrospection = camelContext . adapt ( ExtendedCamelContext . class ) . getBeanIntrospection ( ) ; if ( beanIntrospection . getProperties ( configuration , properties , null , false ) ) { final Set < String > names = properties . keySet ( ) ; names . removeAll ( COMPONENT_CONFIG_FIELDS ) ; } if ( LOG . isDebugEnabled ( ) ) { final Set < String > names = properties . keySet ( ) ; LOG . debug ( ""Component configuration: {}"" , names ) ; } } }","public class A { public static void getEndpointProperties ( CamelContext camelContext , FacebookEndpointConfiguration configuration , Map < String , Object > properties ) { BeanIntrospection beanIntrospection = camelContext . adapt ( ExtendedCamelContext . class ) . getBeanIntrospection ( ) ; if ( beanIntrospection . getProperties ( configuration , properties , null , false ) ) { final Set < String > names = properties . keySet ( ) ; names . removeAll ( COMPONENT_CONFIG_FIELDS ) ; } if ( LOG . isDebugEnabled ( ) ) { final Set < String > names = properties . keySet ( ) ; LOG . debug ( ""Found endpoint properties {}"" , names . retainAll ( ENDPOINT_CONFIG_FIELDS ) ) ; } } }","LOG . debug ( ""Found endpoint properties {}"" , names . retainAll ( ENDPOINT_CONFIG_FIELDS ) ) ;",Meaningful
"public class A { @ DELETE @ Path ( ""{id}"" ) @ Timed @ ApiOperation ( value = ""Delete index set"" ) @ AuditEvent ( type = AuditEventTypes . INDEX_SET_DELETE ) @ ApiResponses ( value = { @ ApiResponse ( code = 403 , message = ""Unauthorized"" ) , @ ApiResponse ( code = 404 , message = ""Index set not found"" ) , } ) public void delete ( @ ApiParam ( name = ""id"" , required = true ) @ PathParam ( ""id"" ) String id , @ ApiParam ( name = ""delete_indices"" ) @ QueryParam ( ""delete_indices"" ) @ DefaultValue ( ""true"" ) boolean deleteIndices ) { checkPermission ( RestPermissions . INDEXSETS_DELETE , id ) ; final IndexSet indexSet = getIndexSet ( indexSetRegistry , id ) ; final IndexSet defaultIndexSet = indexSetRegistry . getDefault ( ) ; if ( indexSet . equals ( defaultIndexSet ) ) { throw new BadRequestException ( ""Default index set <"" + indexSet . getConfig ( ) . id ( ) + ""> cannot be deleted!"" ) ; } if ( indexSetService . delete ( id ) == 0 ) { throw new NotFoundException ( ""Couldn't delete index set with ID <"" + id + "">"" ) ; } else { if ( deleteIndices ) { try { systemJobManager . submit ( indexSetCleanupJobFactory . create ( indexSet ) ) ; } catch ( SystemJobConcurrencyException e ) { LOG . error ( ""Unable to delete index set with ID: {}"" , id , e ) ; } } } } }","public class A { @ DELETE @ Path ( ""{id}"" ) @ Timed @ ApiOperation ( value = ""Delete index set"" ) @ AuditEvent ( type = AuditEventTypes . INDEX_SET_DELETE ) @ ApiResponses ( value = { @ ApiResponse ( code = 403 , message = ""Unauthorized"" ) , @ ApiResponse ( code = 404 , message = ""Index set not found"" ) , } ) public void delete ( @ ApiParam ( name = ""id"" , required = true ) @ PathParam ( ""id"" ) String id , @ ApiParam ( name = ""delete_indices"" ) @ QueryParam ( ""delete_indices"" ) @ DefaultValue ( ""true"" ) boolean deleteIndices ) { checkPermission ( RestPermissions . INDEXSETS_DELETE , id ) ; final IndexSet indexSet = getIndexSet ( indexSetRegistry , id ) ; final IndexSet defaultIndexSet = indexSetRegistry . getDefault ( ) ; if ( indexSet . equals ( defaultIndexSet ) ) { throw new BadRequestException ( ""Default index set <"" + indexSet . getConfig ( ) . id ( ) + ""> cannot be deleted!"" ) ; } if ( indexSetService . delete ( id ) == 0 ) { throw new NotFoundException ( ""Couldn't delete index set with ID <"" + id + "">"" ) ; } else { if ( deleteIndices ) { try { systemJobManager . submit ( indexSetCleanupJobFactory . create ( indexSet ) ) ; } catch ( SystemJobConcurrencyException e ) { LOG . error ( ""Error running system job"" , e ) ; } } } } }","LOG . error ( ""Error running system job"" , e ) ;",Meaningful
"public class A { @ Override public void next ( ) throws IOException { log . info ( ""starting merge gc next key"" ) ; boolean yielded = false ; yieldNextKey . set ( ! yieldNextKey . get ( ) ) ; if ( yield . isPresent ( ) && yieldNextKey . get ( ) ) { yielded = true ; yieldNexts . incrementAndGet ( ) ; yield . get ( ) . yield ( getTopKey ( ) . followingKey ( PartialKey . ROW_COLFAM_COLQUAL_COLVIS_TIME ) ) ; log . info ( ""end YieldingIterator.next: yielded at "" + getTopKey ( ) ) ; } if ( ! yielded ) { super . next ( ) ; log . info ( ""end YieldingIterator.next: "" + ( hasTop ( ) ? getTopKey ( ) + "" "" + getTopValue ( ) : ""no top"" ) ) ; } } }","public class A { @ Override public void next ( ) throws IOException { log . info ( ""start YieldingIterator.next: "" + getTopValue ( ) ) ; boolean yielded = false ; yieldNextKey . set ( ! yieldNextKey . get ( ) ) ; if ( yield . isPresent ( ) && yieldNextKey . get ( ) ) { yielded = true ; yieldNexts . incrementAndGet ( ) ; yield . get ( ) . yield ( getTopKey ( ) . followingKey ( PartialKey . ROW_COLFAM_COLQUAL_COLVIS_TIME ) ) ; log . info ( ""end YieldingIterator.next: yielded at "" + getTopKey ( ) ) ; } if ( ! yielded ) { super . next ( ) ; log . info ( ""end YieldingIterator.next: "" + ( hasTop ( ) ? getTopKey ( ) + "" "" + getTopValue ( ) : ""no top"" ) ) ; } } }","log . info ( ""start YieldingIterator.next: "" + getTopValue ( ) ) ;",Meaningful
"public class A { @ GET @ Produces ( MediaType . APPLICATION_XML ) public Response getProviders ( ) { logger . debug ( ""StartOf getProviders - REQUEST for /providers"" ) ; ProviderHelper providerRestService = getProviderHelper ( ) ; String serializedProviders = null ; try { serializedProviders = providerRestService . getProviders ( ) ; } catch ( HelperException e ) { logger . info ( ""getProviders exception:"" + e . getMessage ( ) ) ; return buildResponse ( e ) ; } logger . debug ( ""EndOf getTemplates"" ) ; return buildResponse ( 200 , serializedProviders ) ; } }","public class A { @ GET @ Produces ( MediaType . APPLICATION_XML ) public Response getProviders ( ) { logger . debug ( ""StartOf getProviders - REQUEST for /providers"" ) ; ProviderHelper providerRestService = getProviderHelper ( ) ; String serializedProviders = null ; try { serializedProviders = providerRestService . getProviders ( ) ; } catch ( HelperException e ) { logger . info ( ""getTemplates exception:"" + e . getMessage ( ) ) ; return buildResponse ( e ) ; } logger . debug ( ""EndOf getTemplates"" ) ; return buildResponse ( 200 , serializedProviders ) ; } }","logger . info ( ""getTemplates exception:"" + e . getMessage ( ) ) ;",Meaningful
"public class A { private void scanForSystemPropertyClasses ( ) { try { final Set < ClassPath . ClassInfo > classesInPackage = ClassPath . from ( getClass ( ) . getClassLoader ( ) ) . getTopLevelClassesRecursive ( ""org.jivesoftware.openfire"" ) ; for ( final ClassPath . ClassInfo classInfo : classesInPackage ) { final String className = classInfo . getName ( ) ; if ( ! CLASSES_TO_EXCLUDE . contains ( className ) ) { try { final Class < ? > clazz = classInfo . load ( ) ; final Field [ ] fields = clazz . getDeclaredFields ( ) ; for ( final Field field : fields ) { if ( field . getType ( ) . equals ( SystemProperty . class ) ) { try { field . setAccessible ( true ) ; logger . info ( ""Accessing SystemProperty field {}#{}"" , className , field . getName ( ) ) ; field . get ( null ) ; } catch ( final Throwable t ) { logger . warn ( ""Unable to access field {}#{}"" , className , field . getName ( ) , t ) ; } } } } catch ( final Throwable t ) { logger . warn ( ""Unable to load property {} for component {}"" , className , classInfo . getName ( ) , t ) ; } } } } catch ( final Throwable t ) { logger . warn ( ""Unable to scan classpath for SystemProperty classes"" , t ) ; } } }","public class A { private void scanForSystemPropertyClasses ( ) { try { final Set < ClassPath . ClassInfo > classesInPackage = ClassPath . from ( getClass ( ) . getClassLoader ( ) ) . getTopLevelClassesRecursive ( ""org.jivesoftware.openfire"" ) ; for ( final ClassPath . ClassInfo classInfo : classesInPackage ) { final String className = classInfo . getName ( ) ; if ( ! CLASSES_TO_EXCLUDE . contains ( className ) ) { try { final Class < ? > clazz = classInfo . load ( ) ; final Field [ ] fields = clazz . getDeclaredFields ( ) ; for ( final Field field : fields ) { if ( field . getType ( ) . equals ( SystemProperty . class ) ) { try { field . setAccessible ( true ) ; logger . info ( ""Accessing SystemProperty field {}#{}"" , className , field . getName ( ) ) ; field . get ( null ) ; } catch ( final Throwable t ) { logger . warn ( ""Unable to access field {}#{}"" , className , field . getName ( ) , t ) ; } } } } catch ( final Throwable t ) { logger . warn ( ""Unable to load class {}"" , className , t ) ; } } } } catch ( final Throwable t ) { logger . warn ( ""Unable to scan classpath for SystemProperty classes"" , t ) ; } } }","logger . warn ( ""Unable to load class {}"" , className , t ) ;",Meaningful
"public class A { protected JsonElement invokeRPC ( String methodName , Object [ ] args ) { int id = rand . nextInt ( Integer . MAX_VALUE ) ; JsonObject req = new JsonObject ( ) ; req . addProperty ( ""jsonrpc"" , ""2.0"" ) ; req . addProperty ( ""id"" , id ) ; req . addProperty ( ""method"" , methodName ) ; JsonElement result = null ; JsonArray params = new JsonArray ( ) ; if ( args != null ) { for ( Object o : args ) { params . add ( gson . toJsonTree ( o ) ) ; } } req . add ( ""params"" , params ) ; String requestData = req . toString ( ) ; String responseData = null ; try { responseData = post ( url , headers , requestData ) ; } catch ( Exception e ) { logger . debug ( e . getMessage ( ) ) ; } if ( responseData != null ) { logger . debug ( ""The request '{}' yields '{}'"" , requestData , responseData ) ; JsonObject resp = ( JsonObject ) JsonParser . parseReader ( new StringReader ( responseData ) ) ; result = resp . get ( ""result"" ) ; JsonElement error = resp . get ( ""error"" ) ; if ( error != null && ! error . isJsonNull ( ) ) { if ( error . isJsonPrimitive ( ) ) { logger . debug ( ""A remote exception occurred: '{}'"" , error . getAsString ( ) ) ; } else if ( error . isJsonObject ( ) ) { JsonObject o = error . getAsJsonObject ( ) ; Integer code = ( o . has ( ""code"" ) ? o . get ( ""code"" ) . getAsInt ( ) : null ) ; String message = ( o . has ( ""message"" ) ? o . get ( ""message"" ) . getAsString ( ) : null ) ; String data = ( o . has ( ""data"" ) ? ( o . get ( ""data"" ) instanceof JsonObject ? o . get ( ""data"" ) . toString ( ) : o . get ( ""data"" ) . getAsString ( ) ) : null ) ; logger . debug ( ""A remote exception occurred: '{}':'{}':'{}'"" , code , message , data ) ; } else { logger . debug ( ""An unknown remote exception occurred: '{}'"" , error . toString ( ) ) ; } } } return result ; } }","public class A { protected JsonElement invokeRPC ( String methodName , Object [ ] args ) { int id = rand . nextInt ( Integer . MAX_VALUE ) ; JsonObject req = new JsonObject ( ) ; req . addProperty ( ""jsonrpc"" , ""2.0"" ) ; req . addProperty ( ""id"" , id ) ; req . addProperty ( ""method"" , methodName ) ; JsonElement result = null ; JsonArray params = new JsonArray ( ) ; if ( args != null ) { for ( Object o : args ) { params . add ( gson . toJsonTree ( o ) ) ; } } req . add ( ""params"" , params ) ; String requestData = req . toString ( ) ; String responseData = null ; try { responseData = post ( url , headers , requestData ) ; } catch ( Exception e ) { logger . debug ( ""An exception occurred while posting data : '{}'"" , e . getMessage ( ) ) ; } if ( responseData != null ) { logger . debug ( ""The request '{}' yields '{}'"" , requestData , responseData ) ; JsonObject resp = ( JsonObject ) JsonParser . parseReader ( new StringReader ( responseData ) ) ; result = resp . get ( ""result"" ) ; JsonElement error = resp . get ( ""error"" ) ; if ( error != null && ! error . isJsonNull ( ) ) { if ( error . isJsonPrimitive ( ) ) { logger . debug ( ""A remote exception occurred: '{}'"" , error . getAsString ( ) ) ; } else if ( error . isJsonObject ( ) ) { JsonObject o = error . getAsJsonObject ( ) ; Integer code = ( o . has ( ""code"" ) ? o . get ( ""code"" ) . getAsInt ( ) : null ) ; String message = ( o . has ( ""message"" ) ? o . get ( ""message"" ) . getAsString ( ) : null ) ; String data = ( o . has ( ""data"" ) ? ( o . get ( ""data"" ) instanceof JsonObject ? o . get ( ""data"" ) . toString ( ) : o . get ( ""data"" ) . getAsString ( ) ) : null ) ; logger . debug ( ""A remote exception occurred: '{}':'{}':'{}'"" , code , message , data ) ; } else { logger . debug ( ""An unknown remote exception occurred: '{}'"" , error . toString ( ) ) ; } } } return result ; } }","logger . debug ( ""An exception occurred while posting data : '{}'"" , e . getMessage ( ) ) ;",Meaningful
"public class A { static public Mat loadMat ( String infile ) { String tryfile = infile ; if ( tryfile . startsWith ( ""http"" ) ) { tryfile = getImageFromUrl ( tryfile ) ; } File f = new File ( tryfile ) ; if ( f . exists ( ) ) { return read ( tryfile ) ; } tryfile = Service . getResourceDir ( OpenCV . class , infile ) ; f = new File ( tryfile ) ; if ( f . exists ( ) ) { return read ( tryfile ) ; } tryfile = ""src"" + File . separator + ""main"" + File . separator + ""resources"" + File . separator + ""resource"" + File . separator + OpenCV . class . getSimpleName ( ) + File . separator + infile ; f = new File ( tryfile ) ; if ( f . exists ( ) ) { return read ( tryfile ) ; } tryfile = ""src"" + File . separator + ""test"" + File . separator + ""resources"" + File . separator + OpenCV . class . getSimpleName ( ) + File . separator + infile ; f = new File ( tryfile ) ; if ( f . exists ( ) ) { return read ( tryfile ) ; } log . error ( ""Could not loadMat from file: "" + tryfile ) ; return null ; } }","public class A { static public Mat loadMat ( String infile ) { String tryfile = infile ; if ( tryfile . startsWith ( ""http"" ) ) { tryfile = getImageFromUrl ( tryfile ) ; } File f = new File ( tryfile ) ; if ( f . exists ( ) ) { return read ( tryfile ) ; } tryfile = Service . getResourceDir ( OpenCV . class , infile ) ; f = new File ( tryfile ) ; if ( f . exists ( ) ) { return read ( tryfile ) ; } tryfile = ""src"" + File . separator + ""main"" + File . separator + ""resources"" + File . separator + ""resource"" + File . separator + OpenCV . class . getSimpleName ( ) + File . separator + infile ; f = new File ( tryfile ) ; if ( f . exists ( ) ) { return read ( tryfile ) ; } tryfile = ""src"" + File . separator + ""test"" + File . separator + ""resources"" + File . separator + OpenCV . class . getSimpleName ( ) + File . separator + infile ; f = new File ( tryfile ) ; if ( f . exists ( ) ) { return read ( tryfile ) ; } log . error ( ""could not load Mat {}"" , infile ) ; return null ; } }","log . error ( ""could not load Mat {}"" , infile ) ;",Meaningful
"public class A { @ Override protected void parse ( final ProtocolFactory protocols , final Local file ) throws AccessDeniedException { final NSDictionary serialized = NSDictionary . dictionaryWithContentsOfFile ( file . getAbsolute ( ) ) ; if ( null == serialized ) { throw new LocalAccessDeniedException ( String . format ( ""Invalid bookmark file %s"" , file ) ) ; } final List < NSDictionary > array = new PlistDeserializer ( serialized ) . listForKey ( ""CustomPluginSettings"" ) ; if ( null == array ) { log . warn ( ""Missing key CustomPluginSettings"" ) ; return ; } for ( NSDictionary dict : array ) { final PlistDeserializer bookmark = new PlistDeserializer ( dict ) ; final String identifier = bookmark . stringForKey ( ""MountFSClassName"" ) ; if ( StringUtils . isBlank ( identifier ) ) { log . warn ( ""Missing key MountFSClassName"" ) ; continue ; } final Protocol protocol ; switch ( identifier ) { case ""FtpConnection"" : protocol = protocols . forType ( Protocol . Type . ftp ) ; break ; case ""WebDAVConnection"" : protocol = protocols . forType ( Protocol . Type . dav ) ; break ; case ""OpenStackConnection"" : protocol = protocols . forType ( Protocol . Type . swift ) ; break ; case ""BBConnection"" : protocol = protocols . forType ( Protocol . Type . b2 ) ; break ; case ""S3Connection"" : protocol = protocols . forType ( Protocol . Type . s3 ) ; break ; case ""DropboxConnection"" : protocol = protocols . forType ( Protocol . Type . dropbox ) ; break ; case ""GDriveConnection"" : protocol = protocols . forType ( Protocol . Type . googledrive ) ; break ; default : protocol = null ; break ; } if ( null == protocol ) { log . warn ( String . format ( ""Missing bookmark %s"" , file ) ) ; continue ; } final NSDictionary details = bookmark . objectForKey ( ""MountFSOptions"" ) ; if ( null == details ) { continue ; } final PlistDeserializer options = new PlistDeserializer ( details ) ; final String hostname = options . stringForKey ( ""host"" ) ; if ( StringUtils . isBlank ( hostname ) ) { continue ; } final Host host = new Host ( protocol , hostname , new Credentials ( options . stringForKey ( ""login"" ) ) ) ; host . setNickname ( bookmark . stringForKey ( ""MountFSLabel"" ) ) ; host . setDefaultPath ( options . stringForKey ( ""remotePath"" ) ) ; this . add ( host ) ; } } }","public class A { @ Override protected void parse ( final ProtocolFactory protocols , final Local file ) throws AccessDeniedException { final NSDictionary serialized = NSDictionary . dictionaryWithContentsOfFile ( file . getAbsolute ( ) ) ; if ( null == serialized ) { throw new LocalAccessDeniedException ( String . format ( ""Invalid bookmark file %s"" , file ) ) ; } final List < NSDictionary > array = new PlistDeserializer ( serialized ) . listForKey ( ""CustomPluginSettings"" ) ; if ( null == array ) { log . warn ( ""Missing key CustomPluginSettings"" ) ; return ; } for ( NSDictionary dict : array ) { final PlistDeserializer bookmark = new PlistDeserializer ( dict ) ; final String identifier = bookmark . stringForKey ( ""MountFSClassName"" ) ; if ( StringUtils . isBlank ( identifier ) ) { log . warn ( ""Missing key MountFSClassName"" ) ; continue ; } final Protocol protocol ; switch ( identifier ) { case ""FtpConnection"" : protocol = protocols . forType ( Protocol . Type . ftp ) ; break ; case ""WebDAVConnection"" : protocol = protocols . forType ( Protocol . Type . dav ) ; break ; case ""OpenStackConnection"" : protocol = protocols . forType ( Protocol . Type . swift ) ; break ; case ""BBConnection"" : protocol = protocols . forType ( Protocol . Type . b2 ) ; break ; case ""S3Connection"" : protocol = protocols . forType ( Protocol . Type . s3 ) ; break ; case ""DropboxConnection"" : protocol = protocols . forType ( Protocol . Type . dropbox ) ; break ; case ""GDriveConnection"" : protocol = protocols . forType ( Protocol . Type . googledrive ) ; break ; default : protocol = null ; break ; } if ( null == protocol ) { log . warn ( String . format ( ""Unable to determine protocol for %s"" , identifier ) ) ; continue ; } final NSDictionary details = bookmark . objectForKey ( ""MountFSOptions"" ) ; if ( null == details ) { continue ; } final PlistDeserializer options = new PlistDeserializer ( details ) ; final String hostname = options . stringForKey ( ""host"" ) ; if ( StringUtils . isBlank ( hostname ) ) { continue ; } final Host host = new Host ( protocol , hostname , new Credentials ( options . stringForKey ( ""login"" ) ) ) ; host . setNickname ( bookmark . stringForKey ( ""MountFSLabel"" ) ) ; host . setDefaultPath ( options . stringForKey ( ""remotePath"" ) ) ; this . add ( host ) ; } } }","log . warn ( String . format ( ""Unable to determine protocol for %s"" , identifier ) ) ;",Meaningful
"public class A { private void removeEventServiceSubscription ( ) { if ( filterlessSubscriptionId != null && subscribeClientFactory != null ) { CswSubscribe cswSubscribe = subscribeClientFactory . getClientForSystemSubject ( getSystemSubject ( ) ) ; try { cswSubscribe . deleteRecordsSubscription ( filterlessSubscriptionId ) ; } catch ( CswException e ) { LOGGER . info ( ""Failed to remove Event Service subscription"" , e ) ; } filterlessSubscriptionId = null ; } } }","public class A { private void removeEventServiceSubscription ( ) { if ( filterlessSubscriptionId != null && subscribeClientFactory != null ) { CswSubscribe cswSubscribe = subscribeClientFactory . getClientForSystemSubject ( getSystemSubject ( ) ) ; try { cswSubscribe . deleteRecordsSubscription ( filterlessSubscriptionId ) ; } catch ( CswException e ) { LOGGER . info ( ""Failed to remove filterless subscription registered for id {} for csw source with id of {}"" , filterlessSubscriptionId , this . getId ( ) ) ; } filterlessSubscriptionId = null ; } } }","LOGGER . info ( ""Failed to remove filterless subscription registered for id {} for csw source with id of {}"" , filterlessSubscriptionId , this . getId ( ) ) ;",Meaningful
"public class A { @ Override public Map < String , IMonomer > getMonomers ( ) { logger . debug ( ""Get monomers: "" , super . getMonomers ( ) ) ; return super . getMonomers ( ) ; } }","public class A { @ Override public Map < String , IMonomer > getMonomers ( ) { logger . debug ( ""Getting monomers as hashtable"" ) ; return super . getMonomers ( ) ; } }","logger . debug ( ""Getting monomers as hashtable"" ) ;",Meaningful
"public class A { @ Override public void exists ( ) { logger . debug ( ""exists()"" ) ; status = Status . EXIST ; signalChildren ( ) ; } }","public class A { @ Override public void exists ( ) { logger . debug ( String . format ( ""Skipped '%s' because idempotent and exists in the machine."" , id ) ) ; status = Status . EXIST ; signalChildren ( ) ; } }","logger . debug ( String . format ( ""Skipped '%s' because idempotent and exists in the machine."" , id ) ) ;",Meaningful
"public class A { @ Test public void testDropMessagesExpiring ( ) throws Exception { clearDataRecreateServerDirs ( ) ; HashMap < String , AddressSettings > settings = new HashMap < > ( ) ; AddressSettings set = new AddressSettings ( ) ; set . setAddressFullMessagePolicy ( AddressFullMessagePolicy . DROP ) ; settings . put ( PagingTest . ADDRESS . toString ( ) , set ) ; server = createServer ( true , createDefaultInVMConfig ( ) , 1024 , 1024 * 1024 , settings ) ; server . start ( ) ; final int numberOfMessages = 30000 ; locator . setAckBatchSize ( 0 ) ; sf = createSessionFactory ( locator ) ; ClientSession sessionProducer = sf . createSession ( ) ; sessionProducer . createQueue ( new QueueConfiguration ( PagingTest . ADDRESS ) ) ; ClientProducer producer = sessionProducer . createProducer ( PagingTest . ADDRESS ) ; ClientMessage message = null ; ClientSession sessionConsumer = sf . createSession ( ) ; class MyHandler implements MessageHandler { int count ; @ Override public void onMessage ( ClientMessage message1 ) { try { Thread . sleep ( 1 ) ; } catch ( Exception e ) { } count ++ ; if ( count % 1000 == 0 ) { logger . debug ( ""Done waiting for messages"" ) ; } try { message1 . acknowledge ( ) ; } catch ( Exception e ) { e . printStackTrace ( ) ; } } } ClientConsumer consumer = sessionConsumer . createConsumer ( PagingTest . ADDRESS ) ; sessionConsumer . start ( ) ; consumer . setMessageHandler ( new MyHandler ( ) ) ; for ( int i = 0 ; i < numberOfMessages ; i ++ ) { byte [ ] body = new byte [ 1024 ] ; message = sessionProducer . createMessage ( false ) ; message . getBodyBuffer ( ) . writeBytes ( body ) ; message . setExpiration ( System . currentTimeMillis ( ) + 100 ) ; producer . send ( message ) ; } sessionProducer . close ( ) ; sessionConsumer . close ( ) ; } }","public class A { @ Test public void testDropMessagesExpiring ( ) throws Exception { clearDataRecreateServerDirs ( ) ; HashMap < String , AddressSettings > settings = new HashMap < > ( ) ; AddressSettings set = new AddressSettings ( ) ; set . setAddressFullMessagePolicy ( AddressFullMessagePolicy . DROP ) ; settings . put ( PagingTest . ADDRESS . toString ( ) , set ) ; server = createServer ( true , createDefaultInVMConfig ( ) , 1024 , 1024 * 1024 , settings ) ; server . start ( ) ; final int numberOfMessages = 30000 ; locator . setAckBatchSize ( 0 ) ; sf = createSessionFactory ( locator ) ; ClientSession sessionProducer = sf . createSession ( ) ; sessionProducer . createQueue ( new QueueConfiguration ( PagingTest . ADDRESS ) ) ; ClientProducer producer = sessionProducer . createProducer ( PagingTest . ADDRESS ) ; ClientMessage message = null ; ClientSession sessionConsumer = sf . createSession ( ) ; class MyHandler implements MessageHandler { int count ; @ Override public void onMessage ( ClientMessage message1 ) { try { Thread . sleep ( 1 ) ; } catch ( Exception e ) { } count ++ ; if ( count % 1000 == 0 ) { log . debug ( ""received "" + count ) ; } try { message1 . acknowledge ( ) ; } catch ( Exception e ) { e . printStackTrace ( ) ; } } } ClientConsumer consumer = sessionConsumer . createConsumer ( PagingTest . ADDRESS ) ; sessionConsumer . start ( ) ; consumer . setMessageHandler ( new MyHandler ( ) ) ; for ( int i = 0 ; i < numberOfMessages ; i ++ ) { byte [ ] body = new byte [ 1024 ] ; message = sessionProducer . createMessage ( false ) ; message . getBodyBuffer ( ) . writeBytes ( body ) ; message . setExpiration ( System . currentTimeMillis ( ) + 100 ) ; producer . send ( message ) ; } sessionProducer . close ( ) ; sessionConsumer . close ( ) ; } }","log . debug ( ""received "" + count ) ;",Meaningful
"public class A { public void service ( HttpServletRequest req , HttpServletResponse res ) throws Exception { boolean debug = log . isDebugEnabled ( ) ; try { req . setCharacterEncoding ( ""UTF-8"" ) ; } catch ( UnsupportedEncodingException e ) { log . error ( ""Encoding not supported"" , e ) ; } String portalPath = req . getRequestURI ( ) . substring ( req . getContextPath ( ) . length ( ) ) ; Router router = routerRef . get ( ) ; if ( router != null ) { Iterator < Map < QualifiedName , String > > matcher = router . matcher ( portalPath , req . getParameterMap ( ) ) ; boolean started = false ; boolean processed = false ; try { while ( matcher . hasNext ( ) && ! processed ) { Map < QualifiedName , String > parameters = matcher . next ( ) ; String handlerKey = parameters . get ( HANDLER_PARAM ) ; if ( handlerKey != null ) { WebRequestHandler handler = handlers . get ( handlerKey ) ; if ( handler != null ) { if ( debug ) { log . debug ( ""Serving request path="" + portalPath + "", parameters="" + parameters + "" with handler "" + handler ) ; } if ( ! started && handler . getRequiresLifeCycle ( ) ) { if ( debug ) { log . debug ( ""Starting RequestLifeCycle for handler "" + handler ) ; } RequestLifeCycle . begin ( ExoContainerContext . getCurrentContainer ( ) ) ; started = true ; } processed = handler . execute ( new ControllerContext ( this , router , req , res , parameters ) ) ; } else { if ( debug ) { log . debug ( ""No handler "" + handlerKey + "" for request path="" + portalPath + "", parameters="" + parameters ) ; } } } } } finally { if ( started ) { if ( debug ) { log . debug ( ""Finishing RequestLifeCycle for current request"" ) ; } RequestLifeCycle . end ( ) ; } } if ( ! processed ) { log . error ( ""AppHub request "" + req . getRequestURI ( ) + "" doesn't exist, couldn't be processed."" ) ; res . sendError ( HttpServletResponse . SC_NOT_FOUND ) ; } } else { log . error ( ""Missing valid router configuration "" + configurationPathRef . get ( ) ) ; res . sendError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR ) ; } } }","public class A { public void service ( HttpServletRequest req , HttpServletResponse res ) throws Exception { boolean debug = log . isDebugEnabled ( ) ; try { req . setCharacterEncoding ( ""UTF-8"" ) ; } catch ( UnsupportedEncodingException e ) { log . error ( ""Encoding not supported"" , e ) ; } String portalPath = req . getRequestURI ( ) . substring ( req . getContextPath ( ) . length ( ) ) ; Router router = routerRef . get ( ) ; if ( router != null ) { Iterator < Map < QualifiedName , String > > matcher = router . matcher ( portalPath , req . getParameterMap ( ) ) ; boolean started = false ; boolean processed = false ; try { while ( matcher . hasNext ( ) && ! processed ) { Map < QualifiedName , String > parameters = matcher . next ( ) ; String handlerKey = parameters . get ( HANDLER_PARAM ) ; if ( handlerKey != null ) { WebRequestHandler handler = handlers . get ( handlerKey ) ; if ( handler != null ) { if ( debug ) { log . debug ( ""Serving request path="" + portalPath + "", parameters="" + parameters + "" with handler "" + handler ) ; } if ( ! started && handler . getRequiresLifeCycle ( ) ) { if ( debug ) { log . debug ( ""Starting RequestLifeCycle for handler "" + handler ) ; } RequestLifeCycle . begin ( ExoContainerContext . getCurrentContainer ( ) ) ; started = true ; } processed = handler . execute ( new ControllerContext ( this , router , req , res , parameters ) ) ; } else { if ( debug ) { log . debug ( ""No handler "" + handlerKey + "" for request path="" + portalPath + "", parameters="" + parameters ) ; } } } } } finally { if ( started ) { if ( debug ) { log . debug ( ""Finishing RequestLifeCycle for current request"" ) ; } RequestLifeCycle . end ( ) ; } } if ( ! processed ) { log . error ( ""Could not associate the request path="" + portalPath + "" with an handler"" ) ; res . sendError ( HttpServletResponse . SC_NOT_FOUND ) ; } } else { log . error ( ""Missing valid router configuration "" + configurationPathRef . get ( ) ) ; res . sendError ( HttpServletResponse . SC_INTERNAL_SERVER_ERROR ) ; } } }","log . error ( ""Could not associate the request path="" + portalPath + "" with an handler"" ) ;",Meaningful
"public class A { private static List < CloudOffer > getSuitableOffersOfModule ( List < Map < String , Object > > listOptions , List < String > listOfferNames ) { ArrayList < CloudOffer > listOffers = new ArrayList < CloudOffer > ( ) ; for ( String offerName : listOfferNames ) { CloudOffer offer = getAllCharacteristicsOfCloudOffer ( offerName , listOptions ) ; if ( offer != null ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""found a suitable offer of the requested class '"" + offerName + ""'."" ) ; } listOffers . add ( offer ) ; } else { log . warn ( ""not found information for cloud offer name '"" + offerName + ""'"" ) ; } } return listOffers ; } }","public class A { private static List < CloudOffer > getSuitableOffersOfModule ( List < Map < String , Object > > listOptions , List < String > listOfferNames ) { ArrayList < CloudOffer > listOffers = new ArrayList < CloudOffer > ( ) ; for ( String offerName : listOfferNames ) { CloudOffer offer = getAllCharacteristicsOfCloudOffer ( offerName , listOptions ) ; if ( offer != null ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Found cloud offer with info:"" + offer . toString ( ) ) ; } listOffers . add ( offer ) ; } else { log . warn ( ""not found information for cloud offer name '"" + offerName + ""'"" ) ; } } return listOffers ; } }","log . debug ( ""Found cloud offer with info:"" + offer . toString ( ) ) ;",Meaningful
"public class A { @ Override public Map < Short , JoyQueueCode > batchCommitIndex ( String topic , String app , Map < Short , Long > indexes , long timeout ) { Table < String , Short , Long > topicTable = HashBasedTable . create ( ) ; for ( Map . Entry < Short , Long > entry : indexes . entrySet ( ) ) { topicTable . put ( topic , entry . getKey ( ) , entry . getValue ( ) ) ; } Map < Short , JoyQueueCode > result = Maps . newHashMap ( ) ; Map < BrokerNode , Table < String , Short , Long > > brokerCommitMap = buildCommitIndexRequest ( topic , app , indexes ) ; for ( Map . Entry < BrokerNode , Table < String , Short , Long > > entry : brokerCommitMap . entrySet ( ) ) { try { ConsumerClient client = consumerClientManager . getOrCreateClient ( entry . getKey ( ) ) ; CommitIndexResponse commitIndexResponse = client . commitIndex ( entry . getValue ( ) , app , timeout ) ; for ( Map . Entry < String , Map < Short , JoyQueueCode > > resultEntry : commitIndexResponse . getResult ( ) . rowMap ( ) . entrySet ( ) ) { for ( Map . Entry < Short , JoyQueueCode > ackEntry : resultEntry . getValue ( ) . entrySet ( ) ) { result . put ( ackEntry . getKey ( ) , ackEntry . getValue ( ) ) ; } } } catch ( ClientException e ) { logger . error ( ""Bulk commit exception exception: {}"" , e . getMessage ( ) ) ; for ( Map . Entry < String , Map < Short , Long > > topicEntry : entry . getValue ( ) . rowMap ( ) . entrySet ( ) ) { for ( Map . Entry < Short , Long > partitionEntry : topicEntry . getValue ( ) . entrySet ( ) ) { result . put ( partitionEntry . getKey ( ) , JoyQueueCode . valueOf ( e . getCode ( ) ) ) ; } } } } for ( Map . Entry < Short , JoyQueueCode > entry : result . entrySet ( ) ) { if ( result . containsKey ( entry . getKey ( ) ) ) { continue ; } result . put ( entry . getKey ( ) , JoyQueueCode . CN_UNKNOWN_ERROR ) ; } return result ; } }","public class A { @ Override public Map < Short , JoyQueueCode > batchCommitIndex ( String topic , String app , Map < Short , Long > indexes , long timeout ) { Table < String , Short , Long > topicTable = HashBasedTable . create ( ) ; for ( Map . Entry < Short , Long > entry : indexes . entrySet ( ) ) { topicTable . put ( topic , entry . getKey ( ) , entry . getValue ( ) ) ; } Map < Short , JoyQueueCode > result = Maps . newHashMap ( ) ; Map < BrokerNode , Table < String , Short , Long > > brokerCommitMap = buildCommitIndexRequest ( topic , app , indexes ) ; for ( Map . Entry < BrokerNode , Table < String , Short , Long > > entry : brokerCommitMap . entrySet ( ) ) { try { ConsumerClient client = consumerClientManager . getOrCreateClient ( entry . getKey ( ) ) ; CommitIndexResponse commitIndexResponse = client . commitIndex ( entry . getValue ( ) , app , timeout ) ; for ( Map . Entry < String , Map < Short , JoyQueueCode > > resultEntry : commitIndexResponse . getResult ( ) . rowMap ( ) . entrySet ( ) ) { for ( Map . Entry < Short , JoyQueueCode > ackEntry : resultEntry . getValue ( ) . entrySet ( ) ) { result . put ( ackEntry . getKey ( ) , ackEntry . getValue ( ) ) ; } } } catch ( ClientException e ) { logger . error ( ""commit index exception, commitMap: {}, app: {}"" , entry . getValue ( ) , app , e ) ; for ( Map . Entry < String , Map < Short , Long > > topicEntry : entry . getValue ( ) . rowMap ( ) . entrySet ( ) ) { for ( Map . Entry < Short , Long > partitionEntry : topicEntry . getValue ( ) . entrySet ( ) ) { result . put ( partitionEntry . getKey ( ) , JoyQueueCode . valueOf ( e . getCode ( ) ) ) ; } } } } for ( Map . Entry < Short , JoyQueueCode > entry : result . entrySet ( ) ) { if ( result . containsKey ( entry . getKey ( ) ) ) { continue ; } result . put ( entry . getKey ( ) , JoyQueueCode . CN_UNKNOWN_ERROR ) ; } return result ; } }","logger . error ( ""commit index exception, commitMap: {}, app: {}"" , entry . getValue ( ) , app , e ) ;",Meaningful
"public class A { @ Override public void close ( ) throws IOException { out . close ( ) ; final byte [ ] bytes = ByteArrayOutputStream . class . cast ( out ) . toByteArray ( ) ; if ( ! destination . exists ( ) || isDifferent ( bytes ) ) { try ( final OutputStream out = new FileOutputStream ( destination ) ) { out . write ( bytes ) ; } log . info ( ""Changed destination "" + destination + "" to "" + toString ( ) ) ; } else { log . info ( destination + "" didn't change, skip rewriting"" ) ; } } }","public class A { @ Override public void close ( ) throws IOException { out . close ( ) ; final byte [ ] bytes = ByteArrayOutputStream . class . cast ( out ) . toByteArray ( ) ; if ( ! destination . exists ( ) || isDifferent ( bytes ) ) { try ( final OutputStream out = new FileOutputStream ( destination ) ) { out . write ( bytes ) ; } log . info ( destination + "" created"" ) ; } else { log . info ( destination + "" didn't change, skip rewriting"" ) ; } } }","log . info ( destination + "" created"" ) ;",Meaningful
"public class A { @ Override public void release ( ) { try { autoPingManager . release ( ) ; bookmarkManager . release ( ) ; mediaFileManager . release ( ) ; fileContentManager . release ( ) ; pingTargetManager . release ( ) ; pingQueueManager . release ( ) ; pluginManager . release ( ) ; threadManager . release ( ) ; userManager . release ( ) ; weblogManager . release ( ) ; } catch ( Exception e ) { log . error ( ""error releaseing file"" , e ) ; } } }","public class A { @ Override public void release ( ) { try { autoPingManager . release ( ) ; bookmarkManager . release ( ) ; mediaFileManager . release ( ) ; fileContentManager . release ( ) ; pingTargetManager . release ( ) ; pingQueueManager . release ( ) ; pluginManager . release ( ) ; threadManager . release ( ) ; userManager . release ( ) ; weblogManager . release ( ) ; } catch ( Exception e ) { log . error ( ""Error calling Roller.release()"" , e ) ; } } }","log . error ( ""Error calling Roller.release()"" , e ) ;",Meaningful
"public class A { private void redoSubscribe ( ) { LogUtils . NAMING_LOGGER . info ( ""Grpc re-connect, redo subscribe services"" ) ; for ( String each : subscribes ) { ServiceInfo serviceInfo = ServiceInfo . fromKey ( each ) ; try { clientProxy . subscribe ( serviceInfo . getName ( ) , serviceInfo . getGroupName ( ) , serviceInfo . getClusters ( ) ) ; } catch ( NacosException e ) { LogUtils . NAMING_LOGGER . warn ( ""Grpc re-connect, redo subscribe services failed"" , e ) ; } } } }","public class A { private void redoSubscribe ( ) { LogUtils . NAMING_LOGGER . info ( ""Grpc re-connect, redo subscribe services"" ) ; for ( String each : subscribes ) { ServiceInfo serviceInfo = ServiceInfo . fromKey ( each ) ; try { clientProxy . subscribe ( serviceInfo . getName ( ) , serviceInfo . getGroupName ( ) , serviceInfo . getClusters ( ) ) ; } catch ( NacosException e ) { LogUtils . NAMING_LOGGER . warn ( String . format ( ""re subscribe service %s failed"" , serviceInfo . getName ( ) ) , e ) ; } } } }","LogUtils . NAMING_LOGGER . warn ( String . format ( ""re subscribe service %s failed"" , serviceInfo . getName ( ) ) , e ) ;",Meaningful
"public class A { private void handleEvent ( EvdevDevice . InputEvent event ) { if ( event . type ( ) != EvdevLibrary . Type . KEY ) { return ; } @ Nullable Channel channel = channels . get ( event . getCode ( ) ) ; if ( channel == null ) { String msg = ""Could not find channel for code {}"" ; if ( isInitialized ( ) ) { logger . warn ( msg , event . getCode ( ) ) ; } else { logger . debug ( msg ) ; } return ; } logger . debug ( ""Got event: {}"" , event ) ; int eventValue = event . getValue ( ) ; switch ( eventValue ) { case EvdevLibrary . KeyEventValue . DOWN : String keyCode = channel . getUID ( ) . getIdWithoutGroup ( ) ; updateState ( keyChannel . getUID ( ) , new StringType ( keyCode ) ) ; updateState ( channel . getUID ( ) , OpenClosedType . CLOSED ) ; triggerChannel ( keyChannel . getUID ( ) , keyCode ) ; triggerChannel ( channel . getUID ( ) , CommonTriggerEvents . PRESSED ) ; updateState ( keyChannel . getUID ( ) , new StringType ( ) ) ; break ; case EvdevLibrary . KeyEventValue . UP : updateState ( channel . getUID ( ) , OpenClosedType . OPEN ) ; triggerChannel ( channel . getUID ( ) , CommonTriggerEvents . RELEASED ) ; break ; case EvdevLibrary . KeyEventValue . REPEAT : break ; default : logger . debug ( ""Unexpected event value for channel {}: {}"" , channel , eventValue ) ; break ; } } }","public class A { private void handleEvent ( EvdevDevice . InputEvent event ) { if ( event . type ( ) != EvdevLibrary . Type . KEY ) { return ; } @ Nullable Channel channel = channels . get ( event . getCode ( ) ) ; if ( channel == null ) { String msg = ""Could not find channel for code {}"" ; if ( isInitialized ( ) ) { logger . warn ( msg , event . getCode ( ) ) ; } else { logger . debug ( msg , event . getCode ( ) ) ; } return ; } logger . debug ( ""Got event: {}"" , event ) ; int eventValue = event . getValue ( ) ; switch ( eventValue ) { case EvdevLibrary . KeyEventValue . DOWN : String keyCode = channel . getUID ( ) . getIdWithoutGroup ( ) ; updateState ( keyChannel . getUID ( ) , new StringType ( keyCode ) ) ; updateState ( channel . getUID ( ) , OpenClosedType . CLOSED ) ; triggerChannel ( keyChannel . getUID ( ) , keyCode ) ; triggerChannel ( channel . getUID ( ) , CommonTriggerEvents . PRESSED ) ; updateState ( keyChannel . getUID ( ) , new StringType ( ) ) ; break ; case EvdevLibrary . KeyEventValue . UP : updateState ( channel . getUID ( ) , OpenClosedType . OPEN ) ; triggerChannel ( channel . getUID ( ) , CommonTriggerEvents . RELEASED ) ; break ; case EvdevLibrary . KeyEventValue . REPEAT : break ; default : logger . debug ( ""Unexpected event value for channel {}: {}"" , channel , eventValue ) ; break ; } } }","logger . debug ( msg , event . getCode ( ) ) ;",Meaningful
"public class A { protected void redoStoreOverflow ( final OverflowStoreLoggable loggable ) { try { SinglePage page = getSinglePageForRedo ( loggable , loggable . pageNum ) ; if ( page != null && requiresRedo ( loggable , page ) ) { final BFilePageHeader ph = page . getPageHeader ( ) ; try { System . arraycopy ( loggable . data , 0 , page . getData ( ) , 0 , loggable . size ) ; } catch ( final ArrayIndexOutOfBoundsException e ) { LOG . warn ( ""page {} not found!"" , page . getPageNum ) ; throw e ; } ph . setDataLength ( loggable . size ) ; ph . setNextInChain ( 0 ) ; ph . setLsn ( loggable . getLsn ( ) ) ; page . setDirty ( true ) ; dataCache . add ( page ) ; if ( loggable . prevPage != Page . NO_PAGE ) { page = getSinglePage ( loggable . prevPage ) ; SanityCheck . ASSERT ( page != null , ""Previous page is null"" ) ; page . getPageHeader ( ) . setNextInChain ( loggable . pageNum ) ; page . setDirty ( true ) ; dataCache . add ( page ) ; } } } catch ( final IOException e ) { LOG . warn ( ""An IOException occurred during redo: {}"" , e . getMessage ( ) , e ) ; } } }","public class A { protected void redoStoreOverflow ( final OverflowStoreLoggable loggable ) { try { SinglePage page = getSinglePageForRedo ( loggable , loggable . pageNum ) ; if ( page != null && requiresRedo ( loggable , page ) ) { final BFilePageHeader ph = page . getPageHeader ( ) ; try { System . arraycopy ( loggable . data , 0 , page . getData ( ) , 0 , loggable . size ) ; } catch ( final ArrayIndexOutOfBoundsException e ) { LOG . warn ( ""{}; {}; {}; {}"" , loggable . data . length , page . getData ( ) . length , ph . getDataLength ( ) , loggable . size ) ; throw e ; } ph . setDataLength ( loggable . size ) ; ph . setNextInChain ( 0 ) ; ph . setLsn ( loggable . getLsn ( ) ) ; page . setDirty ( true ) ; dataCache . add ( page ) ; if ( loggable . prevPage != Page . NO_PAGE ) { page = getSinglePage ( loggable . prevPage ) ; SanityCheck . ASSERT ( page != null , ""Previous page is null"" ) ; page . getPageHeader ( ) . setNextInChain ( loggable . pageNum ) ; page . setDirty ( true ) ; dataCache . add ( page ) ; } } } catch ( final IOException e ) { LOG . warn ( ""An IOException occurred during redo: {}"" , e . getMessage ( ) , e ) ; } } }","LOG . warn ( ""{}; {}; {}; {}"" , loggable . data . length , page . getData ( ) . length , ph . getDataLength ( ) , loggable . size ) ;",Meaningful
"public class A { @ Override public void onMode ( String string , IRCUser ircUser , IRCModeParser ircModeParser ) { super . onMode ( string , ircUser , ircModeParser ) ; LOG . info ( ""onMode.string = "" + string ) ; LOG . info ( ""onMode.ircUser = "" + ircUser ) ; LOG . info ( ""onMode.string = "" + string ) ; } }","public class A { @ Override public void onMode ( String string , IRCUser ircUser , IRCModeParser ircModeParser ) { super . onMode ( string , ircUser , ircModeParser ) ; LOG . info ( ""onMode.string = "" + string ) ; LOG . info ( ""onMode.ircUser = "" + ircUser ) ; LOG . info ( ""onMode.ircModeParser = "" + ircModeParser ) ; } }","LOG . info ( ""onMode.ircModeParser = "" + ircModeParser ) ;",Meaningful
"public class A { public void addAllSpouts ( List < SpoutDef > spouts , boolean override ) { for ( SpoutDef spout : spouts ) { String id = spout . getId ( ) ; if ( this . spoutMap . get ( id ) == null || override ) { this . spoutMap . put ( spout . getId ( ) , spout ) ; } else { LOG . warn ( ""Grouped {}, but group contains {} "" , id , this . spoutMap . size ( ) ) ; } } } }","public class A { public void addAllSpouts ( List < SpoutDef > spouts , boolean override ) { for ( SpoutDef spout : spouts ) { String id = spout . getId ( ) ; if ( this . spoutMap . get ( id ) == null || override ) { this . spoutMap . put ( spout . getId ( ) , spout ) ; } else { LOG . warn ( ""Ignoring attempt to create spout '{}' with override == false."" , id ) ; } } } }","LOG . warn ( ""Ignoring attempt to create spout '{}' with override == false."" , id ) ;",Meaningful
"public class A { @ Override public Long saveNewTask ( T task , Long crisisID ) { if ( task == null ) { logger . error ( ""Attempting to save new task for crisisID : "" + crisisID ) ; return - 1L ; } try { DocumentDTO doc = ( DocumentDTO ) task ; CollectionDTO crisisDTO = remoteCrisisEJB . findCrisisByID ( crisisID ) ; doc . setCrisisDTO ( crisisDTO ) ; doc . setHasHumanLabels ( false ) ; DocumentDTO savedDoc = remoteDocumentEJB . addDocument ( doc ) ; if ( savedDoc != null ) { return savedDoc . getDocumentID ( ) ; } } catch ( Exception e ) { logger . error ( ""Error in saving new document for crisisID : "" + crisisID , e ) ; } return - 1L ; } }","public class A { @ Override public Long saveNewTask ( T task , Long crisisID ) { if ( task == null ) { logger . error ( ""Attempting to insert empty task"" ) ; return - 1L ; } try { DocumentDTO doc = ( DocumentDTO ) task ; CollectionDTO crisisDTO = remoteCrisisEJB . findCrisisByID ( crisisID ) ; doc . setCrisisDTO ( crisisDTO ) ; doc . setHasHumanLabels ( false ) ; DocumentDTO savedDoc = remoteDocumentEJB . addDocument ( doc ) ; if ( savedDoc != null ) { return savedDoc . getDocumentID ( ) ; } } catch ( Exception e ) { logger . error ( ""Error in saving new document for crisisID : "" + crisisID , e ) ; } return - 1L ; } }","logger . error ( ""Attempting to insert empty task"" ) ;",Meaningful
"public class A { @ Override protected void processCommittedData ( FSRecordCompactionOperator . OutputMetaData outputMetaData ) { try { Path path = new Path ( outputMetaData . getPath ( ) ) ; if ( fs . exists ( path ) == false ) { logger . debug ( ""Ignoring non-existent path assuming replay : {}"" , path ) ; return ; } FSDataInputStream fsinput = fs . open ( path ) ; ObjectMetadata omd = new ObjectMetadata ( ) ; omd . setContentLength ( outputMetaData . getSize ( ) ) ; String keyName = directoryName + Path . SEPARATOR + outputMetaData . getFileName ( ) ; PutObjectRequest request = new PutObjectRequest ( bucketName , keyName , fsinput , omd ) ; if ( outputMetaData . getSize ( ) < Integer . MAX_VALUE ) { request . getRequestClientOptions ( ) . setReadLimit ( ( int ) outputMetaData . getSize ( ) ) ; } else { throw new RuntimeException ( ""PutRequestSize greater than Integer.MAX_VALUE"" ) ; } if ( fs . exists ( path ) ) { PutObjectResult result = s3client . putObject ( request ) ; logger . debug ( ""File {} Uploaded at {}"" , keyName , result . getETag ( ) ) ; } } catch ( FileNotFoundException e ) { logger . debug ( ""Ignoring non-existent path assuming replay : {}"" , outputMetaData . getPath ( ) ) ; } catch ( IOException e ) { logger . error ( ""Exception while updating key {} for file {}"" , outputMetaData . getKey ( ) , outputMetaData . getPath ( ) , e ) ; } } }","public class A { @ Override protected void processCommittedData ( FSRecordCompactionOperator . OutputMetaData outputMetaData ) { try { Path path = new Path ( outputMetaData . getPath ( ) ) ; if ( fs . exists ( path ) == false ) { logger . debug ( ""Ignoring non-existent path assuming replay : {}"" , path ) ; return ; } FSDataInputStream fsinput = fs . open ( path ) ; ObjectMetadata omd = new ObjectMetadata ( ) ; omd . setContentLength ( outputMetaData . getSize ( ) ) ; String keyName = directoryName + Path . SEPARATOR + outputMetaData . getFileName ( ) ; PutObjectRequest request = new PutObjectRequest ( bucketName , keyName , fsinput , omd ) ; if ( outputMetaData . getSize ( ) < Integer . MAX_VALUE ) { request . getRequestClientOptions ( ) . setReadLimit ( ( int ) outputMetaData . getSize ( ) ) ; } else { throw new RuntimeException ( ""PutRequestSize greater than Integer.MAX_VALUE"" ) ; } if ( fs . exists ( path ) ) { PutObjectResult result = s3client . putObject ( request ) ; logger . debug ( ""File {} Uploaded at {}"" , keyName , result . getETag ( ) ) ; } } catch ( FileNotFoundException e ) { logger . debug ( ""Ignoring non-existent path assuming replay : {}"" , outputMetaData . getPath ( ) ) ; } catch ( IOException e ) { logger . error ( ""Unable to create Stream: {}"" , e . getMessage ( ) ) ; } } }","logger . error ( ""Unable to create Stream: {}"" , e . getMessage ( ) ) ;",Meaningful
"public class A { private String getBearerToken ( final Tenant tenant , final ID principal , final Secret secret ) { try { final String payload = String . format ( ""grant_type=client_credentials&client_id=%s&client_secret=%s&resource=%s"" , principal . getValue ( ) , URLEncoder . encode ( secret . getValue ( ) , ""UTF-8"" ) , URLEncoder . encode ( ""https://graph.windows.net"" , ""UTF-8"" ) ) ; final URL url = new URL ( String . format ( ""https://login.microsoftonline.com/%s/oauth2/token"" , tenant . getName ( ) ) ) ; final HttpsURLConnection connection = ( HttpsURLConnection ) url . openConnection ( ) ; connection . setRequestMethod ( ""POST"" ) ; connection . setRequestProperty ( ""Host"" , ""login.microsoftonline.com"" ) ; connection . setRequestProperty ( ""Content-Type"" , ""application/x-www-form-urlencoded"" ) ; connection . setRequestProperty ( ""Accept"" , ""application/json"" ) ; connection . setDoOutput ( true ) ; connection . getOutputStream ( ) . write ( payload . getBytes ( ) ) ; connection . getOutputStream ( ) . flush ( ) ; final StringBuilder result = new StringBuilder ( ) ; try ( final BufferedReader in = new BufferedReader ( new InputStreamReader ( connection . getInputStream ( ) ) ) ) { for ( String line = in . readLine ( ) ; line != null ; line = in . readLine ( ) ) { result . append ( line ) ; } } final ObjectMapper mapper = new ObjectMapper ( ) ; final JsonNode node = mapper . readValue ( result . toString ( ) . getBytes ( ) , JsonNode . class ) ; return node . get ( ""access_token"" ) . asText ( ) ; } catch ( IOException e ) { LOGGER . error ( ""Encountered an error generating token from JWT"" , e ) ; return null ; } catch ( RuntimeException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; return null ; } } }","public class A { private String getBearerToken ( final Tenant tenant , final ID principal , final Secret secret ) { try { final String payload = String . format ( ""grant_type=client_credentials&client_id=%s&client_secret=%s&resource=%s"" , principal . getValue ( ) , URLEncoder . encode ( secret . getValue ( ) , ""UTF-8"" ) , URLEncoder . encode ( ""https://graph.windows.net"" , ""UTF-8"" ) ) ; final URL url = new URL ( String . format ( ""https://login.microsoftonline.com/%s/oauth2/token"" , tenant . getName ( ) ) ) ; final HttpsURLConnection connection = ( HttpsURLConnection ) url . openConnection ( ) ; connection . setRequestMethod ( ""POST"" ) ; connection . setRequestProperty ( ""Host"" , ""login.microsoftonline.com"" ) ; connection . setRequestProperty ( ""Content-Type"" , ""application/x-www-form-urlencoded"" ) ; connection . setRequestProperty ( ""Accept"" , ""application/json"" ) ; connection . setDoOutput ( true ) ; connection . getOutputStream ( ) . write ( payload . getBytes ( ) ) ; connection . getOutputStream ( ) . flush ( ) ; final StringBuilder result = new StringBuilder ( ) ; try ( final BufferedReader in = new BufferedReader ( new InputStreamReader ( connection . getInputStream ( ) ) ) ) { for ( String line = in . readLine ( ) ; line != null ; line = in . readLine ( ) ) { result . append ( line ) ; } } final ObjectMapper mapper = new ObjectMapper ( ) ; final JsonNode node = mapper . readValue ( result . toString ( ) . getBytes ( ) , JsonNode . class ) ; return node . get ( ""access_token"" ) . asText ( ) ; } catch ( IOException e ) { LOGGER . error ( ""IO Exception"" , e ) ; return null ; } catch ( RuntimeException e ) { LOGGER . error ( e . getMessage ( ) , e ) ; return null ; } } }","LOGGER . error ( ""IO Exception"" , e ) ;",Meaningful
"public class A { public static void close ( Closeable closeable , String name , Logger log ) { if ( closeable != null ) { try { closeable . close ( ) ; } catch ( IOException e ) { if ( log == null ) { log = LOG ; } if ( name != null ) { log . warn ( ""Cannot close {}"" , name ) ; } else { log . warn ( ""Cannot close. Reason: {}"" , e . getMessage ( ) , e ) ; } } } } }","public class A { public static void close ( Closeable closeable , String name , Logger log ) { if ( closeable != null ) { try { closeable . close ( ) ; } catch ( IOException e ) { if ( log == null ) { log = LOG ; } if ( name != null ) { log . warn ( ""Cannot close: "" + name + "". Reason: "" + e . getMessage ( ) , e ) ; } else { log . warn ( ""Cannot close. Reason: {}"" , e . getMessage ( ) , e ) ; } } } } }","log . warn ( ""Cannot close: "" + name + "". Reason: "" + e . getMessage ( ) , e ) ;",Meaningful
"public class A { private String getMaxDateWithDataInDB ( UpdateInfo updateInfo ) { final String entityName = JPAUtils . getEntityName ( MovesPlaceFacet . class ) ; final List < MovesPlaceFacet > newest = jpaDaoService . executeQueryWithLimit ( ""SELECT facet from "" + entityName + "" facet WHERE facet.apiKeyId=? ORDER BY facet.end DESC,facet.date DESC"" , 1 , MovesPlaceFacet . class , updateInfo . apiKey . getId ( ) ) ; String ret = null ; if ( newest . size ( ) > 0 ) { ret = newest . get ( 0 ) . date ; logger . info ( ""Moves: guestId="" + updateInfo . getGuestId ( ) + "", maxDateInDB="" + ret ) ; } else { logger . info ( ""Moves: latest date already known for guestId="" + updateInfo . getGuestId ( ) ) ; } return ret ; } }","public class A { private String getMaxDateWithDataInDB ( UpdateInfo updateInfo ) { final String entityName = JPAUtils . getEntityName ( MovesPlaceFacet . class ) ; final List < MovesPlaceFacet > newest = jpaDaoService . executeQueryWithLimit ( ""SELECT facet from "" + entityName + "" facet WHERE facet.apiKeyId=? ORDER BY facet.end DESC,facet.date DESC"" , 1 , MovesPlaceFacet . class , updateInfo . apiKey . getId ( ) ) ; String ret = null ; if ( newest . size ( ) > 0 ) { ret = newest . get ( 0 ) . date ; logger . info ( ""Moves: guestId="" + updateInfo . getGuestId ( ) + "", maxDateInDB="" + ret ) ; } else { logger . info ( ""Moves: guestId="" + updateInfo . getGuestId ( ) + "", maxDateInDB=null"" ) ; } return ret ; } }","logger . info ( ""Moves: guestId="" + updateInfo . getGuestId ( ) + "", maxDateInDB=null"" ) ;",Meaningful
"public class A { public Optional < Throwable > execFor ( HapiApiSpec spec ) { pauseIfRequested ( ) ; configureProtoStructureFor ( spec ) ; try { boolean hasCompleteLifecycle = submitOp ( spec ) ; if ( ! ( this instanceof UtilOp ) ) { spec . incrementNumLedgerOps ( ) ; } if ( shouldRegisterTxn ) { registerTxnSubmitted ( spec ) ; } if ( hasCompleteLifecycle ) { assertExpectationsGiven ( spec ) ; updateStateOf ( spec ) ; } } catch ( Throwable t ) { if ( unavailableNode && t . getMessage ( ) . startsWith ( ""UNAVAILABLE"" ) ) { log . info ( ""Node {} is unavailable as expected!"" , HapiPropertySource . asAccountString ( node . get ( ) ) ) ; return Optional . empty ( ) ; } if ( verboseLoggingOn ) { log . warn ( spec . logPrefix ( ) + this + "" failed - {}"" , t ) ; } else if ( ! loggingOff ) { log . warn ( spec . logPrefix ( ) + this + "" failed"" , t ) ; } return Optional . of ( t ) ; } if ( unavailableNode ) { String message = String . format ( ""Node %s is NOT unavailable as expected!!!"" , HapiPropertySource . asAccountString ( node . get ( ) ) ) ; log . error ( message ) ; return Optional . of ( new RuntimeException ( message ) ) ; } return Optional . empty ( ) ; } }","public class A { public Optional < Throwable > execFor ( HapiApiSpec spec ) { pauseIfRequested ( ) ; configureProtoStructureFor ( spec ) ; try { boolean hasCompleteLifecycle = submitOp ( spec ) ; if ( ! ( this instanceof UtilOp ) ) { spec . incrementNumLedgerOps ( ) ; } if ( shouldRegisterTxn ) { registerTxnSubmitted ( spec ) ; } if ( hasCompleteLifecycle ) { assertExpectationsGiven ( spec ) ; updateStateOf ( spec ) ; } } catch ( Throwable t ) { if ( unavailableNode && t . getMessage ( ) . startsWith ( ""UNAVAILABLE"" ) ) { log . info ( ""Node {} is unavailable as expected!"" , HapiPropertySource . asAccountString ( node . get ( ) ) ) ; return Optional . empty ( ) ; } if ( verboseLoggingOn ) { log . warn ( spec . logPrefix ( ) + this + "" failed - {}"" , t ) ; } else if ( ! loggingOff ) { log . warn ( spec . logPrefix ( ) + this + "" failed {}!"" , t . getMessage ( ) ) ; } return Optional . of ( t ) ; } if ( unavailableNode ) { String message = String . format ( ""Node %s is NOT unavailable as expected!!!"" , HapiPropertySource . asAccountString ( node . get ( ) ) ) ; log . error ( message ) ; return Optional . of ( new RuntimeException ( message ) ) ; } return Optional . empty ( ) ; } }","log . warn ( spec . logPrefix ( ) + this + "" failed {}!"" , t . getMessage ( ) ) ;",Meaningful
"public class A { private void registerServletDescriptor ( ServletDescriptor servletDescriptor ) { try { servletDescriptor . register ( httpService ) ; } catch ( RuntimeException e ) { LOG . error ( ""Unable to register servlet on mount point '{}', the resource is not registered in the init method"" , servletDescriptor . getAlias ( ) , e ) ; } catch ( ServletException e ) { LOG . error ( ""Unable to mount servlet on mount point '{}', either it was already registered under the same alias or the init method throws an exception"" , servletDescriptor . getAlias ( ) , e ) ; } catch ( NamespaceException e ) { LOG . error ( ""Unable to mount servlet on mount point '{}', another resource is already bound to this alias"" , servletDescriptor . getAlias ( ) , e ) ; } } }","public class A { private void registerServletDescriptor ( ServletDescriptor servletDescriptor ) { try { servletDescriptor . register ( httpService ) ; } catch ( RuntimeException e ) { LOG . error ( ""Registration of ServletDescriptor under mountpoint {} fails with unexpected RuntimeException!"" , servletDescriptor . getAlias ( ) , e ) ; } catch ( ServletException e ) { LOG . error ( ""Unable to mount servlet on mount point '{}', either it was already registered under the same alias or the init method throws an exception"" , servletDescriptor . getAlias ( ) , e ) ; } catch ( NamespaceException e ) { LOG . error ( ""Unable to mount servlet on mount point '{}', another resource is already bound to this alias"" , servletDescriptor . getAlias ( ) , e ) ; } } }","LOG . error ( ""Registration of ServletDescriptor under mountpoint {} fails with unexpected RuntimeException!"" , servletDescriptor . getAlias ( ) , e ) ;",Meaningful
"public class A { @ Override public void cancelAllActiveSearches ( ) { for ( SearchTask next : myIdToSearchTask . values ( ) ) { ourLog . info ( ""Canceling next search task: {}"" , next ) ; next . requestImmediateAbort ( ) ; AsyncUtil . awaitLatchAndIgnoreInterrupt ( next . getCompletionLatch ( ) , 30 , TimeUnit . SECONDS ) ; } } }","public class A { @ Override public void cancelAllActiveSearches ( ) { for ( SearchTask next : myIdToSearchTask . values ( ) ) { ourLog . info ( ""Requesting immediate abort of search: {}"" , next . getSearch ( ) . getUuid ( ) ) ; next . requestImmediateAbort ( ) ; AsyncUtil . awaitLatchAndIgnoreInterrupt ( next . getCompletionLatch ( ) , 30 , TimeUnit . SECONDS ) ; } } }","ourLog . info ( ""Requesting immediate abort of search: {}"" , next . getSearch ( ) . getUuid ( ) ) ;",Meaningful
"public class A { private void setXATerminator ( final XATerminator terminator ) { this . xaTerminator = terminator ; logger . debug ( ""XA terminator: {}"" , terminator ) ; } }","public class A { private void setXATerminator ( final XATerminator terminator ) { this . xaTerminator = terminator ; logger . debug ( ""XATerminator set."" ) ; } }","logger . debug ( ""XATerminator set."" ) ;",Meaningful
"public class A { private void flushBuffer ( final boolean scheduled ) { if ( _queue . isEmpty ( ) ) { return ; } if ( ! scheduled ) { if ( _queue . size ( ) < _maxBatchSize ) { logger . debug ( ""Batch ignored, flush operation not scheduled and queue is not full"" ) ; return ; } } final List < BatchEntry < ? , O > > entries = new ArrayList < > ( _maxBatchSize ) ; final int batchSize = _queue . drainTo ( entries ) ; if ( batchSize == 0 ) { logger . debug ( ""Batch #{} - no more job needed"" , batchSize ) ; return ; } final int batchNumber = _batchNo . incrementAndGet ( ) ; logger . info ( ""Batch #{} - Preparing {} entries, scheduled={}"" , batchNumber , batchSize , scheduled ) ; final Object [ ] input = new Object [ batchSize ] ; for ( int i = 0 ; i < batchSize ; i ++ ) { input [ i ] = entries . get ( i ) . getInput ( ) ; } final BatchSource < I > source = new ArrayBatchSource < > ( input ) ; final BatchEntryBatchSink < O > sink = new BatchEntryBatchSink < > ( entries ) ; _transformation . map ( source , sink ) ; logger . info ( ""Batch #{} - Finished"" , batchNumber , batchSize ) ; } }","public class A { private void flushBuffer ( final boolean scheduled ) { if ( _queue . isEmpty ( ) ) { return ; } if ( ! scheduled ) { if ( _queue . size ( ) < _maxBatchSize ) { logger . debug ( ""Batch ignored, flush operation not scheduled and queue is not full"" ) ; return ; } } final List < BatchEntry < ? , O > > entries = new ArrayList < > ( _maxBatchSize ) ; final int batchSize = _queue . drainTo ( entries ) ; if ( batchSize == 0 ) { logger . debug ( ""Batch ignored, no elements left in queue"" ) ; return ; } final int batchNumber = _batchNo . incrementAndGet ( ) ; logger . info ( ""Batch #{} - Preparing {} entries, scheduled={}"" , batchNumber , batchSize , scheduled ) ; final Object [ ] input = new Object [ batchSize ] ; for ( int i = 0 ; i < batchSize ; i ++ ) { input [ i ] = entries . get ( i ) . getInput ( ) ; } final BatchSource < I > source = new ArrayBatchSource < > ( input ) ; final BatchEntryBatchSink < O > sink = new BatchEntryBatchSink < > ( entries ) ; _transformation . map ( source , sink ) ; logger . info ( ""Batch #{} - Finished"" , batchNumber , batchSize ) ; } }","logger . debug ( ""Batch ignored, no elements left in queue"" ) ;",Meaningful
"public class A { public void registerClient ( ClientProxyMembershipID proxyID , int maximumTimeBetweenPings ) { if ( ! clientMaximumTimeBetweenPings . containsKey ( proxyID ) ) { clientMaximumTimeBetweenPings . putIfAbsent ( proxyID , maximumTimeBetweenPings ) ; } if ( ! clientHeartbeats . containsKey ( proxyID ) ) { if ( null == clientHeartbeats . putIfAbsent ( proxyID , new AtomicLong ( System . currentTimeMillis ( ) ) ) ) { if ( stats != null ) { stats . incClientRegisterRequests ( ) ; } if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""registered {} services for proxy, {}"" , clientHeartbeats . size ( ) , proxyID ) ; } } } } }","public class A { public void registerClient ( ClientProxyMembershipID proxyID , int maximumTimeBetweenPings ) { if ( ! clientMaximumTimeBetweenPings . containsKey ( proxyID ) ) { clientMaximumTimeBetweenPings . putIfAbsent ( proxyID , maximumTimeBetweenPings ) ; } if ( ! clientHeartbeats . containsKey ( proxyID ) ) { if ( null == clientHeartbeats . putIfAbsent ( proxyID , new AtomicLong ( System . currentTimeMillis ( ) ) ) ) { if ( stats != null ) { stats . incClientRegisterRequests ( ) ; } if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""ClientHealthMonitor: Registering client with member id {}"" , proxyID ) ; } } } } }","logger . debug ( ""ClientHealthMonitor: Registering client with member id {}"" , proxyID ) ;",Meaningful
"public class A { private @ Nullable String getChannelTypeId ( ChannelUID channelUID ) { Channel channel = thing . getChannel ( channelUID ) ; if ( channel == null ) { logger . debug ( ""channel is null"" ) ; return null ; } ChannelTypeUID channelTypeUID = channel . getChannelTypeUID ( ) ; if ( channelTypeUID == null ) { logger . debug ( ""channel typeUID for channel {} is null"" , channelUID ) ; return null ; } String channelTypeId = channelTypeUID . getId ( ) ; if ( channelTypeId == null ) { logger . debug ( ""channelTypeId for channelTypeUID {} is null"" , channelTypeUID ) ; return null ; } return channelTypeId ; } }","public class A { private @ Nullable String getChannelTypeId ( ChannelUID channelUID ) { Channel channel = thing . getChannel ( channelUID ) ; if ( channel == null ) { logger . debug ( ""channel is null"" ) ; return null ; } ChannelTypeUID channelTypeUID = channel . getChannelTypeUID ( ) ; if ( channelTypeUID == null ) { logger . debug ( ""channelTypeUID for channel {} is null"" , channel ) ; return null ; } String channelTypeId = channelTypeUID . getId ( ) ; if ( channelTypeId == null ) { logger . debug ( ""channelTypeId for channelTypeUID {} is null"" , channelTypeUID ) ; return null ; } return channelTypeId ; } }","logger . debug ( ""channelTypeUID for channel {} is null"" , channel ) ;",Meaningful
"public class A { public void loginEvent ( final User user , final String surrogateIdentifier , final String ip , final String errorMessage ) { log . info ( ""Login for user {}/{}:{} - {}"" , user . getEmail ( ) , user . getNick ( ) , ip , errorMessage ) ; } }","public class A { public void loginEvent ( final User user , final String surrogateIdentifier , final String ip , final String errorMessage ) { LOGGER . info ( ""Login statut: {} / user: {} - {} / surrogate: {} / IP: {} / errorMessage: {}"" , errorMessage != null ? StatusCode . KO : StatusCode . OK , user . getIdentifier ( ) , user . getEmail ( ) , surrogateIdentifier , ip , errorMessage ) ; } }","LOGGER . info ( ""Login statut: {} / user: {} - {} / surrogate: {} / IP: {} / errorMessage: {}"" , errorMessage != null ? StatusCode . KO : StatusCode . OK , user . getIdentifier ( ) , user . getEmail ( ) , surrogateIdentifier , ip , errorMessage ) ;",Meaningful
"public class A { private void onUploadSuccess ( final String result ) { final FileUploadResponse response = FileUploadResponse . parse ( result ) ; final FileVersionDTO fileVersion = response . getFileVersion ( ) ; final MonitoredPointDTO point = response . getMonitoredPoint ( ) ; if ( fileVersion == null ) { throw new UnsupportedOperationException ( ""Invalid required file version."" ) ; } fileVersion . setAvailable ( true ) ; final ReportReference reportReference = new ReportReference ( fileVersion ) ; reportReference . setId ( fileVersion . getId ( ) ) ; reportReference . setName ( view . getFileUploadButtonField ( ) . getValue ( ) ) ; reportReference . setLastEditDate ( new Date ( ) ) ; reportReference . setEditorName ( auth ( ) . getUserShortName ( ) ) ; reportReference . setFlexibleElementLabel ( ( String ) view . getElementField ( ) . getValue ( ) ) ; reportReference . setPhaseName ( ( String ) view . getPhaseField ( ) . getValue ( ) ) ; N10N . infoNotif ( I18N . CONSTANTS . infoConfirmation ( ) , I18N . CONSTANTS . flexibleElementFilesListUploadFileConfirm ( ) ) ; if ( point != null ) { if ( Log . isDebugEnabled ( ) ) { Log . debug ( ""Add report for file version {}"" , fileVersion . getId ( ) ) ; } N10N . infoNotif ( I18N . CONSTANTS . infoConfirmation ( ) , I18N . CONSTANTS . monitoredPointAddConfirm ( ) ) ; } eventBus . fireEvent ( new UpdateEvent ( UpdateEvent . REPORT_DOCUMENTS_UPDATE , reportReference ) ) ; hideView ( ) ; } }","public class A { private void onUploadSuccess ( final String result ) { final FileUploadResponse response = FileUploadResponse . parse ( result ) ; final FileVersionDTO fileVersion = response . getFileVersion ( ) ; final MonitoredPointDTO point = response . getMonitoredPoint ( ) ; if ( fileVersion == null ) { throw new UnsupportedOperationException ( ""Invalid required file version."" ) ; } fileVersion . setAvailable ( true ) ; final ReportReference reportReference = new ReportReference ( fileVersion ) ; reportReference . setId ( fileVersion . getId ( ) ) ; reportReference . setName ( view . getFileUploadButtonField ( ) . getValue ( ) ) ; reportReference . setLastEditDate ( new Date ( ) ) ; reportReference . setEditorName ( auth ( ) . getUserShortName ( ) ) ; reportReference . setFlexibleElementLabel ( ( String ) view . getElementField ( ) . getValue ( ) ) ; reportReference . setPhaseName ( ( String ) view . getPhaseField ( ) . getValue ( ) ) ; N10N . infoNotif ( I18N . CONSTANTS . infoConfirmation ( ) , I18N . CONSTANTS . flexibleElementFilesListUploadFileConfirm ( ) ) ; if ( point != null ) { if ( Log . isDebugEnabled ( ) ) { Log . debug ( ""Adds a monitored point '"" + point . getLabel ( ) + ""' to container with id #"" + containerId + ""."" ) ; } N10N . infoNotif ( I18N . CONSTANTS . infoConfirmation ( ) , I18N . CONSTANTS . monitoredPointAddConfirm ( ) ) ; } eventBus . fireEvent ( new UpdateEvent ( UpdateEvent . REPORT_DOCUMENTS_UPDATE , reportReference ) ) ; hideView ( ) ; } }","Log . debug ( ""Adds a monitored point '"" + point . getLabel ( ) + ""' to container with id #"" + containerId + ""."" ) ;",Meaningful
"public class A { @ Override public PooledDataSourceFactory getDataSourceFactory ( DPCAttributionConfiguration configuration ) { logger . debug ( ""Getting database factory for dPCAttributionConfiguration: "" + configuration . getUserDataBase ( ) ) ; return configuration . getDatabase ( ) ; } }","public class A { @ Override public PooledDataSourceFactory getDataSourceFactory ( DPCAttributionConfiguration configuration ) { logger . debug ( ""Connecting to database {} at {}"" , configuration . getDatabase ( ) . getDriverClass ( ) , configuration . getDatabase ( ) . getUrl ( ) ) ; return configuration . getDatabase ( ) ; } }","logger . debug ( ""Connecting to database {} at {}"" , configuration . getDatabase ( ) . getDriverClass ( ) , configuration . getDatabase ( ) . getUrl ( ) ) ;",Meaningful
"public class A { private AddressBook parseHistoricAddressBooks ( ) { var fileDataEntries = 0 ; long currentConsensusTimestamp = 0 ; AddressBook lastAddressBook = null ; var pageSize = 1000 ; List < FileData > fileDataList = fileDataRepository . findAddressBooksAfter ( currentConsensusTimestamp , pageSize ) ; while ( ! CollectionUtils . isEmpty ( fileDataList ) ) { log . info ( ""Retrieved {} file_data rows for address book processing"" , fileDataList . size ( ) ) ; for ( FileData fileData : fileDataList ) { if ( fileData . getFileData ( ) != null && fileData . getFileData ( ) . length > 0 ) { lastAddressBook = parse ( fileData ) ; fileDataEntries ++ ; } currentConsensusTimestamp = fileData . getConsensusTimestamp ( ) ; } fileDataList = fileDataRepository . findAddressBooksAfter ( currentConsensusTimestamp , pageSize ) ; } log . info ( ""Retrieved {} number of files for address book processing"" , fileDataEntries . size ( ) ) ; return lastAddressBook ; } }","public class A { private AddressBook parseHistoricAddressBooks ( ) { var fileDataEntries = 0 ; long currentConsensusTimestamp = 0 ; AddressBook lastAddressBook = null ; var pageSize = 1000 ; List < FileData > fileDataList = fileDataRepository . findAddressBooksAfter ( currentConsensusTimestamp , pageSize ) ; while ( ! CollectionUtils . isEmpty ( fileDataList ) ) { log . info ( ""Retrieved {} file_data rows for address book processing"" , fileDataList . size ( ) ) ; for ( FileData fileData : fileDataList ) { if ( fileData . getFileData ( ) != null && fileData . getFileData ( ) . length > 0 ) { lastAddressBook = parse ( fileData ) ; fileDataEntries ++ ; } currentConsensusTimestamp = fileData . getConsensusTimestamp ( ) ; } fileDataList = fileDataRepository . findAddressBooksAfter ( currentConsensusTimestamp , pageSize ) ; } log . info ( ""Processed {} historic address books"" , fileDataEntries ) ; return lastAddressBook ; } }","log . info ( ""Processed {} historic address books"" , fileDataEntries ) ;",Meaningful
"public class A { public static String map ( String principal ) { for ( AuthorizationMapping am : authorizationMapping ) { if ( Log . isDebugEnabled ( ) ) { Log . debug ( ""Mapping "" + am . getClass ( ) . getName ( ) + "" => "" + principal ) ; } String username = am . map ( principal ) ; if ( ! username . equals ( principal ) ) { return username ; } } return principal ; } }","public class A { public static String map ( String principal ) { for ( AuthorizationMapping am : authorizationMapping ) { if ( Log . isDebugEnabled ( ) ) { Log . debug ( ""AuthorizationManager: Trying "" + am . name ( ) + "".map("" + principal + "")"" ) ; } String username = am . map ( principal ) ; if ( ! username . equals ( principal ) ) { return username ; } } return principal ; } }","Log . debug ( ""AuthorizationManager: Trying "" + am . name ( ) + "".map("" + principal + "")"" ) ;",Meaningful
"public class A { private Publisher getOrCreatePublisher ( String topic ) { if ( publisherMap . containsKey ( topic ) ) { return publisherMap . get ( topic ) ; } else { try { admin . createTopicIfNotExists ( topic ) ; ProjectTopicName topicName = ProjectTopicName . of ( pubSubSettings . getProjectId ( ) , topic ) ; Publisher publisher = Publisher . newBuilder ( topicName ) . setCredentialsProvider ( pubSubSettings . getCredentialsProvider ( ) ) . build ( ) ; publisherMap . put ( topic , publisher ) ; return publisher ; } catch ( IOException e ) { log . error ( ""Failed to create Publisher for the topic."" , e ) ; throw new RuntimeException ( ""Failed to create Publisher for the topic."" , e ) ; } } } }","public class A { private Publisher getOrCreatePublisher ( String topic ) { if ( publisherMap . containsKey ( topic ) ) { return publisherMap . get ( topic ) ; } else { try { admin . createTopicIfNotExists ( topic ) ; ProjectTopicName topicName = ProjectTopicName . of ( pubSubSettings . getProjectId ( ) , topic ) ; Publisher publisher = Publisher . newBuilder ( topicName ) . setCredentialsProvider ( pubSubSettings . getCredentialsProvider ( ) ) . build ( ) ; publisherMap . put ( topic , publisher ) ; return publisher ; } catch ( IOException e ) { log . error ( ""Failed to create Publisher for the topic [{}]."" , topic , e ) ; throw new RuntimeException ( ""Failed to create Publisher for the topic."" , e ) ; } } } }","log . error ( ""Failed to create Publisher for the topic [{}]."" , topic , e ) ;",Meaningful
"public class A { @ Override public void open ( ) { try { Class . forName ( IGNITE_JDBC_DRIVER_NAME ) ; } catch ( ClassNotFoundException e ) { logger . error ( ""Driver for property "" + IGNITE_JDBC_DRIVER_NAME + "" not found, make sure it is for client threads"" ) ; connEx = e ; return ; } try { logger . info ( ""connect to "" + getProperty ( IGNITE_JDBC_URL ) ) ; conn = DriverManager . getConnection ( getProperty ( IGNITE_JDBC_URL ) ) ; connEx = null ; logger . info ( ""Successfully created JDBC connection"" ) ; } catch ( Exception e ) { logger . error ( ""Can't open connection: "" , e ) ; connEx = e ; } } }","public class A { @ Override public void open ( ) { try { Class . forName ( IGNITE_JDBC_DRIVER_NAME ) ; } catch ( ClassNotFoundException e ) { logger . error ( ""Can't find Ignite JDBC driver"" , e ) ; connEx = e ; return ; } try { logger . info ( ""connect to "" + getProperty ( IGNITE_JDBC_URL ) ) ; conn = DriverManager . getConnection ( getProperty ( IGNITE_JDBC_URL ) ) ; connEx = null ; logger . info ( ""Successfully created JDBC connection"" ) ; } catch ( Exception e ) { logger . error ( ""Can't open connection: "" , e ) ; connEx = e ; } } }","logger . error ( ""Can't find Ignite JDBC driver"" , e ) ;",Meaningful
"public class A { public void onFailure ( Throwable failure ) { submitDialog . hide ( ) ; failure . printStackTrace ( System . err ) ; MessageDialog . error ( failure . getMessage ( ) , failure ) ; } }","public class A { public void onFailure ( Throwable failure ) { submitDialog . hide ( ) ; failure . printStackTrace ( System . err ) ; Dialog . error ( ""Server-side Production Error"" , failure . getMessage ( ) ) ; } }","Dialog . error ( ""Server-side Production Error"" , failure . getMessage ( ) ) ;",Meaningful
"public class A { @ Override public ConnectorPageSource createPageSource ( ConnectorTransactionHandle transaction , ConnectorSession session , ConnectorSplit split , ConnectorTableHandle table , List < ColumnHandle > columns , DynamicFilter dynamicFilter ) { log . debug ( ""createPageSource for %s"" , table ) ; BigQuerySplit bigQuerySplit = ( BigQuerySplit ) split ; checkArgument ( bigQuerySplit . getColumns ( ) . isEmpty ( ) || bigQuerySplit . getColumns ( ) . equals ( columns ) , ""Requested columns %s do not match list in split %s"" , columns , bigQuerySplit . getColumns ( ) ) ; if ( bigQuerySplit . representsEmptyProjection ( ) ) { return new BigQueryEmptyProjectionPageSource ( bigQuerySplit . getEmptyRowsToGenerate ( ) ) ; } List < BigQueryColumnHandle > bigQueryColumnHandles = columns . stream ( ) . map ( BigQueryColumnHandle . class :: cast ) . collect ( toImmutableList ( ) ) ; return new BigQueryResultPageSource ( bigQueryStorageClientFactory , maxReadRowsRetries , bigQuerySplit , bigQueryColumnHandles ) ; } }","public class A { @ Override public ConnectorPageSource createPageSource ( ConnectorTransactionHandle transaction , ConnectorSession session , ConnectorSplit split , ConnectorTableHandle table , List < ColumnHandle > columns , DynamicFilter dynamicFilter ) { log . debug ( ""createPageSource(transaction=%s, session=%s, split=%s, table=%s, columns=%s)"" , transaction , session , split , table , columns ) ; BigQuerySplit bigQuerySplit = ( BigQuerySplit ) split ; checkArgument ( bigQuerySplit . getColumns ( ) . isEmpty ( ) || bigQuerySplit . getColumns ( ) . equals ( columns ) , ""Requested columns %s do not match list in split %s"" , columns , bigQuerySplit . getColumns ( ) ) ; if ( bigQuerySplit . representsEmptyProjection ( ) ) { return new BigQueryEmptyProjectionPageSource ( bigQuerySplit . getEmptyRowsToGenerate ( ) ) ; } List < BigQueryColumnHandle > bigQueryColumnHandles = columns . stream ( ) . map ( BigQueryColumnHandle . class :: cast ) . collect ( toImmutableList ( ) ) ; return new BigQueryResultPageSource ( bigQueryStorageClientFactory , maxReadRowsRetries , bigQuerySplit , bigQueryColumnHandles ) ; } }","log . debug ( ""createPageSource(transaction=%s, session=%s, split=%s, table=%s, columns=%s)"" , transaction , session , split , table , columns ) ;",Meaningful
"public class A { @ Override public boolean isSameFile ( FileObject a , FileObject b ) { log . debug ( "" isSame file: {}"" , a ) ; return a . equals ( b ) ; } }","public class A { @ Override public boolean isSameFile ( FileObject a , FileObject b ) { logger . debug ( ""isSameFile({},{})"" , a , b ) ; return a . equals ( b ) ; } }","logger . debug ( ""isSameFile({},{})"" , a , b ) ;",Meaningful
"public class A { public String getInstanceState ( String instanceId ) { LOGGER . debug ( ""getInstanceState('{}') entered"" , instanceId ) ; DescribeInstancesResult result = getEC2 ( ) . describeInstances ( new DescribeInstancesRequest ( ) . withInstanceIds ( instanceId ) ) ; List < Reservation > reservations = result . getReservations ( ) ; Set < Instance > instances = new HashSet < Instance > ( ) ; for ( Reservation reservation : reservations ) { instances . addAll ( reservation . getInstances ( ) ) ; if ( instances . size ( ) > 0 ) { String state = instances . iterator ( ) . next ( ) . getState ( ) . getName ( ) ; LOGGER . debug ( ""getInstanceState('{}') found in the next one: {}"" , instanceId , state ) ; return state ; } } LOGGER . debug ( ""getInstanceState('{}') left"" , instanceId ) ; return null ; } }","public class A { public String getInstanceState ( String instanceId ) { LOGGER . debug ( ""getInstanceState('{}') entered"" , instanceId ) ; DescribeInstancesResult result = getEC2 ( ) . describeInstances ( new DescribeInstancesRequest ( ) . withInstanceIds ( instanceId ) ) ; List < Reservation > reservations = result . getReservations ( ) ; Set < Instance > instances = new HashSet < Instance > ( ) ; for ( Reservation reservation : reservations ) { instances . addAll ( reservation . getInstances ( ) ) ; if ( instances . size ( ) > 0 ) { String state = instances . iterator ( ) . next ( ) . getState ( ) . getName ( ) ; LOGGER . debug ( "" InstanceState: {}"" , state ) ; return state ; } } LOGGER . debug ( ""getInstanceState('{}') left"" , instanceId ) ; return null ; } }","LOGGER . debug ( "" InstanceState: {}"" , state ) ;",Meaningful
"public class A { @ Override public User addUserWithWorkflow ( long companyId , boolean autoPassword , String password1 , String password2 , boolean autoScreenName , String screenName , String emailAddress , Locale locale , String firstName , String middleName , String lastName , long prefixId , long suffixId , boolean male , int birthdayMonth , int birthdayDay , int birthdayYear , String jobTitle , long [ ] groupIds , long [ ] organizationIds , long [ ] roleIds , long [ ] userGroupIds , boolean sendEmail , ServiceContext serviceContext ) throws PortalException { long creatorUserId = 0 ; try { creatorUserId = getGuestOrUserId ( ) ; } catch ( PrincipalException principalException ) { if ( _log . isWarnEnabled ( ) ) { _log . warn ( StringBundler . concat ( ""Unable to add user with email "" , principalException . getMessage ( ) ) , principalException ) ; } } checkAddUserPermission ( creatorUserId , companyId , emailAddress , groupIds , organizationIds , roleIds , userGroupIds , serviceContext ) ; User user = userLocalService . addUserWithWorkflow ( creatorUserId , companyId , autoPassword , password1 , password2 , autoScreenName , screenName , emailAddress , locale , firstName , middleName , lastName , prefixId , suffixId , male , birthdayMonth , birthdayDay , birthdayYear , jobTitle , groupIds , organizationIds , roleIds , userGroupIds , sendEmail , serviceContext ) ; checkMembership ( new long [ ] { user . getUserId ( ) } , groupIds , organizationIds , roleIds , userGroupIds ) ; propagateMembership ( new long [ ] { user . getUserId ( ) } , groupIds , organizationIds , roleIds , userGroupIds ) ; return user ; } }","public class A { @ Override public User addUserWithWorkflow ( long companyId , boolean autoPassword , String password1 , String password2 , boolean autoScreenName , String screenName , String emailAddress , Locale locale , String firstName , String middleName , String lastName , long prefixId , long suffixId , boolean male , int birthdayMonth , int birthdayDay , int birthdayYear , String jobTitle , long [ ] groupIds , long [ ] organizationIds , long [ ] roleIds , long [ ] userGroupIds , boolean sendEmail , ServiceContext serviceContext ) throws PortalException { long creatorUserId = 0 ; try { creatorUserId = getGuestOrUserId ( ) ; } catch ( PrincipalException principalException ) { if ( _log . isWarnEnabled ( ) ) { _log . warn ( ""Unable to get guest or current user ID"" , principalException ) ; } } checkAddUserPermission ( creatorUserId , companyId , emailAddress , groupIds , organizationIds , roleIds , userGroupIds , serviceContext ) ; User user = userLocalService . addUserWithWorkflow ( creatorUserId , companyId , autoPassword , password1 , password2 , autoScreenName , screenName , emailAddress , locale , firstName , middleName , lastName , prefixId , suffixId , male , birthdayMonth , birthdayDay , birthdayYear , jobTitle , groupIds , organizationIds , roleIds , userGroupIds , sendEmail , serviceContext ) ; checkMembership ( new long [ ] { user . getUserId ( ) } , groupIds , organizationIds , roleIds , userGroupIds ) ; propagateMembership ( new long [ ] { user . getUserId ( ) } , groupIds , organizationIds , roleIds , userGroupIds ) ; return user ; } }","_log . warn ( ""Unable to get guest or current user ID"" , principalException ) ;",Meaningful
"public class A { @ Override public String toJsonString ( ) throws IOException { if ( partitionToWriteStats . containsKey ( null ) ) { LOG . info ( ""partition path is null for "" + partitionToWriteStats . get ( null ) ) ; partitionToWriteStats . remove ( null ) ; } if ( partitionToReplaceFileIds . containsKey ( null ) ) { LOG . info ( ""partition path is null for "" + partitionToWriteStats . get ( null ) ) ; partitionToReplaceFileIds . remove ( null ) ; } return getObjectMapper ( ) . writerWithDefaultPrettyPrinter ( ) . writeValueAsString ( this ) ; } }","public class A { @ Override public String toJsonString ( ) throws IOException { if ( partitionToWriteStats . containsKey ( null ) ) { LOG . info ( ""partition path is null for "" + partitionToWriteStats . get ( null ) ) ; partitionToWriteStats . remove ( null ) ; } if ( partitionToReplaceFileIds . containsKey ( null ) ) { LOG . info ( ""partition path is null for "" + partitionToReplaceFileIds . get ( null ) ) ; partitionToReplaceFileIds . remove ( null ) ; } return getObjectMapper ( ) . writerWithDefaultPrettyPrinter ( ) . writeValueAsString ( this ) ; } }","LOG . info ( ""partition path is null for "" + partitionToReplaceFileIds . get ( null ) ) ;",Meaningful
"public class A { @ RequestMapping ( value = ""/{mvnGroup:.+}/{artifact:.+}/{version:.+}/{type:.+}/{qname:.+}"" , method = RequestMethod . GET ) public void getConstructSourceForGav ( @ PathVariable String mvnGroup , @ PathVariable String artifact , @ PathVariable String version , @ PathVariable String type , @ PathVariable String qname , HttpServletResponse response ) { Path file = null ; JavaId jid = null ; JavaId def_ctx = null ; try { jid = JavaId . getJavaId ( type , qname ) ; def_ctx = JavaId . getCompilationUnit ( jid ) ; log . debug ( ""Determined compilation unit "" + def_ctx + "" for qname ["" + qname + ""]"" ) ; file = ClassDownloader . getInstance ( ) . getClass ( mvnGroup , artifact , version , def_ctx . getQualifiedName ( ) , ClassDownloader . Format . JAVA ) ; if ( file == null ) { response . sendError ( HttpServletResponse . SC_NOT_FOUND , ""Cannot retrieve class ["" + def_ctx . getQualifiedName ( ) + ""]"" ) ; response . flushBuffer ( ) ; } else { FileAnalyzer jfa = FileAnalyzerFactory . buildFileAnalyzer ( file . toFile ( ) ) ; if ( jfa . containsConstruct ( jid ) ) { response . setContentType ( ""text/plain"" ) ; final String source_code = jfa . getConstruct ( jid ) . getContent ( ) ; response . getWriter ( ) . print ( source_code ) ; response . flushBuffer ( ) ; } else { response . sendError ( HttpServletResponse . SC_NOT_FOUND , ""Cannot find construct "" + jid ) ; response . flushBuffer ( ) ; } try { Files . delete ( file ) ; } catch ( Exception e ) { log . error ( ""Error: "" + e . getMessage ( ) , e ) ; } } } catch ( IllegalArgumentException iae ) { log . error ( ""Error: "" + iae . getMessage ( ) , iae ) ; throw new RuntimeException ( iae . getMessage ( ) ) ; } catch ( FileNotFoundException e ) { log . error ( ""Error: "" + e . getMessage ( ) , e ) ; throw new RuntimeException ( ""Cannot read file ["" + file + ""]"" ) ; } catch ( IOException e ) { log . error ( ""Error: "" + e . getMessage ( ) , e ) ; throw new RuntimeException ( ""IO exception when reading file ["" + file + ""]"" ) ; } catch ( Exception e ) { log . error ( ""Error: "" + e . getMessage ( ) , e ) ; throw new RuntimeException ( e . getClass ( ) . getSimpleName ( ) + "" when writing file to output stream: "" + e . getMessage ( ) ) ; } } }","public class A { @ RequestMapping ( value = ""/{mvnGroup:.+}/{artifact:.+}/{version:.+}/{type:.+}/{qname:.+}"" , method = RequestMethod . GET ) public void getConstructSourceForGav ( @ PathVariable String mvnGroup , @ PathVariable String artifact , @ PathVariable String version , @ PathVariable String type , @ PathVariable String qname , HttpServletResponse response ) { Path file = null ; JavaId jid = null ; JavaId def_ctx = null ; try { jid = JavaId . getJavaId ( type , qname ) ; def_ctx = JavaId . getCompilationUnit ( jid ) ; log . debug ( ""Determined compilation unit "" + def_ctx + "" for qname ["" + qname + ""]"" ) ; file = ClassDownloader . getInstance ( ) . getClass ( mvnGroup , artifact , version , def_ctx . getQualifiedName ( ) , ClassDownloader . Format . JAVA ) ; if ( file == null ) { response . sendError ( HttpServletResponse . SC_NOT_FOUND , ""Cannot retrieve class ["" + def_ctx . getQualifiedName ( ) + ""]"" ) ; response . flushBuffer ( ) ; } else { FileAnalyzer jfa = FileAnalyzerFactory . buildFileAnalyzer ( file . toFile ( ) ) ; if ( jfa . containsConstruct ( jid ) ) { response . setContentType ( ""text/plain"" ) ; final String source_code = jfa . getConstruct ( jid ) . getContent ( ) ; response . getWriter ( ) . print ( source_code ) ; response . flushBuffer ( ) ; } else { response . sendError ( HttpServletResponse . SC_NOT_FOUND , ""Cannot find construct "" + jid ) ; response . flushBuffer ( ) ; } try { Files . delete ( file ) ; } catch ( Exception e ) { log . error ( ""Cannot delete file ["" + file + ""]: "" + e . getMessage ( ) ) ; } } } catch ( IllegalArgumentException iae ) { log . error ( ""Error: "" + iae . getMessage ( ) , iae ) ; throw new RuntimeException ( iae . getMessage ( ) ) ; } catch ( FileNotFoundException e ) { log . error ( ""Error: "" + e . getMessage ( ) , e ) ; throw new RuntimeException ( ""Cannot read file ["" + file + ""]"" ) ; } catch ( IOException e ) { log . error ( ""Error: "" + e . getMessage ( ) , e ) ; throw new RuntimeException ( ""IO exception when reading file ["" + file + ""]"" ) ; } catch ( Exception e ) { log . error ( ""Error: "" + e . getMessage ( ) , e ) ; throw new RuntimeException ( e . getClass ( ) . getSimpleName ( ) + "" when writing file to output stream: "" + e . getMessage ( ) ) ; } } }","log . error ( ""Cannot delete file ["" + file + ""]: "" + e . getMessage ( ) ) ;",Meaningful
"public class A { @ Override public void jvmRouteSwitchover ( String fromJvmRoute , String toJvmRoute ) { try { Node oldNode = getBalancerContext ( ) . jvmRouteToSipNode . get ( fromJvmRoute ) ; Node newNode = getBalancerContext ( ) . jvmRouteToSipNode . get ( toJvmRoute ) ; if ( oldNode != null && newNode != null ) { int updatedRoutes = 0 ; for ( String key : userToMap . keySet ( ) ) { Node n = userToMap . get ( key ) ; if ( n . equals ( oldNode ) ) { userToMap . replace ( key , newNode ) ; updatedRoutes ++ ; } } if ( logger . isInfoEnabled ( ) ) { logger . info ( ""Switchover occured where oldNode="" + oldNode + "" and newNode="" + newNode + "" with "" + updatedRoutes + "" updated routes."" ) ; } } else { if ( logger . isInfoEnabled ( ) ) { logger . info ( ""Switchover failed where fromJvmRoute="" + fromJvmRoute + "" and toJvmRoute="" + toJvmRoute ) ; } } } catch ( Throwable t ) { if ( logger . isInfoEnabled ( ) ) { logger . info ( ""Switchover failed where fromJvmRoute="" + fromJvmRoute + "" and toJvmRoute="" + toJvmRoute ) ; logger . info ( ""This is not a fatal failure, logging the reason for the failure "" , t ) ; } } } }","public class A { @ Override public void jvmRouteSwitchover ( String fromJvmRoute , String toJvmRoute ) { try { Node oldNode = getBalancerContext ( ) . jvmRouteToSipNode . get ( fromJvmRoute ) ; Node newNode = getBalancerContext ( ) . jvmRouteToSipNode . get ( toJvmRoute ) ; if ( oldNode != null && newNode != null ) { int updatedRoutes = 0 ; for ( String key : userToMap . keySet ( ) ) { Node n = userToMap . get ( key ) ; if ( n . equals ( oldNode ) ) { userToMap . replace ( key , newNode ) ; updatedRoutes ++ ; } } if ( logger . isInfoEnabled ( ) ) { logger . info ( ""Switchover occured where fromJvmRoute="" + fromJvmRoute + "" and toJvmRoute="" + toJvmRoute + "" with "" + updatedRoutes + "" updated routes."" ) ; } } else { if ( logger . isInfoEnabled ( ) ) { logger . info ( ""Switchover failed where fromJvmRoute="" + fromJvmRoute + "" and toJvmRoute="" + toJvmRoute ) ; } } } catch ( Throwable t ) { if ( logger . isInfoEnabled ( ) ) { logger . info ( ""Switchover failed where fromJvmRoute="" + fromJvmRoute + "" and toJvmRoute="" + toJvmRoute ) ; logger . info ( ""This is not a fatal failure, logging the reason for the failure "" , t ) ; } } } }","logger . info ( ""Switchover occured where fromJvmRoute="" + fromJvmRoute + "" and toJvmRoute="" + toJvmRoute + "" with "" + updatedRoutes + "" updated routes."" ) ;",Meaningful
"public class A { private void setupSpringSecurity ( ) { ClassPathXmlApplicationContext appContext = new ClassPathXmlApplicationContext ( new String [ ] { SPRING_SECURITY_METADATA } ) ; System . setProperty ( ""spring-beans-config"" , SPRING_SECURITY_METADATA ) ; AuthZ authZ = AuthZ . get ( ) ; txManager = ( org . springframework . jdbc . datasource . DataSourceTransactionManager ) appContext . getBean ( ""transactionManager"" ) ; if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Spring Security Started"" ) ; } } }","public class A { private void setupSpringSecurity ( ) { ClassPathXmlApplicationContext appContext = new ClassPathXmlApplicationContext ( new String [ ] { SPRING_SECURITY_METADATA } ) ; System . setProperty ( ""spring-beans-config"" , SPRING_SECURITY_METADATA ) ; AuthZ authZ = AuthZ . get ( ) ; txManager = ( org . springframework . jdbc . datasource . DataSourceTransactionManager ) appContext . getBean ( ""transactionManager"" ) ; if ( logger . isDebugEnabled ( ) ) { logger . debug ( ""Spring Security setup complete."" ) ; } } }","logger . debug ( ""Spring Security setup complete."" ) ;",Meaningful
"public class A { @ Override public Optional < ScriptParameterDesignTrace > get ( ScriptParameterDesignTraceKey scriptParameterDesignTraceKey ) { try { String query = ""SELECT SCRIPT_PAR_VAL FROM "" + getMetadataRepository ( ) . getTableNameByLabel ( ""ScriptParameterDesignTraces"" ) + "" WHERE "" + "" RUN_ID = "" + SQLTools . getStringForSQL ( scriptParameterDesignTraceKey . getRunId ( ) ) + "" AND "" + "" PRC_ID = "" + SQLTools . getStringForSQL ( scriptParameterDesignTraceKey . getProcessId ( ) ) + "";"" ; CachedRowSet cachedRowSet = getMetadataRepository ( ) . executeQuery ( query , ""reader"" ) ; if ( cachedRowSet . size ( ) == 0 ) { return Optional . empty ( ) ; } else if ( cachedRowSet . size ( ) > 1 ) { LOGGER . warn ( MessageFormat . format ( ""Found multiple implementations for ScriptParameterDesignTrace {0}. Returning first implementation"" , scriptParameterDesignTraceKey . toString ( ) ) ) ; } cachedRowSet . next ( ) ; return Optional . of ( new ScriptParameterDesignTrace ( scriptParameterDesignTraceKey , cachedRowSet . getString ( ""SCRIPT_PAR_VAL"" ) ) ) ; } catch ( SQLException e ) { throw new RuntimeException ( e ) ; } } }","public class A { @ Override public Optional < ScriptParameterDesignTrace > get ( ScriptParameterDesignTraceKey scriptParameterDesignTraceKey ) { try { String query = ""SELECT SCRIPT_PAR_VAL FROM "" + getMetadataRepository ( ) . getTableNameByLabel ( ""ScriptParameterDesignTraces"" ) + "" WHERE "" + "" RUN_ID = "" + SQLTools . getStringForSQL ( scriptParameterDesignTraceKey . getRunId ( ) ) + "" AND "" + "" PRC_ID = "" + SQLTools . getStringForSQL ( scriptParameterDesignTraceKey . getProcessId ( ) ) + "";"" ; CachedRowSet cachedRowSet = getMetadataRepository ( ) . executeQuery ( query , ""reader"" ) ; if ( cachedRowSet . size ( ) == 0 ) { return Optional . empty ( ) ; } else if ( cachedRowSet . size ( ) > 1 ) { LOGGER . warn ( MessageFormat . format ( ""Found multiple implementations for ActionTrace {0}. Returning first implementation"" , scriptParameterDesignTraceKey . toString ( ) ) ) ; } cachedRowSet . next ( ) ; return Optional . of ( new ScriptParameterDesignTrace ( scriptParameterDesignTraceKey , cachedRowSet . getString ( ""SCRIPT_PAR_VAL"" ) ) ) ; } catch ( SQLException e ) { throw new RuntimeException ( e ) ; } } }","LOGGER . warn ( MessageFormat . format ( ""Found multiple implementations for ActionTrace {0}. Returning first implementation"" , scriptParameterDesignTraceKey . toString ( ) ) ) ;",Meaningful
"public class A { @ Override protected void afterReturning ( AopMethodInvocation aopMethodInvocation , Object [ ] arguments , Object result ) throws Throwable { if ( result == null ) { return ; } if ( CompanyThreadLocal . isDeleteInProcess ( ) || IndexWriterHelperUtil . isIndexReadOnly ( ) ) { if ( _log . isDebugEnabled ( ) ) { if ( CompanyThreadLocal . isDeleteInProcess ( ) ) { _log . debug ( ""Skip indexing because the index is read in process"" ) ; } else if ( IndexWriterHelperUtil . isIndexReadOnly ( ) ) { _log . debug ( ""Skip indexing because the index is read only"" ) ; } } return ; } IndexableContext indexableContext = aopMethodInvocation . getAdviceMethodContext ( ) ; String name = indexableContext . _name ; Indexer < Object > indexer = IndexerRegistryUtil . getIndexer ( name ) ; if ( indexer == null ) { return ; } if ( IndexWriterHelperUtil . isIndexReadOnly ( indexer . getClassName ( ) ) ) { if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Skipping indexing read only index for "" + indexer . getClassName ( ) ) ; } return ; } int serviceContextIndex = indexableContext . _serviceContextIndex ; if ( serviceContextIndex >= 0 ) { ServiceContext serviceContext = ( ServiceContext ) arguments [ serviceContextIndex ] ; if ( ( serviceContext != null ) && ! serviceContext . isIndexingEnabled ( ) ) { return ; } } if ( indexableContext . _indexableType == IndexableType . DELETE ) { indexer . delete ( result ) ; } else { indexer . reindex ( result ) ; } } }","public class A { @ Override protected void afterReturning ( AopMethodInvocation aopMethodInvocation , Object [ ] arguments , Object result ) throws Throwable { if ( result == null ) { return ; } if ( CompanyThreadLocal . isDeleteInProcess ( ) || IndexWriterHelperUtil . isIndexReadOnly ( ) ) { if ( _log . isDebugEnabled ( ) ) { if ( CompanyThreadLocal . isDeleteInProcess ( ) ) { _log . debug ( ""Skip indexing because company delete is in process"" ) ; } else if ( IndexWriterHelperUtil . isIndexReadOnly ( ) ) { _log . debug ( ""Skip indexing because the index is read only"" ) ; } } return ; } IndexableContext indexableContext = aopMethodInvocation . getAdviceMethodContext ( ) ; String name = indexableContext . _name ; Indexer < Object > indexer = IndexerRegistryUtil . getIndexer ( name ) ; if ( indexer == null ) { return ; } if ( IndexWriterHelperUtil . isIndexReadOnly ( indexer . getClassName ( ) ) ) { if ( _log . isDebugEnabled ( ) ) { _log . debug ( ""Skipping indexing read only index for "" + indexer . getClassName ( ) ) ; } return ; } int serviceContextIndex = indexableContext . _serviceContextIndex ; if ( serviceContextIndex >= 0 ) { ServiceContext serviceContext = ( ServiceContext ) arguments [ serviceContextIndex ] ; if ( ( serviceContext != null ) && ! serviceContext . isIndexingEnabled ( ) ) { return ; } } if ( indexableContext . _indexableType == IndexableType . DELETE ) { indexer . delete ( result ) ; } else { indexer . reindex ( result ) ; } } }","_log . debug ( ""Skip indexing because company delete is in process"" ) ;",Meaningful
"public class A { public static RawOccurrenceRecord parseRecord ( byte [ ] xml , OccurrenceSchemaType schemaType , String unitQualifier ) { RawOccurrenceRecord result = null ; List < RawOccurrenceRecord > records = parseRecord ( xml , schemaType ) ; if ( records == null || records . isEmpty ( ) ) { log . warn ( ""Could not parse any records from given xml - returning null."" ) ; } else if ( records . size ( ) == 1 ) { result = records . get ( 0 ) ; } else if ( unitQualifier == null ) { log . warn ( ""Got multiple records from given xml, but no unitQualifier set - returning first record as a guess."" ) ; result = records . get ( 0 ) ; } else { for ( RawOccurrenceRecord record : records ) { if ( record . getScientificName ( ) . equals ( unitQualifier ) ) { result = record ; break ; } } if ( result == null ) { log . warn ( ""Could not parse the given xml"" ) ; } } return result ; } }","public class A { public static RawOccurrenceRecord parseRecord ( byte [ ] xml , OccurrenceSchemaType schemaType , String unitQualifier ) { RawOccurrenceRecord result = null ; List < RawOccurrenceRecord > records = parseRecord ( xml , schemaType ) ; if ( records == null || records . isEmpty ( ) ) { log . warn ( ""Could not parse any records from given xml - returning null."" ) ; } else if ( records . size ( ) == 1 ) { result = records . get ( 0 ) ; } else if ( unitQualifier == null ) { log . warn ( ""Got multiple records from given xml, but no unitQualifier set - returning first record as a guess."" ) ; result = records . get ( 0 ) ; } else { for ( RawOccurrenceRecord record : records ) { if ( record . getScientificName ( ) . equals ( unitQualifier ) ) { result = record ; break ; } } if ( result == null ) { log . warn ( ""Got multiple records from xml but none matched unitQualifier - returning null"" ) ; } } return result ; } }","log . warn ( ""Got multiple records from xml but none matched unitQualifier - returning null"" ) ;",Meaningful
"public class A { void closeInternal ( long lastWriteTime , boolean explicit ) throws CIFSException { SmbTreeHandleImpl t = this . tree ; try { if ( t != null && isValid ( ) ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Close tree {}"" , PathUtil . getPath ( t ) ) ; } if ( t . isSMB2 ( ) ) { Smb2CloseRequest req = new Smb2CloseRequest ( this . cfg , this . fileId ) ; t . send ( req , RequestParam . NO_RETRY ) ; } else { t . send ( new SmbComClose ( this . cfg , this . fid , lastWriteTime ) , new SmbComBlankResponse ( this . cfg ) , RequestParam . NO_RETRY ) ; } } } finally { this . open = false ; if ( t != null ) { t . release ( ) ; } this . tree = null ; } } }","public class A { void closeInternal ( long lastWriteTime , boolean explicit ) throws CIFSException { SmbTreeHandleImpl t = this . tree ; try { if ( t != null && isValid ( ) ) { if ( log . isDebugEnabled ( ) ) { log . debug ( ""Closing file handle "" + this ) ; } if ( t . isSMB2 ( ) ) { Smb2CloseRequest req = new Smb2CloseRequest ( this . cfg , this . fileId ) ; t . send ( req , RequestParam . NO_RETRY ) ; } else { t . send ( new SmbComClose ( this . cfg , this . fid , lastWriteTime ) , new SmbComBlankResponse ( this . cfg ) , RequestParam . NO_RETRY ) ; } } } finally { this . open = false ; if ( t != null ) { t . release ( ) ; } this . tree = null ; } } }","log . debug ( ""Closing file handle "" + this ) ;",Meaningful
"public class A { public void messageReceived ( int senderId , long requestId , int response , boolean shouldDrop ) { if ( shouldDrop ) { synchronized ( clientRequestIdRequestInfoMap ) { clientRequestIdRequestInfoMap . notifyAll ( ) ; } return ; } AckSignalFlag responseFlag = flowControl . getAckSignalFlag ( response ) ; if ( responseFlag == AckSignalFlag . DUPLICATE_REQUEST ) { LOG . info ( ""messageReceived: Already completed request (taskId = "" + senderId + "", requestId = "" + requestId + "")"" ) ; } else if ( responseFlag != AckSignalFlag . NEW_REQUEST ) { throw new IllegalStateException ( ""messageReceived: Got illegal response "" + response ) ; } RequestInfo requestInfo = clientRequestIdRequestInfoMap . remove ( new ClientRequestId ( senderId , requestId ) ) ; if ( requestInfo == null ) { LOG . info ( ""messageReceived: Already received response for (taskId = "" + senderId + "", requestId = "" + requestId + "")"" ) ; } else { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""messageReceived: Received response: "" + requestInfo ) ; } flowControl . messageAckReceived ( senderId , requestId , response ) ; synchronized ( clientRequestIdRequestInfoMap ) { clientRequestIdRequestInfoMap . notifyAll ( ) ; } } } }","public class A { public void messageReceived ( int senderId , long requestId , int response , boolean shouldDrop ) { if ( shouldDrop ) { synchronized ( clientRequestIdRequestInfoMap ) { clientRequestIdRequestInfoMap . notifyAll ( ) ; } return ; } AckSignalFlag responseFlag = flowControl . getAckSignalFlag ( response ) ; if ( responseFlag == AckSignalFlag . DUPLICATE_REQUEST ) { LOG . info ( ""messageReceived: Already completed request (taskId = "" + senderId + "", requestId = "" + requestId + "")"" ) ; } else if ( responseFlag != AckSignalFlag . NEW_REQUEST ) { throw new IllegalStateException ( ""messageReceived: Got illegal response "" + response ) ; } RequestInfo requestInfo = clientRequestIdRequestInfoMap . remove ( new ClientRequestId ( senderId , requestId ) ) ; if ( requestInfo == null ) { LOG . info ( ""messageReceived: Already received response for (taskId = "" + senderId + "", requestId = "" + requestId + "")"" ) ; } else { if ( LOG . isDebugEnabled ( ) ) { LOG . debug ( ""messageReceived: Completed (taskId = "" + senderId + "")"" + requestInfo + "". Waiting on "" + clientRequestIdRequestInfoMap . size ( ) + "" requests"" ) ; } flowControl . messageAckReceived ( senderId , requestId , response ) ; synchronized ( clientRequestIdRequestInfoMap ) { clientRequestIdRequestInfoMap . notifyAll ( ) ; } } } }","LOG . debug ( ""messageReceived: Completed (taskId = "" + senderId + "")"" + requestInfo + "". Waiting on "" + clientRequestIdRequestInfoMap . size ( ) + "" requests"" ) ;",Meaningful
"public class A { @ Test public void playerAutoTerminationTest ( ) throws Exception { String id = uploadFile ( new File ( ""test-files/sample.txt"" ) ) ; log . debug ( ""File uploaded"" ) ; RepositoryHttpPlayer player = getRepository ( ) . findRepositoryItemById ( id ) . createRepositoryHttpPlayer ( ) ; player . setAutoTerminationTimeout ( 1000 ) ; RestTemplate template = getRestTemplate ( ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( ""Request 1 Passed"" ) ; Thread . sleep ( 300 ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( ""Request 2 Passed"" ) ; Thread . sleep ( 1500 ) ; assertEquals ( HttpStatus . NOT_FOUND , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( ""File not found"" ) ; } }","public class A { @ Test public void playerAutoTerminationTest ( ) throws Exception { String id = uploadFile ( new File ( ""test-files/sample.txt"" ) ) ; log . debug ( ""File uploaded"" ) ; RepositoryHttpPlayer player = getRepository ( ) . findRepositoryItemById ( id ) . createRepositoryHttpPlayer ( ) ; player . setAutoTerminationTimeout ( 1000 ) ; RestTemplate template = getRestTemplate ( ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( ""Request 1 Passed"" ) ; Thread . sleep ( 300 ) ; assertEquals ( HttpStatus . OK , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( ""Request 2 Passed"" ) ; Thread . sleep ( 1500 ) ; assertEquals ( HttpStatus . NOT_FOUND , template . getForEntity ( player . getURL ( ) , byte [ ] . class ) . getStatusCode ( ) ) ; log . debug ( ""Request 3 Passed"" ) ; } }","log . debug ( ""Request 3 Passed"" ) ;",Meaningful
"public class A { private static List < IdentityMapping > getMappings ( final NiFiProperties properties , final String patternPrefix , final String valuePrefix , final String transformPrefix , final Supplier < String > getSubject ) { final List < IdentityMapping > mappings = new ArrayList < > ( ) ; for ( String propertyName : properties . getPropertyKeys ( ) ) { if ( StringUtils . startsWith ( propertyName , patternPrefix ) ) { final String key = StringUtils . substringAfter ( propertyName , patternPrefix ) ; final String identityPattern = properties . getProperty ( propertyName ) ; if ( StringUtils . isBlank ( identityPattern ) ) { LOGGER . warn ( ""{} Mapping property {} was found, but was empty"" , new Object [ ] { getSubject . get ( ) , propertyName } ) ; continue ; } final String identityValueProperty = valuePrefix + key ; final String identityValue = properties . getProperty ( identityValueProperty ) ; if ( StringUtils . isBlank ( identityValue ) ) { LOGGER . warn ( ""{} Mapping property {} was found, but corresponding value {} was not found"" , new Object [ ] { getSubject . get ( ) , propertyName , identityValueProperty } ) ; continue ; } final String identityTransformProperty = transformPrefix + key ; String rawIdentityTransform = properties . getProperty ( identityTransformProperty ) ; if ( StringUtils . isBlank ( rawIdentityTransform ) ) { LOGGER . debug ( ""{} Mapping property {} was found, but corresponding transform {} was not found"" , new Object [ ] { identityTransformProperty , propertyName , identityTransformProperty } ) ; rawIdentityTransform = Transform . NONE . name ( ) ; } final Transform identityTransform ; try { identityTransform = Transform . valueOf ( rawIdentityTransform ) ; } catch ( final IllegalArgumentException iae ) { LOGGER . warn ( ""{} Mapping property {} was found, but corresponding transform {} was not valid. Allowed values {}"" , new Object [ ] { getSubject . get ( ) , propertyName , rawIdentityTransform , StringUtils . join ( Transform . values ( ) , "", "" ) } ) ; continue ; } final IdentityMapping identityMapping = new IdentityMapping ( key , Pattern . compile ( identityPattern ) , identityValue , identityTransform ) ; mappings . add ( identityMapping ) ; LOGGER . debug ( ""Found {} Mapping with key = {}, pattern = {}, value = {}, transform = {}"" , new Object [ ] { getSubject . get ( ) , key , identityPattern , identityValue , rawIdentityTransform } ) ; } } Collections . sort ( mappings , new Comparator < IdentityMapping > ( ) { @ Override public int compare ( IdentityMapping m1 , IdentityMapping m2 ) { return m1 . getKey ( ) . compareTo ( m2 . getKey ( ) ) ; } } ) ; return mappings ; } }","public class A { private static List < IdentityMapping > getMappings ( final NiFiProperties properties , final String patternPrefix , final String valuePrefix , final String transformPrefix , final Supplier < String > getSubject ) { final List < IdentityMapping > mappings = new ArrayList < > ( ) ; for ( String propertyName : properties . getPropertyKeys ( ) ) { if ( StringUtils . startsWith ( propertyName , patternPrefix ) ) { final String key = StringUtils . substringAfter ( propertyName , patternPrefix ) ; final String identityPattern = properties . getProperty ( propertyName ) ; if ( StringUtils . isBlank ( identityPattern ) ) { LOGGER . warn ( ""{} Mapping property {} was found, but was empty"" , new Object [ ] { getSubject . get ( ) , propertyName } ) ; continue ; } final String identityValueProperty = valuePrefix + key ; final String identityValue = properties . getProperty ( identityValueProperty ) ; if ( StringUtils . isBlank ( identityValue ) ) { LOGGER . warn ( ""{} Mapping property {} was found, but corresponding value {} was not found"" , new Object [ ] { getSubject . get ( ) , propertyName , identityValueProperty } ) ; continue ; } final String identityTransformProperty = transformPrefix + key ; String rawIdentityTransform = properties . getProperty ( identityTransformProperty ) ; if ( StringUtils . isBlank ( rawIdentityTransform ) ) { LOGGER . debug ( ""{} Mapping property {} was found, but no transform was present. Using NONE."" , new Object [ ] { getSubject . get ( ) , propertyName } ) ; rawIdentityTransform = Transform . NONE . name ( ) ; } final Transform identityTransform ; try { identityTransform = Transform . valueOf ( rawIdentityTransform ) ; } catch ( final IllegalArgumentException iae ) { LOGGER . warn ( ""{} Mapping property {} was found, but corresponding transform {} was not valid. Allowed values {}"" , new Object [ ] { getSubject . get ( ) , propertyName , rawIdentityTransform , StringUtils . join ( Transform . values ( ) , "", "" ) } ) ; continue ; } final IdentityMapping identityMapping = new IdentityMapping ( key , Pattern . compile ( identityPattern ) , identityValue , identityTransform ) ; mappings . add ( identityMapping ) ; LOGGER . debug ( ""Found {} Mapping with key = {}, pattern = {}, value = {}, transform = {}"" , new Object [ ] { getSubject . get ( ) , key , identityPattern , identityValue , rawIdentityTransform } ) ; } } Collections . sort ( mappings , new Comparator < IdentityMapping > ( ) { @ Override public int compare ( IdentityMapping m1 , IdentityMapping m2 ) { return m1 . getKey ( ) . compareTo ( m2 . getKey ( ) ) ; } } ) ; return mappings ; } }","LOGGER . debug ( ""{} Mapping property {} was found, but no transform was present. Using NONE."" , new Object [ ] { getSubject . get ( ) , propertyName } ) ;",Meaningful
"public class A { public static VocabularyCandidates findVocabulariesForUrl ( String resourceId , Function < String , List < Vocabulary > > searchInPersistence ) throws URISyntaxException { final String searchString = new URI ( resourceId . replace ( "" "" , ""%20"" ) ) . getHost ( ) ; final List < Vocabulary > searchedVocabularies = searchInPersistence . apply ( searchString ) ; final List < Vocabulary > candidates ; if ( searchedVocabularies == null ) { candidates = Collections . emptyList ( ) ; } else { candidates = searchedVocabularies . stream ( ) . filter ( vocabulary -> vocabularyMatchesUri ( resourceId , vocabulary ) ) . collect ( Collectors . toList ( ) ) ; } if ( candidates . isEmpty ( ) ) { LOGGER . info ( ""No vocabularies found for uri {}"" , resourceId ) ; } if ( candidates . size ( ) > 1 && LOGGER . isWarnEnabled ( ) ) { LOGGER . warn ( ""Could not find the candidate for vocabulary {}, using first candidate"" , resourceId ) ; } return new VocabularyCandidates ( candidates ) ; } }","public class A { public static VocabularyCandidates findVocabulariesForUrl ( String resourceId , Function < String , List < Vocabulary > > searchInPersistence ) throws URISyntaxException { final String searchString = new URI ( resourceId . replace ( "" "" , ""%20"" ) ) . getHost ( ) ; final List < Vocabulary > searchedVocabularies = searchInPersistence . apply ( searchString ) ; final List < Vocabulary > candidates ; if ( searchedVocabularies == null ) { candidates = Collections . emptyList ( ) ; } else { candidates = searchedVocabularies . stream ( ) . filter ( vocabulary -> vocabularyMatchesUri ( resourceId , vocabulary ) ) . collect ( Collectors . toList ( ) ) ; } if ( candidates . isEmpty ( ) ) { LOGGER . info ( ""No vocabularies found for uri {}"" , resourceId ) ; } if ( candidates . size ( ) > 1 && LOGGER . isWarnEnabled ( ) ) { LOGGER . warn ( ""Multiple vocabularies found for uri {}: {}"" , resourceId , candidates . stream ( ) . map ( Vocabulary :: getName ) . collect ( Collectors . joining ( "", "" ) ) ) ; } return new VocabularyCandidates ( candidates ) ; } }","LOGGER . warn ( ""Multiple vocabularies found for uri {}: {}"" , resourceId , candidates . stream ( ) . map ( Vocabulary :: getName ) . collect ( Collectors . joining ( "", "" ) ) ) ;",Meaningful
"public class A { public boolean update ( Rp rp ) { try { RpObject rpObj = new RpObject ( getDnForRp ( rp . getOxdId ( ) ) , rp . getOxdId ( ) , Jackson2 . serializeWithoutNulls ( rp ) ) ; this . persistenceEntryManager . merge ( rpObj ) ; LOG . debug ( ""RP updated successfully. RP : {} "" , rp ) ; return true ; } catch ( Exception e ) { LOG . error ( ""Failed to update RP: {} "" , rp , e ) ; } return false ; } }","public class A { public boolean update ( Rp rp ) { try { RpObject rpObj = new RpObject ( getDnForRp ( rp . getOxdId ( ) ) , rp . getOxdId ( ) , Jackson2 . serializeWithoutNulls ( rp ) ) ; this . persistenceEntryManager . merge ( rpObj ) ; LOG . debug ( ""RP updated successfully. RP : {} "" , rpObj ) ; return true ; } catch ( Exception e ) { LOG . error ( ""Failed to update RP: {} "" , rp , e ) ; } return false ; } }","LOG . debug ( ""RP updated successfully. RP : {} "" , rpObj ) ;",Meaningful
"public class A { public void startPeriodicFetching ( ) { if ( _running . getAndSet ( true ) ) { _log . debug ( ""Skipping PeriodicFetching splits as splitFetcher is running."" ) ; return ; } _log . debug ( ""Starting PeriodicFetching Splits ..."" ) ; _scheduledFuture = _scheduledExecutorService . scheduleWithFixedDelay ( _splitFetcher . get ( ) , 0L , _refreshEveryNSeconds . get ( ) , TimeUnit . SECONDS ) ; } }","public class A { public void startPeriodicFetching ( ) { if ( _running . getAndSet ( true ) ) { _log . debug ( ""Splits PeriodicFetching is running..."" ) ; return ; } _log . debug ( ""Starting PeriodicFetching Splits ..."" ) ; _scheduledFuture = _scheduledExecutorService . scheduleWithFixedDelay ( _splitFetcher . get ( ) , 0L , _refreshEveryNSeconds . get ( ) , TimeUnit . SECONDS ) ; } }","_log . debug ( ""Splits PeriodicFetching is running..."" ) ;",Meaningful
"public class A { @ Override public void userEventTriggered ( ChannelHandlerContext ctx , Object evt ) { if ( evt instanceof IdleStateEvent ) { IdleStateEvent idleStateEvent = ( IdleStateEvent ) evt ; if ( idleStateEvent . state ( ) == IdleState . READER_IDLE ) { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( ""channel {} is read idle."" , ctx . channel ( ) . remoteAddress ( ) ) ; } try { String serverAddress = NetUtil . toStringAddress ( ctx . channel ( ) . remoteAddress ( ) ) ; clientChannelManager . invalidateObject ( serverAddress , ctx . channel ( ) ) ; } catch ( Exception exx ) { LOGGER . error ( exx . getMessage ( ) ) ; } finally { clientChannelManager . releaseChannel ( ctx . channel ( ) , getAddressFromContext ( ctx ) ) ; } } if ( idleStateEvent == IdleStateEvent . WRITER_IDLE_STATE_EVENT ) { try { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( ""will send ping msg,channel {}"" , ctx . channel ( ) ) ; } AbstractNettyRemotingClient . this . sendAsyncRequest ( ctx . channel ( ) , HeartbeatMessage . PING ) ; } catch ( Throwable throwable ) { LOGGER . error ( ""send request error: {}"" , throwable . getMessage ( ) , throwable ) ; } } } } }","public class A { @ Override public void userEventTriggered ( ChannelHandlerContext ctx , Object evt ) { if ( evt instanceof IdleStateEvent ) { IdleStateEvent idleStateEvent = ( IdleStateEvent ) evt ; if ( idleStateEvent . state ( ) == IdleState . READER_IDLE ) { if ( LOGGER . isInfoEnabled ( ) ) { LOGGER . info ( ""channel {} read idle."" , ctx . channel ( ) ) ; } try { String serverAddress = NetUtil . toStringAddress ( ctx . channel ( ) . remoteAddress ( ) ) ; clientChannelManager . invalidateObject ( serverAddress , ctx . channel ( ) ) ; } catch ( Exception exx ) { LOGGER . error ( exx . getMessage ( ) ) ; } finally { clientChannelManager . releaseChannel ( ctx . channel ( ) , getAddressFromContext ( ctx ) ) ; } } if ( idleStateEvent == IdleStateEvent . WRITER_IDLE_STATE_EVENT ) { try { if ( LOGGER . isDebugEnabled ( ) ) { LOGGER . debug ( ""will send ping msg,channel {}"" , ctx . channel ( ) ) ; } AbstractNettyRemotingClient . this . sendAsyncRequest ( ctx . channel ( ) , HeartbeatMessage . PING ) ; } catch ( Throwable throwable ) { LOGGER . error ( ""send request error: {}"" , throwable . getMessage ( ) , throwable ) ; } } } } }","LOGGER . info ( ""channel {} read idle."" , ctx . channel ( ) ) ;",Meaningful
"public class A { @ Override public void handleEvent ( Event event ) { logger . debug ( ""handleEvent - topic: {}"" , event . getTopic ( ) ) ; if ( event . getTopic ( ) . equals ( RawSocketTransport . ERROR_EVENT_TOPIC ) ) { Exception error = ( Exception ) event . getProperty ( RawSocketTransport . ERROR_PROPERTY_NAME ) ; if ( error != null ) { errorHappened ( error ) ; } } else if ( event . getTopic ( ) . equals ( CMD_RCVD_EVENT_TOPIC ) ) { String response = ( String ) event . getProperty ( COMMAND_PROPERTY_NAME ) ; if ( response != null ) { commandReceived ( response ) ; } } } }","public class A { @ Override public void handleEvent ( Event event ) { log . debug ( ""Event received"" ) ; if ( event . getTopic ( ) . equals ( RawSocketTransport . ERROR_EVENT_TOPIC ) ) { Exception error = ( Exception ) event . getProperty ( RawSocketTransport . ERROR_PROPERTY_NAME ) ; if ( error != null ) { errorHappened ( error ) ; } } else if ( event . getTopic ( ) . equals ( CMD_RCVD_EVENT_TOPIC ) ) { String response = ( String ) event . getProperty ( COMMAND_PROPERTY_NAME ) ; if ( response != null ) { commandReceived ( response ) ; } } } }","log . debug ( ""Event received"" ) ;",Meaningful
"public class A { public void execute ( String strDxExCode , JSONObject jObj ) throws Exception { socketLogger . info ( ""DxT031.execute : "" + strDxExCode ) ; byte [ ] sendBuff = null ; String strErrCode = """" ; String strErrMsg = """" ; String strSuccessCode = ""0"" ; String startLen = ""2500"" ; String seek = ""0"" ; String strRESTORE_SN = ( String ) jObj . get ( ProtocolID . RESTORE_SN ) ; String strLogFileName = ""restore_dump_"" + strRESTORE_SN + "".log"" ; String logDir = ""../logs/pg_resLog/"" ; JSONObject outputObj = new JSONObject ( ) ; try { File inFile = new File ( logDir , strLogFileName ) ; HashMap hp = FileUtil . getRandomAccessFileView ( inFile , Integer . parseInt ( startLen ) , Integer . parseInt ( seek ) , 0 ) ; socketLogger . info ( ""DxT031.execute : "" + strDxExCode ) ; outputObj . put ( ProtocolID . DX_EX_CODE , strDxExCode ) ; outputObj . put ( ProtocolID . RESULT_CODE , strSuccessCode ) ; outputObj . put ( ProtocolID . ERR_CODE , strErrCode ) ; outputObj . put ( ProtocolID . ERR_MSG , strErrMsg ) ; outputObj . put ( ProtocolID . RESULT_DATA , hp . get ( ""file_desc"" ) ) ; inFile = null ; send ( TotalLengthBit , outputObj . toString ( ) . getBytes ( ) ) ; } catch ( Exception e ) { errLogger . error ( ""DxT031 {} "" , e . toString ( ) ) ; outputObj . put ( ProtocolID . DX_EX_CODE , TranCodeType . DxT031 ) ; outputObj . put ( ProtocolID . RESULT_CODE , ""1"" ) ; outputObj . put ( ProtocolID . ERR_CODE , TranCodeType . DxT031 ) ; outputObj . put ( ProtocolID . ERR_MSG , ""DxT031 Error ["" + e . toString ( ) + ""]"" ) ; sendBuff = outputObj . toString ( ) . getBytes ( ) ; send ( 4 , sendBuff ) ; } finally { outputObj = null ; sendBuff = null ; } } }","public class A { public void execute ( String strDxExCode , JSONObject jObj ) throws Exception { socketLogger . info ( ""DxT031.execute : "" + strDxExCode ) ; byte [ ] sendBuff = null ; String strErrCode = """" ; String strErrMsg = """" ; String strSuccessCode = ""0"" ; String startLen = ""2500"" ; String seek = ""0"" ; String strRESTORE_SN = ( String ) jObj . get ( ProtocolID . RESTORE_SN ) ; String strLogFileName = ""restore_dump_"" + strRESTORE_SN + "".log"" ; String logDir = ""../logs/pg_resLog/"" ; JSONObject outputObj = new JSONObject ( ) ; try { File inFile = new File ( logDir , strLogFileName ) ; HashMap hp = FileUtil . getRandomAccessFileView ( inFile , Integer . parseInt ( startLen ) , Integer . parseInt ( seek ) , 0 ) ; socketLogger . info ( ""strFileView : "" + hp . get ( ""file_desc"" ) ) ; outputObj . put ( ProtocolID . DX_EX_CODE , strDxExCode ) ; outputObj . put ( ProtocolID . RESULT_CODE , strSuccessCode ) ; outputObj . put ( ProtocolID . ERR_CODE , strErrCode ) ; outputObj . put ( ProtocolID . ERR_MSG , strErrMsg ) ; outputObj . put ( ProtocolID . RESULT_DATA , hp . get ( ""file_desc"" ) ) ; inFile = null ; send ( TotalLengthBit , outputObj . toString ( ) . getBytes ( ) ) ; } catch ( Exception e ) { errLogger . error ( ""DxT031 {} "" , e . toString ( ) ) ; outputObj . put ( ProtocolID . DX_EX_CODE , TranCodeType . DxT031 ) ; outputObj . put ( ProtocolID . RESULT_CODE , ""1"" ) ; outputObj . put ( ProtocolID . ERR_CODE , TranCodeType . DxT031 ) ; outputObj . put ( ProtocolID . ERR_MSG , ""DxT031 Error ["" + e . toString ( ) + ""]"" ) ; sendBuff = outputObj . toString ( ) . getBytes ( ) ; send ( 4 , sendBuff ) ; } finally { outputObj = null ; sendBuff = null ; } } }","socketLogger . info ( ""strFileView : "" + hp . get ( ""file_desc"" ) ) ;",Meaningful
"public class A { @ Override public void contribute ( Document document , ObjectEntry objectEntry ) { try { _contribute ( document , objectEntry ) ; } catch ( Exception exception ) { if ( _log . isWarnEnabled ( ) ) { _log . warn ( exception , exception ) ; } } } }","public class A { @ Override public void contribute ( Document document , ObjectEntry objectEntry ) { try { _contribute ( document , objectEntry ) ; } catch ( Exception exception ) { if ( _log . isWarnEnabled ( ) ) { _log . warn ( ""Unable to index object entry "" + objectEntry . getObjectEntryId ( ) , exception ) ; } } } }","_log . warn ( ""Unable to index object entry "" + objectEntry . getObjectEntryId ( ) , exception ) ;",Meaningful
"public class A { @ Override public void execute ( final Map < String , Object > parameters ) throws FrameworkException { final String mode = ( String ) parameters . get ( ""mode"" ) ; if ( Boolean . FALSE . equals ( deploymentActive . get ( ) ) ) { try { deploymentActive . set ( true ) ; if ( ""export"" . equals ( mode ) ) { doExport ( parameters ) ; } else if ( ""import"" . equals ( mode ) ) { doImport ( parameters ) ; } else { logger . warn ( ""Skipping import as this is not currently active."" ) ; } } finally { deploymentActive . set ( false ) ; } } else { logger . warn ( ""Prevented deployment '{}' while another deployment is active."" , mode ) ; publishWarningMessage ( ""Prevented deployment '"" + mode + ""'"" , ""Another deployment is currently active. Please wait until it is finished."" ) ; } } }","public class A { @ Override public void execute ( final Map < String , Object > parameters ) throws FrameworkException { final String mode = ( String ) parameters . get ( ""mode"" ) ; if ( Boolean . FALSE . equals ( deploymentActive . get ( ) ) ) { try { deploymentActive . set ( true ) ; if ( ""export"" . equals ( mode ) ) { doExport ( parameters ) ; } else if ( ""import"" . equals ( mode ) ) { doImport ( parameters ) ; } else { logger . warn ( ""Unsupported mode '{}'"" , mode ) ; } } finally { deploymentActive . set ( false ) ; } } else { logger . warn ( ""Prevented deployment '{}' while another deployment is active."" , mode ) ; publishWarningMessage ( ""Prevented deployment '"" + mode + ""'"" , ""Another deployment is currently active. Please wait until it is finished."" ) ; } } }","logger . warn ( ""Unsupported mode '{}'"" , mode ) ;",Meaningful
"public class A { @ Override protected void decode ( ChannelHandlerContext channelHandlerContext , T t , List < Object > list ) throws Exception { logger . trace ( ""Decoding {}"" , t ) ; registrations : for ( Iterator < HandlerRegistration > iter = this . registeredHandlers . iterator ( ) ; iter . hasNext ( ) ; ) { HandlerRegistration registration = iter . next ( ) ; if ( registration . isCancelled ( ) ) { logger . debug ( ""Registration {} cancelled even though it was already cancelled ({})"" , registration , registration . isCancelled ( ) ) ; iter . remove ( ) ; continue ; } final Instant now = Instant . now ( ) ; if ( registration . getTimeoutAt ( ) . isBefore ( now ) ) { logger . debug ( ""Removing {} as its timed out (timeout of {} was set till {} and now is {})"" , registration , registration . getTimeout ( ) , registration . getTimeoutAt ( ) , now ) ; registration . getOnTimeoutConsumer ( ) . accept ( new TimeoutException ( ) ) ; iter . remove ( ) ; continue ; } logger . trace ( ""Checking handler {} for Object of type {}"" , registration , t . getClass ( ) . getSimpleName ( ) ) ; if ( registration . getExpectClazz ( ) . isInstance ( t ) ) { logger . trace ( ""Handler {} has right expected type {}, checking condition"" , registration , registration . getExpectClazz ( ) . getSimpleName ( ) ) ; Deque < Either < Function < ? , ? > , Predicate < ? > > > commands = registration . getCommands ( ) ; Object instance = t ; for ( Iterator < Either < Function < ? , ? > , Predicate < ? > > > iterator = commands . iterator ( ) ; iterator . hasNext ( ) ; ) { Either < Function < ? , ? > , Predicate < ? > > either = iterator . next ( ) ; if ( either . isLeft ( ) ) { Function unwrap = either . getLeft ( ) ; instance = unwrap . apply ( instance ) ; } else { Predicate predicate = either . get ( ) ; if ( predicate . test ( instance ) == false ) { logger . trace ( ""Registration {} with predicate {} does not match object {} (currently wrapped to {})"" , registration , predicate , t . getClass ( ) . getSimpleName ( ) , instance . getClass ( ) . getSimpleName ( ) ) ; continue registrations ; } } } logger . trace ( ""Handler {} accepts element {}, calling handle method"" , registration , t ) ; this . registeredHandlers . remove ( registration ) ; Consumer handler = registration . getPacketConsumer ( ) ; handler . accept ( instance ) ; registration . confirmHandled ( ) ; return ; } } logger . trace ( ""None of {} registered handlers could handle message {}, using default decode method"" , this . registeredHandlers . size ( ) , t ) ; protocolBase . decode ( new DefaultConversationContext < > ( channelHandlerContext , passive ) , t ) ; } }","public class A { @ Override protected void decode ( ChannelHandlerContext channelHandlerContext , T t , List < Object > list ) throws Exception { logger . trace ( ""Decoding {}"" , t ) ; registrations : for ( Iterator < HandlerRegistration > iter = this . registeredHandlers . iterator ( ) ; iter . hasNext ( ) ; ) { HandlerRegistration registration = iter . next ( ) ; if ( registration . isCancelled ( ) ) { logger . debug ( ""Removing {} as it was cancelled!"" , registration ) ; iter . remove ( ) ; continue ; } final Instant now = Instant . now ( ) ; if ( registration . getTimeoutAt ( ) . isBefore ( now ) ) { logger . debug ( ""Removing {} as its timed out (timeout of {} was set till {} and now is {})"" , registration , registration . getTimeout ( ) , registration . getTimeoutAt ( ) , now ) ; registration . getOnTimeoutConsumer ( ) . accept ( new TimeoutException ( ) ) ; iter . remove ( ) ; continue ; } logger . trace ( ""Checking handler {} for Object of type {}"" , registration , t . getClass ( ) . getSimpleName ( ) ) ; if ( registration . getExpectClazz ( ) . isInstance ( t ) ) { logger . trace ( ""Handler {} has right expected type {}, checking condition"" , registration , registration . getExpectClazz ( ) . getSimpleName ( ) ) ; Deque < Either < Function < ? , ? > , Predicate < ? > > > commands = registration . getCommands ( ) ; Object instance = t ; for ( Iterator < Either < Function < ? , ? > , Predicate < ? > > > iterator = commands . iterator ( ) ; iterator . hasNext ( ) ; ) { Either < Function < ? , ? > , Predicate < ? > > either = iterator . next ( ) ; if ( either . isLeft ( ) ) { Function unwrap = either . getLeft ( ) ; instance = unwrap . apply ( instance ) ; } else { Predicate predicate = either . get ( ) ; if ( predicate . test ( instance ) == false ) { logger . trace ( ""Registration {} with predicate {} does not match object {} (currently wrapped to {})"" , registration , predicate , t . getClass ( ) . getSimpleName ( ) , instance . getClass ( ) . getSimpleName ( ) ) ; continue registrations ; } } } logger . trace ( ""Handler {} accepts element {}, calling handle method"" , registration , t ) ; this . registeredHandlers . remove ( registration ) ; Consumer handler = registration . getPacketConsumer ( ) ; handler . accept ( instance ) ; registration . confirmHandled ( ) ; return ; } } logger . trace ( ""None of {} registered handlers could handle message {}, using default decode method"" , this . registeredHandlers . size ( ) , t ) ; protocolBase . decode ( new DefaultConversationContext < > ( channelHandlerContext , passive ) , t ) ; } }","logger . debug ( ""Removing {} as it was cancelled!"" , registration ) ;",Meaningful
"public class A { @ ParameterizedTest @ MethodSource ( ""getJobs"" ) public void testJobLifecycle ( JobInfo jobInfo , String operationName ) throws Exception { log . info ( ""test job:name={}, state={}"" , jobInfo . getName ( ) , operationName ) ; jobInfo = createJob ( jobInfo ) ; jobInfo = template ( ) . requestBody ( ""direct:getJob"" , jobInfo , JobInfo . class ) ; assertSame ( JobStateEnum . OPEN , jobInfo . getState ( ) , ""Job should be OPEN"" ) ; jobInfo = template ( ) . requestBody ( ""direct:closeJob"" , jobInfo , JobInfo . class ) ; assertSame ( JobStateEnum . CLOSED , jobInfo . getState ( ) , ""Job should be CLOSED"" ) ; jobInfo = template ( ) . requestBody ( ""direct:abortJob"" , jobInfo , JobInfo . class ) ; assertSame ( JobStateEnum . ABORTED , jobInfo . getState ( ) , ""Job should be ABORTED"" ) ; } }","public class A { @ ParameterizedTest @ MethodSource ( ""getJobs"" ) public void testJobLifecycle ( JobInfo jobInfo , String operationName ) throws Exception { log . info ( ""Testing Job lifecycle for {} of type {}"" , jobInfo . getOperation ( ) , jobInfo . getContentType ( ) ) ; jobInfo = createJob ( jobInfo ) ; jobInfo = template ( ) . requestBody ( ""direct:getJob"" , jobInfo , JobInfo . class ) ; assertSame ( JobStateEnum . OPEN , jobInfo . getState ( ) , ""Job should be OPEN"" ) ; jobInfo = template ( ) . requestBody ( ""direct:closeJob"" , jobInfo , JobInfo . class ) ; assertSame ( JobStateEnum . CLOSED , jobInfo . getState ( ) , ""Job should be CLOSED"" ) ; jobInfo = template ( ) . requestBody ( ""direct:abortJob"" , jobInfo , JobInfo . class ) ; assertSame ( JobStateEnum . ABORTED , jobInfo . getState ( ) , ""Job should be ABORTED"" ) ; } }","log . info ( ""Testing Job lifecycle for {} of type {}"" , jobInfo . getOperation ( ) , jobInfo . getContentType ( ) ) ;",Meaningful
"public class A { public synchronized void stop ( ) { for ( MLDriver driver : drivers ) { try { driver . stop ( ) ; } catch ( LensException e ) { log . error ( ""error stopping ML driver: "" + driver . getClass ( ) . getName ( ) , e ) ; } } drivers . clear ( ) ; udfStatusExpirySvc . shutdownNow ( ) ; log . info ( ""Stopped ML service"" ) ; } }","public class A { public synchronized void stop ( ) { for ( MLDriver driver : drivers ) { try { driver . stop ( ) ; } catch ( LensException e ) { log . error ( ""Failed to stop driver "" + driver , e ) ; } } drivers . clear ( ) ; udfStatusExpirySvc . shutdownNow ( ) ; log . info ( ""Stopped ML service"" ) ; } }","log . error ( ""Failed to stop driver "" + driver , e ) ;",Meaningful
"public class A { public void activateVfModule ( BuildingBlockExecution execution ) { GeneralBuildingBlock gBBInput = execution . getGeneralBuildingBlock ( ) ; RequestContext requestContext = gBBInput . getRequestContext ( ) ; ServiceInstance serviceInstance = null ; GenericVnf vnf = null ; VfModule vfModule = null ; try { serviceInstance = extractPojosForBB . extractByKey ( execution , ResourceKey . SERVICE_INSTANCE_ID ) ; vnf = extractPojosForBB . extractByKey ( execution , ResourceKey . GENERIC_VNF_ID ) ; vfModule = extractPojosForBB . extractByKey ( execution , ResourceKey . VF_MODULE_ID ) ; Customer customer = gBBInput . getCustomer ( ) ; CloudRegion cloudRegion = gBBInput . getCloudRegion ( ) ; SDNCRequest sdncRequest = new SDNCRequest ( ) ; GenericResourceApiVfModuleOperationInformation req = sdncVfModuleResources . activateVfModule ( vfModule , vnf , serviceInstance , customer , cloudRegion , requestContext , buildCallbackURI ( sdncRequest ) ) ; sdncRequest . setSDNCPayload ( req ) ; sdncRequest . setTopology ( SDNCTopology . VFMODULE ) ; execution . setVariable ( SDNC_REQUEST , sdncRequest ) ; } catch ( Exception ex ) { logger . error ( ""Exception occurred in AAI activateTasks"" , ex ) ; exceptionUtil . buildAndThrowWorkflowException ( execution , 7000 , ex ) ; } } }","public class A { public void activateVfModule ( BuildingBlockExecution execution ) { GeneralBuildingBlock gBBInput = execution . getGeneralBuildingBlock ( ) ; RequestContext requestContext = gBBInput . getRequestContext ( ) ; ServiceInstance serviceInstance = null ; GenericVnf vnf = null ; VfModule vfModule = null ; try { serviceInstance = extractPojosForBB . extractByKey ( execution , ResourceKey . SERVICE_INSTANCE_ID ) ; vnf = extractPojosForBB . extractByKey ( execution , ResourceKey . GENERIC_VNF_ID ) ; vfModule = extractPojosForBB . extractByKey ( execution , ResourceKey . VF_MODULE_ID ) ; Customer customer = gBBInput . getCustomer ( ) ; CloudRegion cloudRegion = gBBInput . getCloudRegion ( ) ; SDNCRequest sdncRequest = new SDNCRequest ( ) ; GenericResourceApiVfModuleOperationInformation req = sdncVfModuleResources . activateVfModule ( vfModule , vnf , serviceInstance , customer , cloudRegion , requestContext , buildCallbackURI ( sdncRequest ) ) ; sdncRequest . setSDNCPayload ( req ) ; sdncRequest . setTopology ( SDNCTopology . VFMODULE ) ; execution . setVariable ( SDNC_REQUEST , sdncRequest ) ; } catch ( Exception ex ) { logger . error ( ""Exception occurred in SDNCActivateTasks activateVfModule process"" , ex ) ; exceptionUtil . buildAndThrowWorkflowException ( execution , 7000 , ex ) ; } } }","logger . error ( ""Exception occurred in SDNCActivateTasks activateVfModule process"" , ex ) ;",Meaningful
"public class A { @ Deprecated public < T > T update ( Collection < String > path , T newValue ) { checkPath ( path ) ; if ( newValue == null ) { newValue = typedNull ( ) ; } if ( log . isTraceEnabled ( ) ) { log . trace ( ""update (path: {}) [{}]"" , path , newValue ) ; } @ SuppressWarnings ( ""unchecked"" ) T oldValue = ( T ) values . put ( path , newValue ) ; return ( isNull ( oldValue ) ) ? null : oldValue ; } }","public class A { @ Deprecated public < T > T update ( Collection < String > path , T newValue ) { checkPath ( path ) ; if ( newValue == null ) { newValue = typedNull ( ) ; } if ( log . isTraceEnabled ( ) ) { log . trace ( ""setting sensor {}={} for {}"" , new Object [ ] { path , newValue , entity } ) ; } @ SuppressWarnings ( ""unchecked"" ) T oldValue = ( T ) values . put ( path , newValue ) ; return ( isNull ( oldValue ) ) ? null : oldValue ; } }","log . trace ( ""setting sensor {}={} for {}"" , new Object [ ] { path , newValue , entity } ) ;",Meaningful
"public class A { protected void scheduleProjectResourceUpdate ( final Path resource ) { final Module module = moduleService . resolveModule ( resource ) ; getExecutor ( ) . execute ( new AsyncIncrementalBuilder ( ) { @ Override public void execute ( final ModuleService projectService , final BuildService buildService , final Event < IncrementalBuildResults > incrementalBuildResultsEvent , final Event < BuildResults > buildResultsEvent ) { try { logger . info ( ""Incremental Build ["" + resource . toURI ( ) + ""] was updated into "" + buildService . getIncrementalBuildResults ( ) + "" (updated)."" ) ; final BuildResults results = buildService . build ( module ) ; buildResultsEvent . fire ( results ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } } @ Override public String getDescription ( ) { return ""Incremental Build ["" + resource . toURI ( ) + "" (updated)]"" ; } } ) ; } }","public class A { protected void scheduleProjectResourceUpdate ( final Path resource ) { final Module module = moduleService . resolveModule ( resource ) ; getExecutor ( ) . execute ( new AsyncIncrementalBuilder ( ) { @ Override public void execute ( final ModuleService projectService , final BuildService buildService , final Event < IncrementalBuildResults > incrementalBuildResultsEvent , final Event < BuildResults > buildResultsEvent ) { try { logger . info ( ""Incremental build request being processed: "" + module . getRootPath ( ) + "" (updated)."" ) ; final BuildResults results = buildService . build ( module ) ; buildResultsEvent . fire ( results ) ; } catch ( Exception e ) { logger . error ( e . getMessage ( ) , e ) ; } } @ Override public String getDescription ( ) { return ""Incremental Build ["" + resource . toURI ( ) + "" (updated)]"" ; } } ) ; } }","logger . info ( ""Incremental build request being processed: "" + module . getRootPath ( ) + "" (updated)."" ) ;",Meaningful
"public class A { private Document getXMLStatusFile ( Board board ) { DocumentBuilderFactory dbFactory = DocumentBuilderFactory . newInstance ( ) ; DocumentBuilder dBuilder = null ; try { dBuilder = dbFactory . newDocumentBuilder ( ) ; } catch ( ParserConfigurationException ex ) { LOG . error ( ""Cannot instantiate the XML parser"" , ex ) ; } Document doc = null ; String statusFileURL = null ; try { statusFileURL = ""http://"" + board . getIpAddress ( ) + "":"" + Integer . toString ( board . getPort ( ) ) + ""/"" + GET_SENSORS_URL ; LOG . info ( ""Zibase gets relay status from file "" + statusFileURL ) ; doc = dBuilder . parse ( new URL ( statusFileURL ) . openStream ( ) ) ; doc . getDocumentElement ( ) . normalize ( ) ; } catch ( ConnectException connEx ) { disconnect ( ) ; this . stop ( ) ; this . setDescription ( ""Connection timed out, no reply from the board at "" + statusFileURL ) ; } catch ( SAXException ex ) { disconnect ( ) ; this . stop ( ) ; LOG . error ( Freedomotic . getStackTraceInfo ( ex ) ) ; } catch ( Exception ex ) { disconnect ( ) ; this . stop ( ) ; setDescription ( ""Unable to connect to "" + statusFileURL ) ; LOG . error ( Freedomotic . getStackTraceInfo ( ex ) ) ; } return doc ; } }","public class A { private Document getXMLStatusFile ( Board board ) { DocumentBuilderFactory dbFactory = DocumentBuilderFactory . newInstance ( ) ; DocumentBuilder dBuilder = null ; try { dBuilder = dbFactory . newDocumentBuilder ( ) ; } catch ( ParserConfigurationException ex ) { LOG . error ( ex . getLocalizedMessage ( ) ) ; } Document doc = null ; String statusFileURL = null ; try { statusFileURL = ""http://"" + board . getIpAddress ( ) + "":"" + Integer . toString ( board . getPort ( ) ) + ""/"" + GET_SENSORS_URL ; LOG . info ( ""Zibase gets relay status from file "" + statusFileURL ) ; doc = dBuilder . parse ( new URL ( statusFileURL ) . openStream ( ) ) ; doc . getDocumentElement ( ) . normalize ( ) ; } catch ( ConnectException connEx ) { disconnect ( ) ; this . stop ( ) ; this . setDescription ( ""Connection timed out, no reply from the board at "" + statusFileURL ) ; } catch ( SAXException ex ) { disconnect ( ) ; this . stop ( ) ; LOG . error ( Freedomotic . getStackTraceInfo ( ex ) ) ; } catch ( Exception ex ) { disconnect ( ) ; this . stop ( ) ; setDescription ( ""Unable to connect to "" + statusFileURL ) ; LOG . error ( Freedomotic . getStackTraceInfo ( ex ) ) ; } return doc ; } }",LOG . error ( ex . getLocalizedMessage ( ) ) ;,Meaningful
"public class A { @ Override protected void internalLogFormatted ( final String msg , final LogRecord record ) { final Level level = record . getLevel ( ) ; final Throwable t = record . getThrown ( ) ; if ( Level . FINE . equals ( level ) || Level . FINER . equals ( level ) || Level . CONFIG . equals ( level ) ) { if ( t == null ) { logger . debug ( msg ) ; } else { logger . debug ( msg , t ) ; } } else if ( Level . INFO . equals ( level ) ) { if ( t == null ) { logger . info ( msg ) ; } else { logger . info ( msg , t ) ; } } else if ( Level . WARNING . equals ( level ) ) { if ( t == null ) { logger . warn ( msg ) ; } else { logger . warn ( msg , t ) ; } } else if ( Level . ALL . equals ( level ) || Level . SEVERE . equals ( level ) ) { if ( t == null ) { logger . error ( msg ) ; } else { logger . error ( msg ) ; } } } }","public class A { @ Override protected void internalLogFormatted ( final String msg , final LogRecord record ) { final Level level = record . getLevel ( ) ; final Throwable t = record . getThrown ( ) ; if ( Level . FINE . equals ( level ) || Level . FINER . equals ( level ) || Level . CONFIG . equals ( level ) ) { if ( t == null ) { logger . debug ( msg ) ; } else { logger . debug ( msg , t ) ; } } else if ( Level . INFO . equals ( level ) ) { if ( t == null ) { logger . info ( msg ) ; } else { logger . info ( msg , t ) ; } } else if ( Level . WARNING . equals ( level ) ) { if ( t == null ) { logger . warn ( msg ) ; } else { logger . warn ( msg , t ) ; } } else if ( Level . ALL . equals ( level ) || Level . SEVERE . equals ( level ) ) { if ( t == null ) { logger . error ( msg ) ; } else { logger . error ( msg , t ) ; } } } }","logger . error ( msg , t ) ;",Meaningful
"public class A { public void action ( KrbSafeContainer krbSafeContainer ) throws DecoderException { TLV tlv = krbSafeContainer . getCurrentTLV ( ) ; if ( tlv . getLength ( ) == 0 ) { LOG . error ( I18n . err ( I18n . ERR_01308_ZERO_LENGTH_TLV ) ) ; throw new DecoderException ( I18n . err ( I18n . ERR_01309_EMPTY_TLV ) ) ; } KrbSafe krbSafe = new KrbSafe ( ) ; krbSafeContainer . setKrbSafe ( krbSafe ) ; if ( IS_DEBUG ) { LOG . debug ( ""KrbSafe: {}"" , krbSafe ) ; } } }","public class A { public void action ( KrbSafeContainer krbSafeContainer ) throws DecoderException { TLV tlv = krbSafeContainer . getCurrentTLV ( ) ; if ( tlv . getLength ( ) == 0 ) { LOG . error ( I18n . err ( I18n . ERR_01308_ZERO_LENGTH_TLV ) ) ; throw new DecoderException ( I18n . err ( I18n . ERR_01309_EMPTY_TLV ) ) ; } KrbSafe krbSafe = new KrbSafe ( ) ; krbSafeContainer . setKrbSafe ( krbSafe ) ; if ( IS_DEBUG ) { LOG . debug ( ""KrbSafe created"" ) ; } } }","LOG . debug ( ""KrbSafe created"" ) ;",Meaningful
"public class A { public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = ""insert into users values ('user', '"" + new Sha256Hash ( ""user"" ) . toBase64 ( ) + ""' )"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Created user."" ) ; query = ""insert into users values ( 'admin', '"" + new Sha256Hash ( ""admin"" ) . toBase64 ( ) + ""' )"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Created admin."" ) ; query = ""insert into roles values ( 'user' )"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Created user"" ) ; query = ""insert into roles values ( 'admin' )"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Created admin"" ) ; query = ""insert into roles_permissions values ( 'user', 'view')"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Created permission view for role user"" ) ; query = ""insert into roles_permissions values ( 'admin', 'user:*')"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Created permission view for role user"" ) ; query = ""insert into user_roles values ( 'user', 'user' )"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Assigned user role user"" ) ; query = ""insert into user_roles values ( 'admin', 'admin' )"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Assigned admin role admin"" ) ; } }","public class A { public void afterPropertiesSet ( ) throws Exception { JdbcTemplate jdbcTemplate = new JdbcTemplate ( dataSource ) ; jdbcTemplate . execute ( CREATE_TABLES ) ; String query = ""insert into users values ('user', '"" + new Sha256Hash ( ""user"" ) . toBase64 ( ) + ""' )"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Created user."" ) ; query = ""insert into users values ( 'admin', '"" + new Sha256Hash ( ""admin"" ) . toBase64 ( ) + ""' )"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Created admin."" ) ; query = ""insert into roles values ( 'user' )"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Created user"" ) ; query = ""insert into roles values ( 'admin' )"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Created admin"" ) ; query = ""insert into roles_permissions values ( 'user', 'view')"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Created permission view for role user"" ) ; query = ""insert into roles_permissions values ( 'admin', 'user:*')"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Created permission user:* for role admin"" ) ; query = ""insert into user_roles values ( 'user', 'user' )"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Assigned user role user"" ) ; query = ""insert into user_roles values ( 'admin', 'admin' )"" ; jdbcTemplate . execute ( query ) ; LOGGER . debug ( ""Assigned admin role admin"" ) ; } }","LOGGER . debug ( ""Created permission user:* for role admin"" ) ;",Meaningful
"public class A { @ Override protected void doFilterInternal ( HttpServletRequest request , HttpServletResponse response , FilterChain chain ) throws IOException , ServletException { if ( ! request . getScheme ( ) . equalsIgnoreCase ( ""https"" ) && ! ignoreHttps ) { LOGGER . debug ( ""not in HTTPS, skipping filter."" ) ; chain . doFilter ( request , response ) ; return ; } if ( ! authenticationIsRequired ( ) ) { LOGGER . debug ( ""the user has already been authenticated, skipping filter."" ) ; chain . doFilter ( request , response ) ; return ; } String auth = request . getHeader ( ""Authorization"" ) ; if ( ( auth == null ) || ! auth . startsWith ( ""Basic "" ) ) { final String userAgent = request . getHeader ( ""User-Agent"" ) ; if ( userAgentMatch ( userAgent ) ) { LOGGER . debug ( ""the user-agent matched and no Authorization header was sent, returning a 401."" ) ; getAuthenticationEntryPoint ( ) . commence ( request , response , _exception ) ; } else { LOGGER . debug ( ""Authorization header sent in the request, activating filter."" ) ; chain . doFilter ( request , response ) ; } } else { LOGGER . debug ( ""Authorization header sent in the request, activating filter ..."" ) ; super . doFilterInternal ( request , response , chain ) ; } } }","public class A { @ Override protected void doFilterInternal ( HttpServletRequest request , HttpServletResponse response , FilterChain chain ) throws IOException , ServletException { if ( ! request . getScheme ( ) . equalsIgnoreCase ( ""https"" ) && ! ignoreHttps ) { LOGGER . debug ( ""not in HTTPS, skipping filter."" ) ; chain . doFilter ( request , response ) ; return ; } if ( ! authenticationIsRequired ( ) ) { LOGGER . debug ( ""the user has already been authenticated, skipping filter."" ) ; chain . doFilter ( request , response ) ; return ; } String auth = request . getHeader ( ""Authorization"" ) ; if ( ( auth == null ) || ! auth . startsWith ( ""Basic "" ) ) { final String userAgent = request . getHeader ( ""User-Agent"" ) ; if ( userAgentMatch ( userAgent ) ) { LOGGER . debug ( ""the user-agent matched and no Authorization header was sent, returning a 401."" ) ; getAuthenticationEntryPoint ( ) . commence ( request , response , _exception ) ; } else { LOGGER . debug ( ""the user-agent does not match, skipping filter."" ) ; chain . doFilter ( request , response ) ; } } else { LOGGER . debug ( ""Authorization header sent in the request, activating filter ..."" ) ; super . doFilterInternal ( request , response , chain ) ; } } }","LOGGER . debug ( ""the user-agent does not match, skipping filter."" ) ;",Meaningful
"public class A { protected void generateUDTPojos ( SchemaDefinition schema ) { log . info ( ""Generating UDT {}"" , schema . getName ( ) ) ; for ( UDTDefinition udt : database . getUDTs ( schema ) ) { try { generateUDTPojo ( udt ) ; } catch ( Exception e ) { log . error ( ""Error while generating UDT POJO "" + udt , e ) ; } } watch . splitInfo ( ""UDT POJOs generated"" ) ; } }","public class A { protected void generateUDTPojos ( SchemaDefinition schema ) { log . info ( ""Generating UDT POJOs"" ) ; for ( UDTDefinition udt : database . getUDTs ( schema ) ) { try { generateUDTPojo ( udt ) ; } catch ( Exception e ) { log . error ( ""Error while generating UDT POJO "" + udt , e ) ; } } watch . splitInfo ( ""UDT POJOs generated"" ) ; } }","log . info ( ""Generating UDT POJOs"" ) ;",Meaningful
"public class A { public DeviceConnection connect ( final DeviceConnectionParameters deviceConnectionParameters , final String organisationIdentification , final boolean cacheConnection ) throws ConnectionFailureException { final String deviceIdentification = deviceConnectionParameters . getDeviceIdentification ( ) ; final String serverName = deviceConnectionParameters . getServerName ( ) ; final IED ied = deviceConnectionParameters . getIed ( ) ; try { if ( cacheConnection && this . testIfConnectionIsCachedAndAlive ( deviceIdentification , ied , serverName , deviceConnectionParameters . getLogicalDevice ( ) ) ) { return new DeviceConnection ( this . fetchIec61850Connection ( deviceIdentification ) , deviceIdentification , organisationIdentification , serverName ) ; } } catch ( final ProtocolAdapterException e ) { this . logProtocolAdapterException ( deviceIdentification , e ) ; } final InetAddress inetAddress = this . convertIpAddress ( deviceConnectionParameters . getIpAddress ( ) ) ; LOGGER . info ( ""Trying to connect to deviceIdentification: {} at IP address {} using response time-out: {}"" , deviceIdentification , deviceConnectionParameters . getIpAddress ( ) , this . responseTimeout ) ; final DateTime startTime = DateTime . now ( ) ; Iec61850ClientBaseEventListener eventListener = null ; try { eventListener = this . iec61850ClientEventListenerFactory . getEventListener ( ied , deviceIdentification , organisationIdentification ) ; } catch ( final ProtocolAdapterException e ) { this . logProtocolAdapterException ( deviceIdentification , e ) ; } final Iec61850Device iec61850Device = this . iec61850DeviceRepository . findByDeviceIdentification ( deviceIdentification ) ; final int port = this . determinePortForIec61850Device ( ied , iec61850Device ) ; final Iec61850ClientAssociation iec61850ClientAssociation = this . iec61850Client . connect ( deviceIdentification , inetAddress , eventListener , port ) ; final ClientAssociation clientAssociation = iec61850ClientAssociation . getClientAssociation ( ) ; clientAssociation . setResponseTimeout ( this . responseTimeout ) ; ServerModel serverModel ; try { serverModel = this . readServerModel ( clientAssociation , deviceIdentification , iec61850Device ) ; } catch ( final ProtocolAdapterException e ) { LOGGER . error ( ""ProtocolAdapterException: unable to read ServerModel for deviceIdentification {}"" , deviceIdentification , e ) ; throw new ConnectionFailureException ( e . getMessage ( ) , e ) ; } final Iec61850Connection iec61850Connection = new Iec61850Connection ( iec61850ClientAssociation , serverModel , startTime , ied ) ; if ( cacheConnection ) { this . cacheIec61850Connection ( deviceIdentification , iec61850Connection ) ; } final DeviceConnection connection = new DeviceConnection ( iec61850Connection , deviceIdentification , organisationIdentification , serverName ) ; final DateTime endTime = DateTime . now ( ) ; LOGGER . info ( ""Total endpoint connection is {} for device: {}, max redelivery count: {} and max retry count: {}"" , connection . getDeviceIdentification ( ) , endpoint . getDeviceIdentification ( ) , endpoint . getMaxRetryCount ( ) , endpoint . getMaxRetryCount ( ) , responseTimeout ) ; this . iec61850RtuDeviceReportingService . enableReportingForDevice ( connection , deviceIdentification , serverName ) ; return connection ; } }","public class A { public DeviceConnection connect ( final DeviceConnectionParameters deviceConnectionParameters , final String organisationIdentification , final boolean cacheConnection ) throws ConnectionFailureException { final String deviceIdentification = deviceConnectionParameters . getDeviceIdentification ( ) ; final String serverName = deviceConnectionParameters . getServerName ( ) ; final IED ied = deviceConnectionParameters . getIed ( ) ; try { if ( cacheConnection && this . testIfConnectionIsCachedAndAlive ( deviceIdentification , ied , serverName , deviceConnectionParameters . getLogicalDevice ( ) ) ) { return new DeviceConnection ( this . fetchIec61850Connection ( deviceIdentification ) , deviceIdentification , organisationIdentification , serverName ) ; } } catch ( final ProtocolAdapterException e ) { this . logProtocolAdapterException ( deviceIdentification , e ) ; } final InetAddress inetAddress = this . convertIpAddress ( deviceConnectionParameters . getIpAddress ( ) ) ; LOGGER . info ( ""Trying to connect to deviceIdentification: {} at IP address {} using response time-out: {}"" , deviceIdentification , deviceConnectionParameters . getIpAddress ( ) , this . responseTimeout ) ; final DateTime startTime = DateTime . now ( ) ; Iec61850ClientBaseEventListener eventListener = null ; try { eventListener = this . iec61850ClientEventListenerFactory . getEventListener ( ied , deviceIdentification , organisationIdentification ) ; } catch ( final ProtocolAdapterException e ) { this . logProtocolAdapterException ( deviceIdentification , e ) ; } final Iec61850Device iec61850Device = this . iec61850DeviceRepository . findByDeviceIdentification ( deviceIdentification ) ; final int port = this . determinePortForIec61850Device ( ied , iec61850Device ) ; final Iec61850ClientAssociation iec61850ClientAssociation = this . iec61850Client . connect ( deviceIdentification , inetAddress , eventListener , port ) ; final ClientAssociation clientAssociation = iec61850ClientAssociation . getClientAssociation ( ) ; clientAssociation . setResponseTimeout ( this . responseTimeout ) ; ServerModel serverModel ; try { serverModel = this . readServerModel ( clientAssociation , deviceIdentification , iec61850Device ) ; } catch ( final ProtocolAdapterException e ) { LOGGER . error ( ""ProtocolAdapterException: unable to read ServerModel for deviceIdentification {}"" , deviceIdentification , e ) ; throw new ConnectionFailureException ( e . getMessage ( ) , e ) ; } final Iec61850Connection iec61850Connection = new Iec61850Connection ( iec61850ClientAssociation , serverModel , startTime , ied ) ; if ( cacheConnection ) { this . cacheIec61850Connection ( deviceIdentification , iec61850Connection ) ; } final DeviceConnection connection = new DeviceConnection ( iec61850Connection , deviceIdentification , organisationIdentification , serverName ) ; final DateTime endTime = DateTime . now ( ) ; LOGGER . info ( ""Connected to device: {}, fetched server model. Start time: {}, end time: {}, total time in milliseconds: {}"" , deviceIdentification , startTime , endTime , endTime . minus ( startTime . getMillis ( ) ) . getMillis ( ) ) ; this . iec61850RtuDeviceReportingService . enableReportingForDevice ( connection , deviceIdentification , serverName ) ; return connection ; } }","LOGGER . info ( ""Connected to device: {}, fetched server model. Start time: {}, end time: {}, total time in milliseconds: {}"" , deviceIdentification , startTime , endTime , endTime . minus ( startTime . getMillis ( ) ) . getMillis ( ) ) ;",Meaningful
"public class A { public void init ( ClassLoader classLoader ) throws Exception { for ( Map . Entry < String , String > entry : _threadLocalSources . entrySet ( ) ) { String className = entry . getKey ( ) ; String fieldName = entry . getValue ( ) ; Class < ? > clazz = classLoader . loadClass ( className ) ; Field field = ReflectionUtil . getDeclaredField ( clazz , fieldName ) ; if ( ! ThreadLocal . class . isAssignableFrom ( field . getType ( ) ) ) { if ( _log . isWarnEnabled ( ) ) { _log . warn ( fieldName + "" is not a class "" + clazz . getName ( ) ) ; } continue ; } if ( ! Modifier . isStatic ( field . getModifiers ( ) ) ) { if ( _log . isWarnEnabled ( ) ) { _log . warn ( fieldName + "" is not a static ThreadLocal. Skip binding."" ) ; } continue ; } ThreadLocal < ? > threadLocal = ( ThreadLocal < ? > ) field . get ( null ) ; if ( threadLocal == null ) { if ( _log . isWarnEnabled ( ) ) { _log . warn ( fieldName + "" is not initialized. Skip binding."" ) ; } continue ; } _threadLocals . add ( threadLocal ) ; } } }","public class A { public void init ( ClassLoader classLoader ) throws Exception { for ( Map . Entry < String , String > entry : _threadLocalSources . entrySet ( ) ) { String className = entry . getKey ( ) ; String fieldName = entry . getValue ( ) ; Class < ? > clazz = classLoader . loadClass ( className ) ; Field field = ReflectionUtil . getDeclaredField ( clazz , fieldName ) ; if ( ! ThreadLocal . class . isAssignableFrom ( field . getType ( ) ) ) { if ( _log . isWarnEnabled ( ) ) { _log . warn ( fieldName + "" is not of type ThreadLocal. Skip binding."" ) ; } continue ; } if ( ! Modifier . isStatic ( field . getModifiers ( ) ) ) { if ( _log . isWarnEnabled ( ) ) { _log . warn ( fieldName + "" is not a static ThreadLocal. Skip binding."" ) ; } continue ; } ThreadLocal < ? > threadLocal = ( ThreadLocal < ? > ) field . get ( null ) ; if ( threadLocal == null ) { if ( _log . isWarnEnabled ( ) ) { _log . warn ( fieldName + "" is not initialized. Skip binding."" ) ; } continue ; } _threadLocals . add ( threadLocal ) ; } } }","_log . warn ( fieldName + "" is not of type ThreadLocal. Skip binding."" ) ;",Meaningful
"public class A { @ PayloadRoot ( localPart = ""FindDevicesWhichHaveNoOwnerRequest"" , namespace = DEVICE_MANAGEMENT_NAMESPACE ) @ ResponsePayload public FindDevicesWhichHaveNoOwnerResponse findDevicesWhichHaveNoOwner ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final FindDevicesWhichHaveNoOwnerRequest request ) throws OsgpException { LOGGER . info ( ""Find DevicesWhichHaveNoOwner request for organisation: {} and device: {}."" , organisationIdentification , request . getDeviceIdentification ( ) ) ; final FindDevicesWhichHaveNoOwnerResponse response = new FindDevicesWhichHaveNoOwnerResponse ( ) ; try { final List < org . opensmartgridplatform . domain . core . entities . Device > devicesWithoutOwner = this . deviceManagementService . findDevicesWhichHaveNoOwner ( organisationIdentification ) ; response . getDevices ( ) . addAll ( this . deviceManagementMapper . mapAsList ( devicesWithoutOwner , org . opensmartgridplatform . adapter . ws . schema . admin . devicemanagement . Device . class ) ) ; } catch ( final ConstraintViolationException e ) { throw new FunctionalException ( FunctionalExceptionType . VALIDATION_ERROR , COMPONENT_TYPE_WS_ADMIN , new ValidationException ( e . getConstraintViolations ( ) ) ) ; } catch ( final Exception e ) { this . handleException ( e ) ; } return response ; } }","public class A { @ PayloadRoot ( localPart = ""FindDevicesWhichHaveNoOwnerRequest"" , namespace = DEVICE_MANAGEMENT_NAMESPACE ) @ ResponsePayload public FindDevicesWhichHaveNoOwnerResponse findDevicesWhichHaveNoOwner ( @ OrganisationIdentification final String organisationIdentification , @ RequestPayload final FindDevicesWhichHaveNoOwnerRequest request ) throws OsgpException { LOGGER . info ( ""Finding devices which have no owner for organisation: {}."" , organisationIdentification ) ; final FindDevicesWhichHaveNoOwnerResponse response = new FindDevicesWhichHaveNoOwnerResponse ( ) ; try { final List < org . opensmartgridplatform . domain . core . entities . Device > devicesWithoutOwner = this . deviceManagementService . findDevicesWhichHaveNoOwner ( organisationIdentification ) ; response . getDevices ( ) . addAll ( this . deviceManagementMapper . mapAsList ( devicesWithoutOwner , org . opensmartgridplatform . adapter . ws . schema . admin . devicemanagement . Device . class ) ) ; } catch ( final ConstraintViolationException e ) { throw new FunctionalException ( FunctionalExceptionType . VALIDATION_ERROR , COMPONENT_TYPE_WS_ADMIN , new ValidationException ( e . getConstraintViolations ( ) ) ) ; } catch ( final Exception e ) { this . handleException ( e ) ; } return response ; } }","LOGGER . info ( ""Finding devices which have no owner for organisation: {}."" , organisationIdentification ) ;",Meaningful
"public class A { @ OnScheduled public final void abstractOnScheduled ( ProcessContext context ) throws IOException { try { HdfsResources resources = hdfsResources . get ( ) ; if ( resources . getConfiguration ( ) == null ) { final ResourceReferences configResources = context . getProperty ( HADOOP_CONFIGURATION_RESOURCES ) . evaluateAttributeExpressions ( ) . asResources ( ) ; resources = resetHDFSResources ( configResources , context ) ; hdfsResources . set ( resources ) ; } } catch ( Exception ex ) { getLogger ( ) . error ( ""Could not retrieve the HDFS resources. This exception is ignored."" , ex ) ; hdfsResources . set ( EMPTY_HDFS_RESOURCES ) ; throw ex ; } } }","public class A { @ OnScheduled public final void abstractOnScheduled ( ProcessContext context ) throws IOException { try { HdfsResources resources = hdfsResources . get ( ) ; if ( resources . getConfiguration ( ) == null ) { final ResourceReferences configResources = context . getProperty ( HADOOP_CONFIGURATION_RESOURCES ) . evaluateAttributeExpressions ( ) . asResources ( ) ; resources = resetHDFSResources ( configResources , context ) ; hdfsResources . set ( resources ) ; } } catch ( Exception ex ) { getLogger ( ) . error ( ""HDFS Configuration error - {}"" , new Object [ ] { ex } ) ; hdfsResources . set ( EMPTY_HDFS_RESOURCES ) ; throw ex ; } } }","getLogger ( ) . error ( ""HDFS Configuration error - {}"" , new Object [ ] { ex } ) ;",Meaningful
"public class A { private void registerCacheListener ( final String metadataTableName ) { if ( log . isDebugEnabled ( ) ) log . debug ( ""table:"" + metadataTableName + "" created UpdateHdfs listener for table:"" + metadataTableName ) ; final SharedCacheCoordinator watcher = new SharedCacheCoordinator ( metadataTableName , this . zookeepers , 30 , 300 , 10 ) ; try { watcher . start ( ) ; } catch ( Exception | Error e ) { throw new RuntimeException ( ""table:"" + metadataTableName + "" Error starting Watcher for MetadataHelper"" , e ) ; } final String triStateName = metadataTableName + "":needsUpdate"" ; try { watcher . registerTriState ( triStateName , new SharedTriStateListener ( ) { @ Override public void stateHasChanged ( SharedTriStateReader reader , SharedTriState . STATE value ) throws Exception { if ( log . isTraceEnabled ( ) ) { log . trace ( ""table:"" + metadataTableName + "" stateHasChanged("" + client + "", "" + value + "")"" ) ; } if ( value == SharedTriState . STATE . NEEDS_UPDATE ) { maybeUpdateTypeMetadataInHdfs ( watcher , triStateName , metadataTableName ) ; } } @ Override public void stateChanged ( CuratorFramework client , ConnectionState newState ) { if ( log . isTraceEnabled ( ) ) log . trace ( ""table:"" + metadataTableName + "" stateChanged("" + client + "", "" + newState + "")"" ) ; } } ) ; if ( ! watcher . checkTriState ( triStateName , SharedTriState . STATE . NEEDS_UPDATE ) ) { watcher . setTriState ( triStateName , SharedTriState . STATE . NEEDS_UPDATE ) ; } } catch ( Exception e ) { log . error ( e ) ; } } }","public class A { private void registerCacheListener ( final String metadataTableName ) { if ( log . isDebugEnabled ( ) ) log . debug ( ""table:"" + metadataTableName + "" created UpdateHdfs listener for table:"" + metadataTableName ) ; final SharedCacheCoordinator watcher = new SharedCacheCoordinator ( metadataTableName , this . zookeepers , 30 , 300 , 10 ) ; try { watcher . start ( ) ; } catch ( Exception | Error e ) { throw new RuntimeException ( ""table:"" + metadataTableName + "" Error starting Watcher for MetadataHelper"" , e ) ; } final String triStateName = metadataTableName + "":needsUpdate"" ; try { watcher . registerTriState ( triStateName , new SharedTriStateListener ( ) { @ Override public void stateHasChanged ( SharedTriStateReader reader , SharedTriState . STATE value ) throws Exception { if ( log . isTraceEnabled ( ) ) { log . trace ( ""table:"" + metadataTableName + "" stateHasChanged("" + reader + "", "" + value + "") for "" + triStateName ) ; } if ( value == SharedTriState . STATE . NEEDS_UPDATE ) { maybeUpdateTypeMetadataInHdfs ( watcher , triStateName , metadataTableName ) ; } } @ Override public void stateChanged ( CuratorFramework client , ConnectionState newState ) { if ( log . isTraceEnabled ( ) ) log . trace ( ""table:"" + metadataTableName + "" stateChanged("" + client + "", "" + newState + "")"" ) ; } } ) ; if ( ! watcher . checkTriState ( triStateName , SharedTriState . STATE . NEEDS_UPDATE ) ) { watcher . setTriState ( triStateName , SharedTriState . STATE . NEEDS_UPDATE ) ; } } catch ( Exception e ) { log . error ( e ) ; } } }","log . trace ( ""table:"" + metadataTableName + "" stateHasChanged("" + reader + "", "" + value + "") for "" + triStateName ) ;",Meaningful
"public class A { @ BuildStep void unlessBuildProfile ( CombinedIndexBuildItem index , BuildProducer < BuildTimeConditionBuildItem > producer , BuildProducer < PreAdditionalBeanBuildTimeConditionBuildItem > producerPreAdditionalBean ) { Collection < AnnotationInstance > annotationInstances = index . getIndex ( ) . getAnnotations ( UNLESS_BUILD_PROFILE ) ; for ( AnnotationInstance instance : annotationInstances ) { String profileOnInstance = instance . value ( ) . asString ( ) ; boolean enabled = ! profileOnInstance . equals ( ProfileManager . getActiveProfile ( ) ) ; if ( enabled ) { LOGGER . debug ( ""Enabling "" + instance . target ( ) + "" since the profile value does not match the active profile."" ) ; } else { LOGGER . debug ( ""Disabling "" + instance . target ( ) + "" with profile "" + profileOnInstance ) ; } producer . produce ( new BuildTimeConditionBuildItem ( instance . target ( ) , enabled ) ) ; producerPreAdditionalBean . produce ( new PreAdditionalBeanBuildTimeConditionBuildItem ( instance . target ( ) , enabled ) ) ; } } }","public class A { @ BuildStep void unlessBuildProfile ( CombinedIndexBuildItem index , BuildProducer < BuildTimeConditionBuildItem > producer , BuildProducer < PreAdditionalBeanBuildTimeConditionBuildItem > producerPreAdditionalBean ) { Collection < AnnotationInstance > annotationInstances = index . getIndex ( ) . getAnnotations ( UNLESS_BUILD_PROFILE ) ; for ( AnnotationInstance instance : annotationInstances ) { String profileOnInstance = instance . value ( ) . asString ( ) ; boolean enabled = ! profileOnInstance . equals ( ProfileManager . getActiveProfile ( ) ) ; if ( enabled ) { LOGGER . debug ( ""Enabling "" + instance . target ( ) + "" since the profile value does not match the active profile."" ) ; } else { LOGGER . debug ( ""Disabling "" + instance . target ( ) + "" since the profile value matches the active profile."" ) ; } producer . produce ( new BuildTimeConditionBuildItem ( instance . target ( ) , enabled ) ) ; producerPreAdditionalBean . produce ( new PreAdditionalBeanBuildTimeConditionBuildItem ( instance . target ( ) , enabled ) ) ; } } }","LOGGER . debug ( ""Disabling "" + instance . target ( ) + "" since the profile value matches the active profile."" ) ;",Meaningful
"public class A { protected boolean restoreLowerBoundIdOrSkipTable ( Connection sourceConnect , TableOperationParams operationParams , ShardRecovery recoveryValue ) throws SQLException { Objects . requireNonNull ( sourceConnect , ""source connection is NULL"" ) ; Objects . requireNonNull ( operationParams , ""operationParams is NULL"" ) ; Objects . requireNonNull ( recoveryValue , ""recoveryValue is NULL"" ) ; if ( recoveryValue . getObjectName ( ) == null || recoveryValue . getObjectName ( ) . isEmpty ( ) ) { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( ""RESTORED object '{}' from = {}"" , operationParams . tableName , recoveryValue ) ; } else if ( recoveryValue . getProcessedObject ( ) != null && ! recoveryValue . getProcessedObject ( ) . isEmpty ( ) && isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { log . debug ( ""SKIP object '{}' by = {}"" , operationParams . tableName , recoveryValue ) ; return true ; } else { if ( recoveryValue . getObjectName ( ) != null && recoveryValue . getObjectName ( ) . equalsIgnoreCase ( currentTableName ) && ! isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { this . lowerBoundIdValue = recoveryValue . getLastColumnValue ( ) ; log . debug ( ""RESTORED object '{}' from = {}"" , operationParams . tableName , recoveryValue ) ; } else { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( ""GO NEXT object '{}' by = {}"" , operationParams . tableName , recoveryValue ) ; } } return false ; } }","public class A { protected boolean restoreLowerBoundIdOrSkipTable ( Connection sourceConnect , TableOperationParams operationParams , ShardRecovery recoveryValue ) throws SQLException { Objects . requireNonNull ( sourceConnect , ""source connection is NULL"" ) ; Objects . requireNonNull ( operationParams , ""operationParams is NULL"" ) ; Objects . requireNonNull ( recoveryValue , ""recoveryValue is NULL"" ) ; if ( recoveryValue . getObjectName ( ) == null || recoveryValue . getObjectName ( ) . isEmpty ( ) ) { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( ""START object '{}' from = {}"" , operationParams . tableName , recoveryValue ) ; } else if ( recoveryValue . getProcessedObject ( ) != null && ! recoveryValue . getProcessedObject ( ) . isEmpty ( ) && isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { log . debug ( ""SKIP object '{}' by = {}"" , operationParams . tableName , recoveryValue ) ; return true ; } else { if ( recoveryValue . getObjectName ( ) != null && recoveryValue . getObjectName ( ) . equalsIgnoreCase ( currentTableName ) && ! isContain ( recoveryValue . getProcessedObject ( ) , currentTableName ) ) { this . lowerBoundIdValue = recoveryValue . getLastColumnValue ( ) ; log . debug ( ""RESTORED object '{}' from = {}"" , operationParams . tableName , recoveryValue ) ; } else { this . lowerBoundIdValue = selectLowerBoundValue ( sourceConnect ) ; log . debug ( ""GO NEXT object '{}' by = {}"" , operationParams . tableName , recoveryValue ) ; } } return false ; } }","log . debug ( ""START object '{}' from = {}"" , operationParams . tableName , recoveryValue ) ;",Meaningful
"public class A { @ Test public void testSendSlowClientSuccess ( ) throws Exception { try ( SiteToSiteClient client = getDefaultBuilder ( ) . idleExpiration ( 1000 , TimeUnit . MILLISECONDS ) . portName ( ""input-running"" ) . build ( ) ) { final Transaction transaction = client . createTransaction ( TransferDirection . SEND ) ; assertNotNull ( transaction ) ; serverChecksum = ""3882825556"" ; for ( int i = 0 ; i < 3 ; i ++ ) { DataPacket packet = new DataPacketBuilder ( ) . contents ( ""Example contents from client."" ) . attr ( ""Client attr 1"" , ""Client attr 1 value"" ) . attr ( ""Client attr 2"" , ""Client attr 2 value"" ) . build ( ) ; transaction . send ( packet ) ; long written = ( ( Peer ) transaction . getCommunicant ( ) ) . getCommunicationsSession ( ) . getBytesWritten ( ) ; logger . info ( ""Sending {}"" , written ) ; Thread . sleep ( 50 ) ; } transaction . confirm ( ) ; transaction . complete ( ) ; } } }","public class A { @ Test public void testSendSlowClientSuccess ( ) throws Exception { try ( SiteToSiteClient client = getDefaultBuilder ( ) . idleExpiration ( 1000 , TimeUnit . MILLISECONDS ) . portName ( ""input-running"" ) . build ( ) ) { final Transaction transaction = client . createTransaction ( TransferDirection . SEND ) ; assertNotNull ( transaction ) ; serverChecksum = ""3882825556"" ; for ( int i = 0 ; i < 3 ; i ++ ) { DataPacket packet = new DataPacketBuilder ( ) . contents ( ""Example contents from client."" ) . attr ( ""Client attr 1"" , ""Client attr 1 value"" ) . attr ( ""Client attr 2"" , ""Client attr 2 value"" ) . build ( ) ; transaction . send ( packet ) ; long written = ( ( Peer ) transaction . getCommunicant ( ) ) . getCommunicationsSession ( ) . getBytesWritten ( ) ; logger . info ( ""{} bytes have been written."" , written ) ; Thread . sleep ( 50 ) ; } transaction . confirm ( ) ; transaction . complete ( ) ; } } }","logger . info ( ""{} bytes have been written."" , written ) ;",Meaningful
"public class A { private void clearOldCheckpoint ( FileSystem fs , Path checkpointPath ) throws IOException { int maxCheckpoint = context . getConf ( ) . getInt ( AngelConf . ANGEL_PS_CHECKPOINTS_MAX_NUM , AngelConf . DEFAULT_ANGEL_PS_CHECKPOINTS_MAX_NUM ) ; FileStatus [ ] status = fs . listStatus ( checkpointPath ) ; if ( status . length <= maxCheckpoint ) { return ; } int [ ] checkpointIds = new int [ status . length ] ; for ( int i = 0 ; i < status . length ; i ++ ) { try { checkpointIds [ i ] = Integer . valueOf ( status [ i ] . getPath ( ) . getName ( ) ) ; } catch ( Throwable x ) { LOG . warn ( ""Path "" + status [ i ] . getPath ( ) . toString ( ) + "" is not a valid checkpoint path, just remove it"" ) ; } } Sort . quickSort ( checkpointIds , status , 0 , checkpointIds . length - 1 ) ; for ( int i = 0 ; i < checkpointIds . length - maxCheckpoint ; i ++ ) { try { if ( ! fs . delete ( status [ i ] . getPath ( ) , true ) ) { LOG . warn ( ""Delete path "" + status [ i ] . getPath ( ) + "" failed "" ) ; } else { LOG . info ( ""Delete old checkpoint "" + status [ i ] . getPath ( ) + "" failed "" ) ; } } catch ( Throwable x ) { LOG . warn ( ""Failed to delete old checkpoint "" + status [ i ] . getPath ( ) + "", just remove it"" ) ; } } } }","public class A { private void clearOldCheckpoint ( FileSystem fs , Path checkpointPath ) throws IOException { int maxCheckpoint = context . getConf ( ) . getInt ( AngelConf . ANGEL_PS_CHECKPOINTS_MAX_NUM , AngelConf . DEFAULT_ANGEL_PS_CHECKPOINTS_MAX_NUM ) ; FileStatus [ ] status = fs . listStatus ( checkpointPath ) ; if ( status . length <= maxCheckpoint ) { return ; } int [ ] checkpointIds = new int [ status . length ] ; for ( int i = 0 ; i < status . length ; i ++ ) { try { checkpointIds [ i ] = Integer . valueOf ( status [ i ] . getPath ( ) . getName ( ) ) ; } catch ( Throwable x ) { LOG . warn ( ""Path "" + status [ i ] . getPath ( ) . toString ( ) + "" is not a valid checkpoint path, just remove it"" ) ; } } Sort . quickSort ( checkpointIds , status , 0 , checkpointIds . length - 1 ) ; for ( int i = 0 ; i < checkpointIds . length - maxCheckpoint ; i ++ ) { try { if ( ! fs . delete ( status [ i ] . getPath ( ) , true ) ) { LOG . warn ( ""Delete path "" + status [ i ] . getPath ( ) + "" failed "" ) ; } else { LOG . info ( ""Delete old checkpoint "" + status [ i ] . getPath ( ) + "" failed "" ) ; } } catch ( Throwable x ) { LOG . warn ( ""Delete path "" + status [ i ] . getPath ( ) + "" failed "" , x ) ; } } } }","LOG . warn ( ""Delete path "" + status [ i ] . getPath ( ) + "" failed "" , x ) ;",Meaningful
"public class A { public final List findByRowKeys ( Class entityClass , List < String > relationNames , boolean isWrapReq , EntityMetadata metadata , Object ... rowIds ) { List entities = null ; MetamodelImpl metaModel = ( MetamodelImpl ) kunderaMetadata . getApplicationMetadata ( ) . getMetamodel ( metadata . getPersistenceUnit ( ) ) ; EntityType entityType = metaModel . entity ( metadata . getEntityClazz ( ) ) ; List < AbstractManagedType > subManagedType = ( ( AbstractManagedType ) entityType ) . getSubManagedType ( ) ; try { if ( ! subManagedType . isEmpty ( ) ) { for ( AbstractManagedType subEntity : subManagedType ) { EntityMetadata subEntityMetadata = KunderaMetadataManager . getEntityMetadata ( kunderaMetadata , subEntity . getJavaType ( ) ) ; entities = getDataHandler ( ) . fromThriftRow ( entityClass , subEntityMetadata , subEntityMetadata . getRelationNames ( ) , isWrapReq , getConsistencyLevel ( ) , rowIds ) ; if ( entities != null && ! entities . isEmpty ( ) ) { break ; } } } else { entities = getDataHandler ( ) . fromThriftRow ( entityClass , metadata , relationNames , isWrapReq , getConsistencyLevel ( ) , rowIds ) ; } } catch ( Exception e ) { log . error ( ""Error while executing query for Class '{}', Caused by: ."" , entityClass , e ) ; throw new KunderaException ( e ) ; } return entities ; } }","public class A { public final List findByRowKeys ( Class entityClass , List < String > relationNames , boolean isWrapReq , EntityMetadata metadata , Object ... rowIds ) { List entities = null ; MetamodelImpl metaModel = ( MetamodelImpl ) kunderaMetadata . getApplicationMetadata ( ) . getMetamodel ( metadata . getPersistenceUnit ( ) ) ; EntityType entityType = metaModel . entity ( metadata . getEntityClazz ( ) ) ; List < AbstractManagedType > subManagedType = ( ( AbstractManagedType ) entityType ) . getSubManagedType ( ) ; try { if ( ! subManagedType . isEmpty ( ) ) { for ( AbstractManagedType subEntity : subManagedType ) { EntityMetadata subEntityMetadata = KunderaMetadataManager . getEntityMetadata ( kunderaMetadata , subEntity . getJavaType ( ) ) ; entities = getDataHandler ( ) . fromThriftRow ( entityClass , subEntityMetadata , subEntityMetadata . getRelationNames ( ) , isWrapReq , getConsistencyLevel ( ) , rowIds ) ; if ( entities != null && ! entities . isEmpty ( ) ) { break ; } } } else { entities = getDataHandler ( ) . fromThriftRow ( entityClass , metadata , relationNames , isWrapReq , getConsistencyLevel ( ) , rowIds ) ; } } catch ( Exception e ) { log . error ( ""Error while retrieving records for entity {}, row keys {}"" , entityClass , rowIds ) ; throw new KunderaException ( e ) ; } return entities ; } }","log . error ( ""Error while retrieving records for entity {}, row keys {}"" , entityClass , rowIds ) ;",Meaningful
"public class A { private static String getMavenHome ( Log log ) { String mavenHome = System . getProperty ( ""maven.home"" ) ; if ( mavenHome == null ) { try { mavenHome = CommandLineUtils . getSystemEnvVars ( ) . getProperty ( ""M2_HOME"" ) ; } catch ( IOException e ) { if ( log != null && log . isDebugEnabled ( ) ) { log . debug ( ""Error setting M2_HOME"" , e ) ; } } } File m2Home = new File ( mavenHome ) ; if ( ! m2Home . exists ( ) ) { if ( log != null && log . isErrorEnabled ( ) ) { log . error ( ""Cannot find Maven application directory. Either specify 'maven.home' system property, or "" + ""M2_HOME environment variable."" ) ; } } return mavenHome ; } }","public class A { private static String getMavenHome ( Log log ) { String mavenHome = System . getProperty ( ""maven.home"" ) ; if ( mavenHome == null ) { try { mavenHome = CommandLineUtils . getSystemEnvVars ( ) . getProperty ( ""M2_HOME"" ) ; } catch ( IOException e ) { if ( log != null && log . isDebugEnabled ( ) ) { log . debug ( ""IOException: "" + e . getMessage ( ) ) ; } } } File m2Home = new File ( mavenHome ) ; if ( ! m2Home . exists ( ) ) { if ( log != null && log . isErrorEnabled ( ) ) { log . error ( ""Cannot find Maven application directory. Either specify 'maven.home' system property, or "" + ""M2_HOME environment variable."" ) ; } } return mavenHome ; } }","log . debug ( ""IOException: "" + e . getMessage ( ) ) ;",Meaningful
"public class A { @ Override public void runJob ( Path input , Path output , BayesParameters params ) throws IOException { Configurable client = new JobClient ( ) ; JobConf conf = new JobConf ( BayesWeightSummerDriver . class ) ; conf . setJobName ( ""TfIdf Driver running over input: "" + input ) ; conf . setOutputKeyClass ( StringTuple . class ) ; conf . setOutputValueClass ( DoubleWritable . class ) ; FileInputFormat . addInputPath ( conf , new Path ( output , ""trainer-termDocCount"" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , ""trainer-wordFreq"" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , ""trainer-featureCount"" ) ) ; Path outPath = new Path ( output , ""trainer-tfIdf"" ) ; FileOutputFormat . setOutputPath ( conf , outPath ) ; conf . setJarByClass ( BayesTfIdfDriver . class ) ; conf . setMapperClass ( BayesTfIdfMapper . class ) ; conf . setInputFormat ( SequenceFileInputFormat . class ) ; conf . setCombinerClass ( BayesTfIdfReducer . class ) ; conf . setReducerClass ( BayesTfIdfReducer . class ) ; conf . setOutputFormat ( BayesTfIdfOutputFormat . class ) ; conf . set ( ""io.serializations"" , ""org.apache.hadoop.io.serializer.JavaSerialization,"" + ""org.apache.hadoop.io.serializer.WritableSerialization"" ) ; HadoopUtil . delete ( conf , outPath ) ; Path interimFile = new Path ( output , ""trainer-docCount/part-*"" ) ; Map < String , Double > labelDocumentCounts = SequenceFileModelReader . readLabelDocumentCounts ( interimFile , conf ) ; DefaultStringifier < Map < String , Double > > mapStringifier = new DefaultStringifier < Map < String , Double > > ( conf , GenericsUtil . getClass ( labelDocumentCounts ) ) ; String labelDocumentCountString = mapStringifier . toString ( labelDocumentCounts ) ; log . info ( ""Counts of documents in Each Label"" ) ; Map < String , Double > c = mapStringifier . fromString ( labelDocumentCountString ) ; log . info ( ""cnaivebayes.labelDocumentCounts"" , c ) ; conf . set ( ""cnaivebayes.labelDocumentCounts"" , labelDocumentCountString ) ; log . info ( params . print ( ) ) ; conf . set ( ""bayes.parameters"" , params . toString ( ) ) ; client . setConf ( conf ) ; JobClient . runJob ( conf ) ; } }","public class A { @ Override public void runJob ( Path input , Path output , BayesParameters params ) throws IOException { Configurable client = new JobClient ( ) ; JobConf conf = new JobConf ( BayesWeightSummerDriver . class ) ; conf . setJobName ( ""TfIdf Driver running over input: "" + input ) ; conf . setOutputKeyClass ( StringTuple . class ) ; conf . setOutputValueClass ( DoubleWritable . class ) ; FileInputFormat . addInputPath ( conf , new Path ( output , ""trainer-termDocCount"" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , ""trainer-wordFreq"" ) ) ; FileInputFormat . addInputPath ( conf , new Path ( output , ""trainer-featureCount"" ) ) ; Path outPath = new Path ( output , ""trainer-tfIdf"" ) ; FileOutputFormat . setOutputPath ( conf , outPath ) ; conf . setJarByClass ( BayesTfIdfDriver . class ) ; conf . setMapperClass ( BayesTfIdfMapper . class ) ; conf . setInputFormat ( SequenceFileInputFormat . class ) ; conf . setCombinerClass ( BayesTfIdfReducer . class ) ; conf . setReducerClass ( BayesTfIdfReducer . class ) ; conf . setOutputFormat ( BayesTfIdfOutputFormat . class ) ; conf . set ( ""io.serializations"" , ""org.apache.hadoop.io.serializer.JavaSerialization,"" + ""org.apache.hadoop.io.serializer.WritableSerialization"" ) ; HadoopUtil . delete ( conf , outPath ) ; Path interimFile = new Path ( output , ""trainer-docCount/part-*"" ) ; Map < String , Double > labelDocumentCounts = SequenceFileModelReader . readLabelDocumentCounts ( interimFile , conf ) ; DefaultStringifier < Map < String , Double > > mapStringifier = new DefaultStringifier < Map < String , Double > > ( conf , GenericsUtil . getClass ( labelDocumentCounts ) ) ; String labelDocumentCountString = mapStringifier . toString ( labelDocumentCounts ) ; log . info ( ""Counts of documents in Each Label"" ) ; Map < String , Double > c = mapStringifier . fromString ( labelDocumentCountString ) ; log . info ( ""{}"" , c ) ; conf . set ( ""cnaivebayes.labelDocumentCounts"" , labelDocumentCountString ) ; log . info ( params . print ( ) ) ; conf . set ( ""bayes.parameters"" , params . toString ( ) ) ; client . setConf ( conf ) ; JobClient . runJob ( conf ) ; } }","log . info ( ""{}"" , c ) ;",Meaningful
"public class A { private void processPageRegExpLabel ( String pageBody , String regExpKey , String regExpValue , boolean isDelta , long time ) { ArrayList < String > labels = new ArrayList < > ( ) ; Pattern patternKey = patterns . get ( regExpKey ) ; if ( patternKey == null ) { try { patternKey = Pattern . compile ( regExpKey ) ; patterns . put ( regExpKey , patternKey ) ; } catch ( PatternSyntaxException ex ) { log . error ( ""Error compiling pattern: "" + regExpKey ) ; } } Pattern patternValue = patterns . get ( regExpValue ) ; if ( patternValue == null ) { try { patternValue = Pattern . compile ( regExpValue ) ; patterns . put ( regExpValue , patternValue ) ; } catch ( PatternSyntaxException ex ) { log . error ( ""Error compiling pattern: "" + regExpValue ) ; } } if ( patternKey != null && patternValue != null ) { Matcher mKey = patternKey . matcher ( pageBody ) ; Matcher mValue = patternValue . matcher ( pageBody ) ; boolean found = false ; while ( mKey . find ( ) && mValue . find ( ) ) { found = true ; String key = mKey . group ( 1 ) ; if ( labels . contains ( key ) ) { int i = 2 ; while ( labels . contains ( key + ""_"" + i ) ) { i ++ ; } key = key + ""_"" + i ; } labels . add ( key ) ; String sValue = mValue . group ( 1 ) ; try { double value = Double . parseDouble ( sValue ) ; if ( isDelta ) { if ( oldValues . containsKey ( key ) ) { double delta = value - oldValues . get ( key ) ; addRecord ( key , time , delta ) ; } oldValues . put ( key , value ) ; } else { addRecord ( key , time , value ) ; } } catch ( NumberFormatException ex ) { log . error ( ""Value extracted is not a number: "" + sValue ) ; } } if ( ! found ) { log . warn ( ""Invalid search domain attribute "" + regExpKey ) ; } } } }","public class A { private void processPageRegExpLabel ( String pageBody , String regExpKey , String regExpValue , boolean isDelta , long time ) { ArrayList < String > labels = new ArrayList < > ( ) ; Pattern patternKey = patterns . get ( regExpKey ) ; if ( patternKey == null ) { try { patternKey = Pattern . compile ( regExpKey ) ; patterns . put ( regExpKey , patternKey ) ; } catch ( PatternSyntaxException ex ) { log . error ( ""Error compiling pattern: "" + regExpKey ) ; } } Pattern patternValue = patterns . get ( regExpValue ) ; if ( patternValue == null ) { try { patternValue = Pattern . compile ( regExpValue ) ; patterns . put ( regExpValue , patternValue ) ; } catch ( PatternSyntaxException ex ) { log . error ( ""Error compiling pattern: "" + regExpValue ) ; } } if ( patternKey != null && patternValue != null ) { Matcher mKey = patternKey . matcher ( pageBody ) ; Matcher mValue = patternValue . matcher ( pageBody ) ; boolean found = false ; while ( mKey . find ( ) && mValue . find ( ) ) { found = true ; String key = mKey . group ( 1 ) ; if ( labels . contains ( key ) ) { int i = 2 ; while ( labels . contains ( key + ""_"" + i ) ) { i ++ ; } key = key + ""_"" + i ; } labels . add ( key ) ; String sValue = mValue . group ( 1 ) ; try { double value = Double . parseDouble ( sValue ) ; if ( isDelta ) { if ( oldValues . containsKey ( key ) ) { double delta = value - oldValues . get ( key ) ; addRecord ( key , time , delta ) ; } oldValues . put ( key , value ) ; } else { addRecord ( key , time , value ) ; } } catch ( NumberFormatException ex ) { log . error ( ""Value extracted is not a number: "" + sValue ) ; } } if ( ! found ) { log . warn ( ""No data found for regExpKey: "" + regExpKey + "" and regExpValue: "" + regExpValue ) ; } } } }","log . warn ( ""No data found for regExpKey: "" + regExpKey + "" and regExpValue: "" + regExpValue ) ;",Meaningful
"public class A { public static List < HoodieRecord > convertMetadataToRecords ( HoodieCleanerPlan cleanerPlan , String instantTime ) { List < HoodieRecord > records = new LinkedList < > ( ) ; int [ ] fileDeleteCount = { 0 } ; cleanerPlan . getFilePathsToBeDeletedPerPartition ( ) . forEach ( ( partition , deletedPathInfo ) -> { fileDeleteCount [ 0 ] += deletedPathInfo . size ( ) ; List < String > deletedFilenames = deletedPathInfo . stream ( ) . map ( p -> new Path ( p . getFilePath ( ) ) . getName ( ) ) . collect ( Collectors . toList ( ) ) ; HoodieRecord record = HoodieMetadataPayload . createPartitionFilesRecord ( partition , Option . empty ( ) , Option . of ( deletedFilenames ) ) ; records . add ( record ) ; } ) ; LOG . info ( ""Updating at "" + instantTime + "" from Clean. #partitions_updated="" + records . size ( ) + "", #files_deleted="" + fileDeleteCount [ 0 ] ) ; return records ; } }","public class A { public static List < HoodieRecord > convertMetadataToRecords ( HoodieCleanerPlan cleanerPlan , String instantTime ) { List < HoodieRecord > records = new LinkedList < > ( ) ; int [ ] fileDeleteCount = { 0 } ; cleanerPlan . getFilePathsToBeDeletedPerPartition ( ) . forEach ( ( partition , deletedPathInfo ) -> { fileDeleteCount [ 0 ] += deletedPathInfo . size ( ) ; List < String > deletedFilenames = deletedPathInfo . stream ( ) . map ( p -> new Path ( p . getFilePath ( ) ) . getName ( ) ) . collect ( Collectors . toList ( ) ) ; HoodieRecord record = HoodieMetadataPayload . createPartitionFilesRecord ( partition , Option . empty ( ) , Option . of ( deletedFilenames ) ) ; records . add ( record ) ; } ) ; LOG . info ( ""Found at "" + instantTime + "" from CleanerPlan. #partitions_updated="" + records . size ( ) + "", #files_deleted="" + fileDeleteCount [ 0 ] ) ; return records ; } }","LOG . info ( ""Found at "" + instantTime + "" from CleanerPlan. #partitions_updated="" + records . size ( ) + "", #files_deleted="" + fileDeleteCount [ 0 ] ) ;",Meaningful
"public class A { private void processMessageRange ( ImapSession session , MessageManager mailbox , FetchData fetch , MailboxSession mailboxSession , Responder responder , FetchResponseBuilder builder , FetchGroup resultToFetch , MessageRange range ) throws MailboxException { MessageResultIterator messages = mailbox . getMessages ( range , resultToFetch , mailboxSession ) ; SelectedMailbox selected = session . getSelected ( ) ; while ( messages . hasNext ( ) ) { final MessageResult result = messages . next ( ) ; if ( fetch . contains ( Item . MODSEQ ) && result . getModSeq ( ) . asLong ( ) <= fetch . getChangedSince ( ) ) { continue ; } try { final FetchResponse response = builder . build ( fetch , result , mailbox , selected , mailboxSession ) ; responder . respond ( response ) ; } catch ( MessageRangeException e ) { LOGGER . debug ( ""Unable to find message with uid {}"" , result . getUid ( ) , e ) ; } catch ( MailboxException e ) { LOGGER . error ( ""Unable to build search response"" , e ) ; } } if ( messages . getException ( ) != null ) { throw messages . getException ( ) ; } } }","public class A { private void processMessageRange ( ImapSession session , MessageManager mailbox , FetchData fetch , MailboxSession mailboxSession , Responder responder , FetchResponseBuilder builder , FetchGroup resultToFetch , MessageRange range ) throws MailboxException { MessageResultIterator messages = mailbox . getMessages ( range , resultToFetch , mailboxSession ) ; SelectedMailbox selected = session . getSelected ( ) ; while ( messages . hasNext ( ) ) { final MessageResult result = messages . next ( ) ; if ( fetch . contains ( Item . MODSEQ ) && result . getModSeq ( ) . asLong ( ) <= fetch . getChangedSince ( ) ) { continue ; } try { final FetchResponse response = builder . build ( fetch , result , mailbox , selected , mailboxSession ) ; responder . respond ( response ) ; } catch ( MessageRangeException e ) { LOGGER . debug ( ""Unable to find message with uid {}"" , result . getUid ( ) , e ) ; } catch ( MailboxException e ) { LOGGER . error ( ""Unable to fetch message with uid {}, so skip it"" , result . getUid ( ) , e ) ; } } if ( messages . getException ( ) != null ) { throw messages . getException ( ) ; } } }","LOGGER . error ( ""Unable to fetch message with uid {}, so skip it"" , result . getUid ( ) , e ) ;",Meaningful
"public class A { @ SuppressWarnings ( ""checkstyle:illegalCatch"" ) private static < T > void executeJob ( JobEntry < T > job ) { LOG . trace ( ""Running job: {}"" , job ) ; final Callable < ListenableFuture < T > > mainWorker = job . getMainWorker ( ) ; if ( mainWorker == null ) { LOG . error ( ""Unexpected no (null) main worker on job: {}"" , job ) ; job . setResult ( null ) ; return ; } final ListenableFuture < T > future ; try { future = mainWorker . call ( ) ; } catch ( Exception e ) { LOG . error ( ""Direct Exception (not failed Future) when executing job, won't even retry: {}"" , job , e ) ; job . setResult ( null ) ; return ; } Futures . addCallback ( future , new FutureCallback < T > ( ) { @ Override public void onSuccess ( final T result ) { LOG . trace ( ""Job {} result: {}"" , job . getKey ( ) , result ) ; job . setResult ( result ) ; } @ Override public void onFailure ( final Throwable cause ) { LOG . error ( ""Job {} failed {}"" , job . getKey ( ) , cause . getMessage ( ) ) ; job . setFailure ( cause ) ; } } , MoreExecutors . directExecutor ( ) ) ; LOG . trace ( ""Finished running job: {}"" , job ) ; } }","public class A { @ SuppressWarnings ( ""checkstyle:illegalCatch"" ) private static < T > void executeJob ( JobEntry < T > job ) { LOG . trace ( ""Running job: {}"" , job ) ; final Callable < ListenableFuture < T > > mainWorker = job . getMainWorker ( ) ; if ( mainWorker == null ) { LOG . error ( ""Unexpected no (null) main worker on job: {}"" , job ) ; job . setResult ( null ) ; return ; } final ListenableFuture < T > future ; try { future = mainWorker . call ( ) ; } catch ( Exception e ) { LOG . error ( ""Direct Exception (not failed Future) when executing job, won't even retry: {}"" , job , e ) ; job . setResult ( null ) ; return ; } Futures . addCallback ( future , new FutureCallback < T > ( ) { @ Override public void onSuccess ( final T result ) { LOG . trace ( ""Job completed successfully: {}"" , job . getKey ( ) ) ; job . setResult ( result ) ; } @ Override public void onFailure ( final Throwable cause ) { LOG . error ( ""Job {} failed {}"" , job . getKey ( ) , cause . getMessage ( ) ) ; job . setFailure ( cause ) ; } } , MoreExecutors . directExecutor ( ) ) ; LOG . trace ( ""Finished running job: {}"" , job ) ; } }","LOG . trace ( ""Job completed successfully: {}"" , job . getKey ( ) ) ;",Meaningful
"public class A { public void addAll ( final String index , final List < Map < String , Object > > docList , final BiConsumer < Map < String , Object > , IndexRequestBuilder > options ) { final FessConfig fessConfig = ComponentUtil . getFessConfig ( ) ; final BulkRequestBuilder bulkRequestBuilder = client . prepareBulk ( ) ; for ( final Map < String , Object > doc : docList ) { final Object id = doc . remove ( fessConfig . getIndexFieldId ( ) ) ; final IndexRequestBuilder builder = client . prepareIndex ( ) . setIndex ( index ) . setId ( id . toString ( ) ) . setSource ( new DocMap ( doc ) ) ; options . accept ( doc , builder ) ; bulkRequestBuilder . add ( builder ) ; } final BulkResponse response = bulkRequestBuilder . execute ( ) . actionGet ( ComponentUtil . getFessConfig ( ) . getIndexBulkTimeout ( ) ) ; if ( response . hasFailures ( ) ) { if ( logger . isDebugEnabled ( ) ) { final List < DocWriteRequest < ? > > requests = bulkRequestBuilder . request ( ) . requests ( ) ; final BulkItemResponse [ ] items = response . getItems ( ) ; if ( requests . size ( ) == items . length ) { for ( int i = 0 ; i < requests . size ( ) ; i ++ ) { final BulkItemResponse resp = items [ i ] ; if ( resp . isFailed ( ) && resp . getFailure ( ) != null ) { final DocWriteRequest < ? > req = requests . get ( i ) ; final Failure failure = resp . getFailure ( ) ; logger . debug ( ""Failed to add {}"" , resp . getFailure ( ) . getMessage ( ) ) ; } } } } throw new SearchEngineClientException ( response . buildFailureMessage ( ) ) ; } } }","public class A { public void addAll ( final String index , final List < Map < String , Object > > docList , final BiConsumer < Map < String , Object > , IndexRequestBuilder > options ) { final FessConfig fessConfig = ComponentUtil . getFessConfig ( ) ; final BulkRequestBuilder bulkRequestBuilder = client . prepareBulk ( ) ; for ( final Map < String , Object > doc : docList ) { final Object id = doc . remove ( fessConfig . getIndexFieldId ( ) ) ; final IndexRequestBuilder builder = client . prepareIndex ( ) . setIndex ( index ) . setId ( id . toString ( ) ) . setSource ( new DocMap ( doc ) ) ; options . accept ( doc , builder ) ; bulkRequestBuilder . add ( builder ) ; } final BulkResponse response = bulkRequestBuilder . execute ( ) . actionGet ( ComponentUtil . getFessConfig ( ) . getIndexBulkTimeout ( ) ) ; if ( response . hasFailures ( ) ) { if ( logger . isDebugEnabled ( ) ) { final List < DocWriteRequest < ? > > requests = bulkRequestBuilder . request ( ) . requests ( ) ; final BulkItemResponse [ ] items = response . getItems ( ) ; if ( requests . size ( ) == items . length ) { for ( int i = 0 ; i < requests . size ( ) ; i ++ ) { final BulkItemResponse resp = items [ i ] ; if ( resp . isFailed ( ) && resp . getFailure ( ) != null ) { final DocWriteRequest < ? > req = requests . get ( i ) ; final Failure failure = resp . getFailure ( ) ; logger . debug ( ""Failed Request: {}\n=>{}"" , req , failure . getMessage ( ) ) ; } } } } throw new SearchEngineClientException ( response . buildFailureMessage ( ) ) ; } } }","logger . debug ( ""Failed Request: {}\n=>{}"" , req , failure . getMessage ( ) ) ;",Meaningful
,,,
,,,
,,,
,,Same Information,85
,,Meaningful,209
,,Meaningless,6